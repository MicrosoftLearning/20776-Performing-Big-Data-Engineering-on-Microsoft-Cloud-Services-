# Module 4: Managing Big Data in Azure Data Lake Store

- [Module 4: Managing Big Data in Azure Data Lake Store](#module-4-managing-big-data-in-azure-data-lake-store)
    - [Demo 1: Creating a UDF and using it in a Stream Analytics job](#demo-1-creating-a-udf-and-using-it-in-a-stream-analytics-job)
        - [Scenario](#scenario)
        - [Setup](#setup)
        - [Use the Azure portal to create a new Data Lake Store](#use-the-azure-portal-to-create-a-new-data-lake-store)
        - [Use the Azure portal to create Data Lake Store folders and upload data](#use-the-azure-portal-to-create-data-lake-store-folders-and-upload-data)
        - [Use PowerShell to access Data Lake Store resources](#use-powershell-to-access-data-lake-store-resources)
        - [Use the Azure portal to download data from Data Lake Store](#use-the-azure-portal-to-download-data-from-data-lake-store)
    - [Demo 2: Working with Data Lake Store using PowerShell](#demo-2-working-with-data-lake-store-using-powershell)
        - [Setup](#setup)
        - [Install AzCopy](#install-azcopy)
        - [Install AdlCopy](#install-adlcopy)
    - [Part 1—Use PowerShell to upload data to Data Lake Store](#part-1use-powershell-to-upload-data-to-data-lake-store)
        - [Use PowerShell to upload a single file directly to Data Lake Store](#use-powershell-to-upload-a-single-file-directly-to-data-lake-store)
        - [Use Data Explorer to verify the file upload](#use-data-explorer-to-verify-the-file-upload)
        - [Repeat the file upload with an increased PerFileThreadCount](#repeat-the-file-upload-with-an-increased-perfilethreadcount)
    - [Part 2—upload a set of files to Data Lake Store from Blob storage:](#part-2upload-a-set-of-files-to-data-lake-store-from-blob-storage)
        - [Use AzCopy to upload files to Blob storage](#use-azcopy-to-upload-files-to-blob-storage)
        - [Use Cloud Explorer in Visual Studio to examine the uploaded blob](#use-cloud-explorer-in-visual-studio-to-examine-the-uploaded-blob)
        - [Use AdlCopy to transfer files from Blob storage to Data Lake Store](#use-adlcopy-to-transfer-files-from-blob-storage-to-data-lake-store)
    - [Demo 3: Using Data Lake Store in an application](#demo-3-using-data-lake-store-in-an-application)
        - [Add a guest user to Azure AD](#add-a-guest-user-to-azure-ad)
        - [Use an app to test user access to Data Lake Store](#use-an-app-to-test-user-access-to-data-lake-store)
        - [Set and test permissions to Data Lake Store resources](#set-and-test-permissions-to-data-lake-store-resources)
  
## Demo 1: Creating a UDF and using it in a Stream Analytics job

### Scenario

In this demonstration, you will see how to:

- Use the Azure portal to create a new Data Lake Store.
- Use the Azure portal to create Data Lake Store folders and upload data.
- Use PowerShell to access Data Lake Store resources.
- Use the Azure portal to download data from Data Lake Store.

### Setup

1. Ensure that the **MT17B-WS2016-NAT**, **20776A-LON-DC**, and **20776A-LON-DEV** virtual machines are running, and then log on to 20776A-LON-DEV as **ADATUM\\AdatumAdmin** with the password **Pa55w.rd**.
2. Using **File Explorer**, move to the folder **E:\\Demofiles\Mod04\\Demo1**, right-click **Setup.cmd**, and then click **Run as administrator**.
3. In the **User Account Control** dialog box, click **Yes**.
4. Unzip the file **bigtestdata.zip** in the **E:\\Demofiles\Mod04\Demo2** folder is **bigtestdata.json**
5. Using Internet Explorer, log in to the Azure portal using the account associated with your Azure pass.

### Use the Azure portal to create a new Data Lake Store

1. In the Azure portal, click **+ Create a Resource**, click **Storage**, and then click **Data Lake Storage Gen1**.
2. On the **New Data Lake Storage Gen1** blade, in the **Name** box, type dlstore&lt;your name&gt;&lt;date&gt;.
3. Under **Resource Group**, click **Create new**, and type **DataLakeRG**.
4. In the **Location** list, select your nearest location from the available regions.
5. Leave all other settings at their defaults, and then click **Create**. Wait until the storage has deployed before continuing with the demo.

### Use the Azure portal to create Data Lake Store folders and upload data

1. In the Azure portal, click **All resources**, and then click **dlstore&lt;your name&gt;&lt;date&gt;**.
2. On the **dlstore&lt;your name&gt;&lt;date&gt; Data Lake Storage Gen1** blade, click **Data explorer**, and then click **New Folder**.
3. In the **Create new folder** dialog box, type **Engineering**, and then click **OK**.
4. Repeat steps 2 and 3 to create three more folders named **Finance**, **Marketing**, and **Production**.
5. Click the **Engineering** folder, and then click **Upload**.
6. On the **Upload files** blade, click the file button (blue folder icon).
7. In the **Open**** dialog box, go to the folder **E:\\Demofiles\\Mod04\\Demo1**, click **testdata.json** and then click **Open**.
8. On the **Upload files** blade, click **Add selected files**.
9. After the file has uploaded, close the **Upload files** blade.
10. On the **dlstore&lt;your name&gt;&lt;date&gt;** blade, click testdata.json. The contents of the file are displayed in the **File preview** blade. Point out that the file contains some data about fictitious criminal offenses.
11. Close the **File Preview** blade.

### Use PowerShell to access Data Lake Store resources

1. On the Windows **Start** menu, type **PowerShell**, right-click **Windows PowerShell ISE**, and then click **Run as administrator**.
2. In the User **Account Control** dialog box, click **Yes**.
3. In the PowerShell ISE, on the **File** menu, click **Open**.
4. In the **Open** dialog box, go to the folder **E:\\Demofiles\\Mod04\\Demo1**, click **ADLS_listfiles.ps1**, and then click **Open**.
5. Explain the cmdlets in the script:

    ``` PowerShell
    # Log in to Azure
    Login-AzureRmAccount

    $dataLakeStoreName = "&lt;name of your Data Lake Store&gt;"

    # List all files and folders in the root directory
    $myrootdir = "/"
    Get-AzureRmDataLakeStoreChildItem –AccountName $dataLakeStoreName -Path $myrootdir

    # List the files in the Engineering directory
    $mydir="/Engineering"
    Get-AzureRmDataLakeStoreChildItem –AccountName $dataLakeStoreName -Path "$mydir"
    ```

6. Edit the script and replace the text **&lt;name of your Data Lake Store&gt;** with **dlstore&lt;your name&gt;&lt;date&gt;** (the name of the Data Lake Store that you created earlier in this demo).
7. Save the updated script.
8. Usnig the mouse, highlight lines 1-8 of the script, and then on the toolbar, click **Run Selection**.
9. In the **Sign in to your account** dialog box, enter the details of the Microsoft account that is associated with your Azure Learning Pass subscription, and then click **Sign in**. The script should display the details of the **Engineering**, **Finanance**, **Marketing**, and **Production** folders in your Data Lake Storage account.
10. Highlight lines 10-12, and then on the toolbar, click **Run Selection**. The script should show the details of the testdata.json file in the **Engineerig** folder

### Use the Azure portal to download data from Data Lake Store

1. Return to the Azure portal.
2. On the **dlstore&lt;your name&gt;&lt;date&gt;** blade, next to **testdata.json**, click the ellipses (...), and then click **Download**.
3. In the **Download** blade, click **Start Download now**.
4. In the Internet Explorer message box, click the **Save** drop-down arrow, and then click **Save as**.
5. In the **Save As dialog** box, go to the folder **E:\\\Demofiles\\Mod04**, in the **File name** box, type **testdatadownload.json**, and then click **Save**.
6. In the Internet Explorer message box, click **Open folder**.
7. In **File Explorer**, right-click **testdatadownload.json**, click **Open with**, and then click **Microsoft Visual Studio Version Selector** to open the file in Visual Studio.
8. Confirm that the file contains the same data about fictitious criminal offenses that you showed earlier.
9. Minimize Visual Studio.
10. In the Azure portal, close the Download pane.

## Demo 2: Working with Data Lake Store using PowerShell

This demonstration comprises two parts. In part 1 you will see how to:

- Use PowerShell to upload a single file directly to Data Lake Store.
- Use Data Explorer to verify the file upload.
- Repeat the file upload with an increased PerFileThreadCount.

In part 2, you will see how to:

- Use AzCopy to upload files to new Blob storage.
- Use Cloud Explorer in Visual Studio to examine the uploaded blob.
- Use AdlCopy to transfer files from Blob storage to Data Lake Store.

### Setup

1. Ensure that the **MT17B-WS2016-NAT**, **20776A-LON-DC**, and **20776A-LON-DEV** virtual machines are running, and then log on to 20776A-LON-DEV as **ADATUM\\AdatumAdmin** with the password **Pa55w.rd**.
2. Using **File Explorer**, move to the folder **E:\\Demofiles\Mod04\\Demo12**, right-click **Setup.cmd**, and then click **Run as administrator**.
3. In the **User Account Control** dialog box, click **Yes**.
4. Using Internet Explorer, log in to the Azure portal using the account associated with your Azure pass.

### Install AzCopy

1. In Internet Explorer, open a new tab, and go to **https://aka.ms/downloadazcopy**
2. In the Internet Explorer message box, click Run.
3. In the **Microsoft Azure Storage AzCopy** dialog box, on the **Welcome to the Microsoft Azure Storage AzCopy Setup Wizard** page, click **Next**.
4. On the **End-User License Agreement** page, select the **I accept the terms in the License Agreement** check box, and then click **Next**.
5. On the **Destination Folder** page, click **Next**.
6. On the **Ready to install Microsoft Azure Storage AzCopy** page, click **Install**.
7. In the **User Account Control** dialog box, click **Yes**.
8. On the **Completed the Microsoft Azure Storage AzCopy Setup Wizard** page, click **Finish**.
9. Right-click the Windows **Start** button, click **System**, and then click **Advanced system settings**.
10. In the **System Properties** dialog box, click **Environment Variables**.
11. In the **Environment Variables** dialog box, under **User variable for AdatumAdmin**, click **Path**, and then click **Edit**.
12. In the **Edit User Variable** dialog box, in the **Variable value** box, at the end of the existing text, type **C:\Program Files (x86)\Microsoft SDKs\Azure\AzCopy;**, and then click **OK**.
13. Leave the **Environment Variables** dialog box open.

### Install AdlCopy

1. In Internet Explorer, open a new tab, and go to **https://www.microsoft.com/en-us/download/details.aspx?id=50358**.
2. On the **AdlCopy** page, click **Download**.
3. In the Internet Explorer message box, click **Run**.
4. In the **AdlCopy** dialog box, on the **Welcome to the AdlCopy Setup Wizard** page, click **Next**.
5. On the **License Agreement** page, click **I Agree**, and then click **Next**.
6. On the **Select Installation Folder** page, click **Next**.
7. On the **Confirm Installation** page, click **Next**.
8. In the **User Account Control** dialog box, click **Yes**.
9. On the **Installation Complete** page, click **Close**.
10. Return to the **Environment Variables** dialog box.
11. Under **User variable for AdatumAdmin**, click **Path**, and then click **Edit**.
12. In the **Edit environment variable** dialog box, click **New**.
13. In the text box, type **%HOMEPATH%\Documents\AdlCopy**, and then click **OK**.
14. In the **Environment Variables** dialog box, click **OK**.
15. In the **System Properties** dialog box, click **OK**.

## Part 1—Use PowerShell to upload data to Data Lake Store

### Use PowerShell to upload a single file directly to Data Lake Store

1. Return to PowerShell ISE, and on the **File** menu, click **Open**.
2. In the **Open** dialog box, go to the folder **E:\Demofiles\Mod04\Demo2**, click **ADLS_upload.ps1**, and then click **Open**.
3. Explain the cmdlets in the script. In particular, highlight the additional parameter in the second upload command (PerFileThreadCount 30), which changes the number of threads used for the upload from the default (10) to 30:

    ```PowerShell
    # Log in to Azure
    Login-AzureRmAccount

    # Upload the file and note the time taken
    $dataLakeStoreName = "&lt;name of your Data Lake Store&gt;"
    $dest= "/Engineering/datafile10.json"
    $fileName = "E:\Demofiles\Mod04\Demo2\bigtestdata.json"
    $time = [System.Diagnostics.Stopwatch]::StartNew()
    $startTime = $time.Elapsed
    Import-AzureRmDataLakeStoreItem -Account $dataLakeStoreName -Path $fileName -
    Destination $dest
    $endTime = $time.Elapsed
    $elapsedTime = $endTime - $startTime
    write-host "Elapsed time: $($elapsedTime.ToString("hh\:mm\:ss"))"

    # Repeat the upload but with PerFileThreadCount set to 30
    # Note that the name of the destination file in the $dest variable is changed to
    prevent an "overwrite" error
    $dest = "/Engineering/datafile30.json"
    $startTime = $time.Elapsed
    Import-AzureRmDataLakeStoreItem -Account $dataLakeStoreName -Path $fileName -
    Destination $dest -PerFileThreadCount 30
    $endTime = $time.Elapsed
    $elapsedTime = $endTime - $startTime
    write-host "Elapsed time: $($elapsedTime.ToString("hh\:mm\:ss"))"
    ```

4. Edit the script, and replace the text  &lt;name of your Data Lake Store&gt; with dlstore&lt;your name&gt;&lt;date&gt; (the name of the Data Lake Store that you created in Demo 1).
5. Save the updated script.
6. Highlight lines 1-13, and then on the toolbar, click **Run Selection**. If prompted, sign in to your account using the details of the Microsoft account that is associated with your Azure Learning Pass subscriptio.
7. When the upload has completed, make a note of the elapsed time.

    &gt; **Note**: The upload may take a few minutes

### Use Data Explorer to verify the file upload

1. Switch to the Azure portal, click **All resources**, and then click **dlstore&lt;your name&gt;&lt;date&gt;**.
2. On the **dlstore&lt;your name&gt;&lt;date&gt;** blade, click **Data Explorer**.
3. Click the **Engineering** folder, and point out the uploaded **datafile10.json** file.

### Repeat the file upload with an increased PerFileThreadCount

1. Return to PowerShell ISE, highlight lines 15-22, and then on the toolbar, click **Run Selection**.
2. When the upload is complete, make a note of the elapsed time.
3. Switch to the Azure portal, and in the **Engineering** folder, point out the uploaded **datafile30.json** file.
5. Explain that, if the time taken for the second transfer is significantly lower than that of the first, then your network connection to Azure has been able to tak advantage of parallel upload threads. If your two transfer times are not significantly different, then the constraining factor in your environment is most likely to be the available network bandwidth rather than the parallelism of the operation—so you need to get a faster network connection!

## Part 2—upload a set of files to Data Lake Store from Blob storage:

### Use AzCopy to upload files to Blob storage

1. In the Azure portal, click **+ Create a resource**, click **Storage**, and then click **Storage account - blob, file, table, queue**.
2. On the **Create storage account** blade, in the **Name box, type **blobstore&lt;your name&gt;&lt;date&gt;**.
3. Under **Resource group**, click **Use existing**, and then click **DataLakeRG**.
4. In the **Location** list, select the same location as you used for the Data Lake Storage account.
5. Leave all other details at their defaults, and then click **Create**. Wait until the storage account has been successfully created before continuing with the demo.
6. Click **All resources**, and then click **blobstore&lt;your name&gt;&lt;date&gt;**.
7. On the **blobstore&lt;your name&gt;&lt;date&gt;** blade, under **Bob service**, click **Blobs**, and then click **+ Container**.
8. In the **New container** dialog box, in the **Name** box, type **datafeed**, and then click **OK**.
9. On the **blobstore&lt;your name&gt;&lt;date&gt; - Containers** blade, under **Settings**, click **Access keys**.
10. Next to the key for **key1**, click the **Click to copy** button, to copy the key to the clipboard.
11. Using **File Explorer** on the desktop, go to the folder **E:\\Demofiles\\Mod04\\Demo2\\TestdataTree**.
12. Show the TestdataTree folder and the subfolders. This is a simple hierarchical structure of folders and data files.
13. Right-click the Windows **Start** button, and then click **Command Prompt (Admin)**.
14. In the **User Account Control** dialog box, click **Yes**.
15. At the command prompt, type the following command (replacing &lt;storage account name&gt; with the name of the account that you created above—that is, blobstore&lt;your name&gt;&lt;date&gt;—and replacing &lt;access key&gt; with the key you copied to the clipboard), and then press Enter. Explain that the /S parameter is used to indicate that AzCopy should recursively traverse and copy subfolders:

    ```Cmd
    azcopy /Source:"E:\Demofiles\Mod04\Demo2\TestdataTree" /Dest:https://<storage account name>.blob.core.windows.net/datafeed /DestKey:<access key> /S
    ```

    You can copy this command from the file **E:\\Demofiles\\Mod04\\Demo2\\AZCopyCmd.txt**.

Verify that this command uploads 7 files.

### Use Cloud Explorer in Visual Studio to examine the uploaded blob

1. Return to Visual Studio, and on the **View** menu, click **Cloud Explorer**.
2. In **Cloud Explorer**, if the resources for your Azure Learning Pass subscription are not displayed, perform the following steps:

    - Click the **Azure Account settings** icon, and then click **Add an account**.
    - In the **Sign in to your account** dialog box, sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.
    - In **Cloud Explorer**, select your Azure Learning Pass subscription, and then click **Apply**.

3. In **Cloud Explorer**, under your Azure Learning Pass subscription folder, expand **Storage Accounts**, expand **blobstore&lt;your name&gt;&lt;date&gt;**, expand **Blob Containers**, and then double-click **datafeed.**
4. Point out that the files and folders Folder1, Folder2, and testdata.json have been uploaded, but Folder3 (which was empty) will be missing because AzCopy does not copy empty folders.

### Use AdlCopy to transfer files from Blob storage to Data Lake Store

1. Switch to the command prompt.
2. At the command prompt, type the following command replacing &lt;storage account name&gt; with
blobstore&lt;your name&gt;&lt;date&gt;, replacing &lt;name of your Data Lake Store&gt; with dlstore&lt;your name&gt;&lt;date&gt;, and replacing &lt;access key&gt; with the blob store key you copied to the clipboard), and press Enter. This command should transfer 7 files and folders.

    ```Cmd
    adlcopy /source https://<storage account name>.blob.core.windows.net/datafeed/ /dest adl://&lt;name of your Data Lake Store&gt;.azuredatalakestore.net/Copytest/ /sourcekey<access ke>
    ```

    You can copy this command from the file **E:\\Demofiles\\Mod04\\Demo2\\AdlCopyCmd.txt**.

3. If you are prompted, sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.
4. Explain that both the source and destination must end with a /; adlCopy is automatically recursive; there is no /S parameter.
5. Switch to the Azure portal
6. Click **All resources**, click **dlstore&lt;your name&gt;&lt;date&gt;**, and then click **Data Explorer**.
7. Click the **Copytest** folder, and point out the uploaded files and folders.

## Demo 3: Using Data Lake Store in an application

In this demonstration, you will see how to:

- Add a guest user to Azure AD.
- Use an app to test user access to Data Lake Store.
- Set and test permissions to Data Lake Store resources.

> **Note**: This demo requires you to have a second Microsoft account that is not associated with your Azure subscription. This email account will act as a guest user to your subscription. YOu can create a Microsoft account by following the instructions at **https://account.microsoft.com/account**.

### Add a guest user to Azure AD

1. In the Azure portal, click **Azure Active Directory**.
2. On the **Azure Active Directory** blade, in the right-hand pane, under **Create**, click **Guest user**.
3. On the **New Guest User**, in the **Enter address** box, type the email address of the demo guest account, and then click **Invite**.
4. Wait for the Microsoft Invitations email to arrive, and in this message click the **Get Started** link.
5. When prompted, enter the password for your Microsoft account.
6. In the **Review permissions** dialog box, click **Accept**

### Use an app to test user access to Data Lake Store

1. Switch to the Azure portal, and under **Manage**, click **Properties**.
2. On the **Properties** blade, next to **Directory ID**, click the **Click to copy button**, to copy the Directory ID to the clipboard.
3. Return to Visual Studio.
4. On the **File** menu, click **Open**, and then click **Project/Solution**.
5. In the **Open Project** dialog box, go to the **E:\\Demofiles\\Mod04\\DataLakeFileAccess** folder, click **DataLakeFileAccess.sln**, and then click **Open**. This is a Windows app that demonstrates security with a Data Lake Storage account.
6. On the toolbar, click **Start**.
7. In the **Data Lake File Access** app, in the **AAD Tenant ID** box, paste the **Directory ID** from the clipboard.
8. On the **Security** menu, click **Connect**.
9. In the **Sign in to your Microsoft account** dialog box, enter the credentials of your demo guest account, and then click **Sign in**.
10. In the **Data Lake Account Name** box, type **dlstore&lt;your name&gt;&lt;date&gt;**.
11. In the **Remote File** box, type **/Engineering/**.
12. On the **File** menu, click **List Files**. The **List Files Failure** dialog box displays an exception with the status code **Forbidden**; this is because the demo guest account does not have any permissions to the Data Lake Store.
13. Click **OK**.

### Set and test permissions to Data Lake Store resources

1. Switch to the Azure portal.
2. Click **All resources**, click **dlstore&lt;your name&gt;&lt;date&gt;**, and then click **Data Explorer**.
3. On the **dlstore&lt;your name&gt;&lt;date&gt;** blade, click **Engineering**, and then click **Access**.
4. On the **Access blade**, under **Everyone else**, select the **Read**, and **Execute** check boxes, and then click **Save**.
5. Switch back to the DataLakeFileAccess app.
6. On the **File** menu, click **List Files**. The **List Files Failure** dialog box still displays an exception with status code **Forbidden**; this is because you also need to grant access to the root folder as well. Click **OK**.
7. Return to the Azure portal.
8. Close the **Access** blade for the **Engineering** folder.
9.  On the **dlstore&lt;your name&gt;&lt;date&gt;** blade, select the root folder (**dlstore&lt;your name&gt;&lt;date&gt;**), and then click **Access**.
10. On the **Access** blade, under **Everyone else**, select the **Read**, and **Execute** check boxes, and then click **Save**.
11. Switch to the DataLakeFileAccess app.
12. On the **File** menu, click **List Files**. This time the app open a dialog box listing the contents of the Engineering folder. Click **OK**.
13. In the **Local Folder** box, type **E:\\Demofiles\\Mod04\\Demo3**.
14. In the **Local File Name** box, type **testdata.json**.
15. In the **Remote File** box, type **/Engineering/writetest.json**.
16. On the **File** menu, click **Upload**. The **Upload Failure** dialog box appears and displays an error because the demo guest account does not have Write permission on the folder. Click **OK**.
17. Switch to the Azure portal.
18. Close the **Access** pane for the root folder.
19. On the **dlstore&lt;your name&gt;&lt;date&gt;** blade, click **Engineering**, and then click **Access**.
20. On the **Access** blade, under **Everyone else**, select the **Write** check box (leave **Read** and **Execute** enabled), and then click **Save**.
21. Switch to the DataLakeFileAccess app.
22. On the **File** menu, click **Upload**. This time the app displays a **Success** dialog box with the message **File Uploaded**. Click **OK**.
23. Switch to the Azure portal.
24. Close the **Access** pane for the **Engineering** folder.
25. Point out the file **writetest.json** that is now available in the **Engineering** folder.

---

©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
