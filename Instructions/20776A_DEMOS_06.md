# Module 6: Implementing Custom Operations and Monitoring Performance in Azure Data Lake Analytics

- [Module 6: Implementing Custom Operations and Monitoring Performance in Azure Data Lake Analytics](#module-6-implementing-custom-operations-and-monitoring-performance-in-azure-data-lake-analytics)
    - [Demo 1: Creating and deploying a .NET Framework assembly to ADLA](#demo-1-creating-and-deploying-a-net-framework-assembly-to-adla)
        - [Scenario](#scenario)
        - [Setup](#setup)
        - [Use a code-behind file to create a .NET Framework assembly](#use-a-code-behind-file-to-create-a-net-framework-assembly)
        - [Manually create and deploy an assembly locally, for testing](#manually-create-and-deploy-an-assembly-locally-for-testing)
        - [Deploy an assembly to an ADLA account in Azure](#deploy-an-assembly-to-an-adla-account-in-azure)
    - [Demo 2: Building and using a user-defined extractor, outputter, and aggregator](#demo-2-building-and-using-a-user-defined-extractor-outputter-and-aggregator)
        - [Scenario:](#scenario)
        - [Create and use a custom extractor](#create-and-use-a-custom-extractor)
        - [Create and use a custom outputter](#create-and-use-a-custom-outputter)
        - [Create and use a custom aggregator](#create-and-use-a-custom-aggregator)
    - [Demo 3: Incorporating R, Python and cognitive capabilities into a U-SQL job](#demo-3-incorporating-r-python-and-cognitive-capabilities-into-a-u-sql-job)
        - [Scenario](#scenario)
        - [Incorporate R Code into a U-SQL job](#incorporate-r-code-into-a-u-sql-job)
        - [Call Python code from a U-SQL job](#call-python-code-from-a-u-sql-job)
        - [Add cognitive capabilities to a U-SQL job](#add-cognitive-capabilities-to-a-u-sql-job)
    - [Demo 4: Partitioning data in the ADLA catalog to optimize a U-SQL job](#demo-4-partitioning-data-in-the-adla-catalog-to-optimize-a-u-sql-job)
        - [Scenario](#scenario)
        - [Partition data in the ADLA catalog](#partition-data-in-the-adla-catalog)
        - [Assess the impact of incorrect data partitioning](#assess-the-impact-of-incorrect-data-partitioning)
        - [Use the Job View in Visual Studio to help optimize resource use](#use-the-job-view-in-visual-studio-to-help-optimize-resource-use)
    - [Demo 5: Creating and using user-defined operators](#demo-5-creating-and-using-user-defined-operators)
        - [Scenario](#scenario)
        - [Create and run a user-defined processor](#create-and-run-a-user-defined-processor)
        - [Create and run a user-defined applier](#create-and-run-a-user-defined-applier)
        - [Create and run a user-defined combiner](#create-and-run-a-user-defined-combiner)
        - [Create and run a user-defined reducer](#create-and-run-a-user-defined-reducer)
  
## Demo 1: Creating and deploying a .NET Framework assembly to ADLA

### Scenario

This demonstration returns to the stock market scenario. In an earler demonstration, you saw how to use Machine Learning with Stream Analytics to detect whether a price change is unusual for a stock item (much higher than might be expected given the price history of the stock). In this demonstration, you will implement similar functionality using C# code integrated into an ADLA job. Note that the C# code adopts an more algorithmic approach to detecting whether a price movement is suspicious than Machine Learning does; it might be less accurate, but it should be faster and consume fewer resources.

In this demonstration, you will see how to:

- Use a code-behind file to create a .NET Framework assembly.
- Manually create and deploy an assembly locally, for testing.
- Deploy an assembly to an ADLA account in Azure.

### Setup

1. Ensure that the **MT17B-WS2016-NAT**, **20776A-LON-DC**, and **20776A-LON-DEV** virtual machines are running, and then log on to 20776A-LON-DEV as **ADATUM\\AdatumAdmin** with the password **Pa55w.rd**.
2. Using **File Explorer**, go to the folder **E:\\Demofiles\\Mod06\\Demo4**, right-click the file **Sales.zip**, and then click **Extract All**.
3. In the **Extract Compressed (Zipped) Folders** dialog box, in the **Files will be extracted to the folder** box, type **E:\\Demofiles\\Mod06\\Demo4**, clear the **Show extracted files when complete** check box, and then click **Extract**. A file named **Sales.csv** should be extracted.

Additionally, if you have not performed the demos for module 5, complete the following steps:

1. Using Internet Explorer, go to the Azure portal at https://portal.azure.com, and sign in with the credentials of the account that is associated with your Azure Pass.
2. In the Azure portal, click **+ Create a resource**, click **Storage**, and then click **Data Lake Storage Gen 1**.
3. On the **New Data Lake Storage Gen 1** blade, in the **Name** box, type **stockstore&lt;your name&gt;&lt;date&gt;**.
4. Under **Resource group**, click **Create new**, and then type **StockMarketRG**.
5. In the **Location** list, select your nearest location from the currently available Data Lake Store regions.
6. Leave all other settings at their defaults, and then click **Create**.
7. Click **+ Create a resource**, click **Analytics**, and then click **Data Lake Analytics**.
8. On the **New Data Lake Analytics Account** blade, in the Name box, type **stocksdla&lt;your name&gt;&lt;date&gt;**.
9. Under **Resource group**, click **Use existing**, and then click **StockMarketRG**.
10. In the **Location** list, select the same location as you used for the Data Lake Store.
11. Under **Data Lake Storage Gen 1**, click **Configure required settings**.
12. On the **Select Data Lake Storage Gen 1** blade, click **stockstore&lt;your name&gt;&lt;date&gt;**
13. Leave all other settings at their defaults, and then click **Create**.
14. Wait until the account has deployed before continuing with the demo.

### Use a code-behind file to create a .NET Framework assembly

1. In File Explorer, go to the **E:\\Demofiles\\Mod06\\Demo1** folder, and double-click the **StockPriceData.csv** file to view it in Excel. The file contains stock price movemnt data, comprising a stock ticker, a new price, and the date and time for the new price.
2. Close Excel.
3. Copy the **StockPriceData.csv** file to the clipboard.
4. Go to the **C:\\Users\\AdatumAdmin\\AppData\\Local\\USQLDataRoot** folder, and paste the **StockPriceData.csv** file in this folder.

    > **Note** AppData is a hidden folder so it might not appear in **File Explorer** by default. TO show hidden items in File Explorer, click the **View** menu, and then check the **Hidden items** check box.

5. On the Windows **Start** menu, type **Visual Studio 2017**, and then press Enter.
6. In Visual Studio, on the **File** menu, click **Open**, and then click **Project/Solution**.
7. In the **Open Project** dialog box, go to the **E:\\Demofiles\\Mod06\\Demo1\\PriceMovementAnalysis-CodeBehind** folder, click **PriceMovementAnalysis.sln**, and then click **Open**.
8. In Solution Explorer, double-click **Script.usql**. Note the following points about the code in this file:
    - A database named **StockPrices** is created if it doesn't already exist.
    - The extractor reads the price movement data from the StockPriceData.csv file.
    - The WHERE clause in the SELECT statement calls the **SuspiciousMovement** function to detect whether each price movement is suspicious. You will see how this function is implemented shortly.
    - The outputter writes all suspicious records to a file named SuspiciousMovements.csv.
9. In Solution Explorer, expand **Script.usql**, and then double-click **Script.usql.cs**. This is a C# code-behind file that implements the SuspiciousMovement function. All functions must be static, and  they must belong to a class (in this case called **PriceAnalyzer**). Additionally, all classes should be part of a namespace (**PriceMovementAnalysis**).
10. Scroll down to the **SuspiciousMovement** function. This function implements the business logic shown in an earlier module to determine whether a stock price change is suspicous (has the price suddenly changed by a large percentage compared to the previous price). It returns a Boolean value.
11. Close the Script.usql.cs pane.
12. In the **Script.usql** pane, in the toolbar, ensure that the ADLA instance is set to **(Local)**, and then click **Submit**. The C# code will be compiled into an assembly called **PriceMovementAnalysis** and deployed to the **StockPrices** database in the catalog. The U-SQL script should then run.
13. When the job has finished, in the app console window, press Enter to continue.
14. In the **Local Run Results** pane, double-click the file **C:\\Users\\AdatumAdmin\\AppData\\Local\\USQLDataRoot\\SuspiciousMovements.csv.** This file contains a list of the suspicious stock trades identified by the job.
15. Close the **File Preview** pane.
16. On the **View** menu, click **Cloud Explorer**.
17. In **Cloud Explorer**, expand **(Local)**, expand **Data Lake Analytics**, expand **(Local)**, expand **Databases**, expand **StockPrices**, and then double-click **Assemblies**. This is where the assembly created for the C# code-behind was deployed, but it was removed when the job finished, so the **Assemblies** folder should be empty.

### Manually create and deploy an assembly locally, for testing

1. In Visual Studio, on the **File** menu, click **Open**, and then click **Project/Solution**.
2. In the **Open Project** dialog box, go to the folder **E:\\Demofiles\\Mod06\\Demo1\\PriceMovementAnalysis-Assembly** folder, click **MovementAnalysisAssembly.sln**, and then click **Open**.
3. In Solution Explorer, double-click **PriceAnalyzer.cs**. This file contains a copy of the same code that you demonstrated previously, except that it is now implemented as a separate project rather than as a code-behind.
4. On the **Build** menu, click **Build Solution**.
5. In Solution Explorer, right-click **MovementAnalysisAssembly**, and then click **Register Assembly**.
6. In the **Assembly Registration** dialog box, set the **ADLA Account** to **(Local)**, set the **Database** to **StockPrices**, select the **Replace assembly if it already exists** check box, and then click **Submit**. This action uploads the assembly to the ADLA catalog and makes it available to U-SQL scripts.
7. In **Cloud Explorer**, under **StockPrices**, right-click the **Assemblies** folder, and then click **Refresh**. The **MovementAnalysisAssembly** assembly should now be listed.
8. On the **File** menu, click **Open**, and then click **Project/Solution**.
9. In the **Open Project** dialog box, go to the folder **E:\\Demofiles\\Mod06\\Demo1\\PriceMovementAnalysis-Assembly**, click **PriceMovementAnalysis.sln**, and then click **Open**.
10. In Solution Explorer, double-click **Script.usql**. Note the following points in this script:

    - The REFERENCE ASSEMBLY statement at the start of the file references the assembly in the catalog, rather than using a code-behind (as in the previous version of this script).
    - The remainder of the code is the same as before.

11. In the **Script.usql** pane, in the toolbar, ensure that the **ADLA instance** is set to **(Local)**, and that the catalog is set to **StockPrices**, and then click **Submit**.
12. When the job has finished, in the app console window, press Enter to continue.
13. In the **Local Run Results** pane, double-click the file **C:\\Users\\AdatumAdmin\\AppData\\Local\\USQLDataRoot\\SuspiciousMovements.csv**. The file should contain the same data as before.
14. Close the **File Preview** pane.

### Deploy an assembly to an ADLA account in Azure

1. In Visual Studio, click **File**, click **Open**, and then click **Project/Solution**.
2. In the **Open Project** dialog box, go to the folder **E:\\Demofiles\\Mod06\\Demo1\\PriceMovementAnalysis-Assembly** folder, click **MovementAnalysisAssembly.sln**, and then click **Open**. This is the same project that you used in the previous task; it contains the **PriceAnalyzer** class that implements the **SuspiciousMovement** function.
3. in Solution Explorer, right-click **MovementAnalysisAssembly**, and then click **Register Assembly**.
4. In the **Assembly Registration** dialog box, set the **ADLA Account** to **stocksdla&lt;your name&gt;&lt;date&gt;**, set the **Database** to **StockPrices**, select the **Replace assembly if it already exists** check box, and then click **Submit**. This action runs a job that uploads the assembly to the ADLA catalog.
5. Using Internet Explorer, go to the Azure portal at https://portal.azure.com and sign in with the derentials of the account that is associated with your Azure Pass.
6. Click **All resources**, click **stockstore&lt;your name&gt;&lt;date&gt;**, and then click **Data explorer**.
7. In **Data explorer**, in the default (root) folder, click **Upload**.
8. On the **Upload files** blade, click the **Select a file** button.
9. In the **Open**** dialog box, go to the folder **E:\\Demofiles\\Mod06\\Demo1**, click **StockPriceData.csv**, and then click **Open**.
10. On the **Upload files** blade, click **Add selected files**.
11. After the file has uploaded, close the **Upload files** blade.
12. On the **Data explorer** blade, click **More**, and then click **Refresh**. The **StockProceData.csv** file should be listed.
13. Click  **catalog**, click **database**, click the subfolder (it has a GUID for a name), click **assembly**, and then click the subfolde. The **MovementAnalysisAssembly** assembly should be located in this folder in the catalog.
14. Return to Visual Studio.
15. On the **File** menu, click **Open**, and then click **Project/Solution**.
16. In the **Open Project** dialog box, go to the folder **E:\\Demofiles\\Mod06\\Demo1\\PriceMovementAnalysis-Assembly** folder, click **PriceMovementAnalysis.sln**, and then click **Open**.
17. In the Script.usql pane, in the toolbar, set the ADLA instance to **stocksdla&lt;your name&gt;&lt;date&gt;**, set the catalog to **StockPrices**, and then click **Submit**.
18. When the job has finished, close the **Job View** pane.
19. Switch to the Azure portal.
20. On the **Data explorer** blade, click **stocksdla&lt;your name&gt;&lt;date&gt;** to go to the root folder of the storage account
21. Click the **SuspiciousMovements.csv** file. This file should contain the same results as when you ran the job locally.
22. Close the **File Preview** blade.
23. Close Visual Studio.

## Demo 2: Building and using a user-defined extractor, outputter, and aggregator

### Scenario:

In this demonstration, you will see how to:

- Create and use a custom extractor. This extractor can retrieve fields from a JSON file. This is a common type of data input, but is not one of the input formats supported natively by USQL.
- Create and use a custom outputter. This outputter writes data as XML. Although it is commonly understood by many tools, XML is not one of the output formats supported natively by USQL.
- Create and use custom aggregators. The first aggregator, **PositiveAVG** calculates the average of values in a column, but discards negative and zero values from the calculation. The second aggregator, **SmoothedRateOfPriceChange**, revisits the stock market scenario and calculates the rate of change in price of a specified stock over a given period of time.

### Create and use a custom extractor

1. In File Explorer, go to the folder **E:\\Demofiles\\Mod06\\Demo2**, and copy the **offenderdata.json** file to the clipboard.
2. Go to the folder **C:\\Users\\AdatumAdmin\\AppData\\Local\\USQLDataRoot**, and paste the **offenderdata.json** file in this folder.
3. On the Windows **Start** menu, type **Notepad**, and then press Enter.
4. In Notepad, on the **File** menu, click **Open**.
5. In the **Open** dialog box, go to the folder **C:\\Users\\AdatumAdmin\\AppData\\Local\\USQLDataRoot**, click **All Files (\*.\*)**, click **offenderdata.json**, and then click **Open**. This file contains a single JSON array called offences, and each item in the array comprises the ForeName, LastName, Alias, and Offence for which various suspects have been arrested in the recent past.
6. Close Notepad.
7. On the Windows **Start** menu, type **Visual Studio 2017**, and then press Enter.
8. In Visual Studio, on the **File** menu, click **Open**, and then click **Project/Solution**.
9. In the **Open Project** dialog box, go to the **E:\\Demofiles\\Mod06\\Demo2\CustomFunctions** folder, click **CustomFunctions.sln**, and then click **Open**.
10. In **Solution Explorer**, expand **CustomExtractors**, and then double-click **JsonExtractor.cs** to open the file in the code editor window. This code implements a custom extractor that can retrieve fields from a JSON file. Note the following points in the JsonExtractor.cs file:
    - The JsonExtractor class extends the IExtractor base class.
    - The Extract function is an override that uses functions in the Newtonsoft JSON library to read the JSON data from the input line (this contains a single line from the input file).
    - After the data has been retrieved, and the fields in the input line extracted into an output row, this output row is returned to USQL.
    - The class is tagged with the [SqlUserDefinedExtractor(AtomicFileProcessing = true)] attribute; this is because the input data consists of a single object (a JSON array), and so its contents cannot be read by separate vertices in parallel.
    - The constructor specifies the name of the array to parse in the input (as there could possibly be multiple arrays in the same JSON file).
    - This class contains a range of supporting functions that do all the parsing, and so on; you should ignore these functions for the purposes of this demo.
11. On the **Build** menu, click **Build Solution**.
12. In **Solution Explorer**, right-click **CustomExtractors**, and then click **Register Assembly**.
13. In the **Assembly Registration** dialog box, set the **ADLA Account** to **(Local)**, leave the **Database** set to **master**, and then select the **Replace assembly if it already exists check box**.
14. Expand **Managed Dependencies**, select the **Newtonsoft.json** check box (this is the assembly containing the JSON Newtonsoft library, and which is used by this extractor), and then click **Submit**.
15. On the Windows **Start** menu, type **Visual Studio 2017**, and then press Enter, to open a second instance of **Visual Studio 2017**.
16. In Visual Studio, on the **File** menu, click **Open**, and then click **Project/Solution**.
17. In the **Open Project** dialog box, go to the **E:\\Demofiles\\Mod06\\Demo2\\UserDefinedFunctionsTest* folder, click **UserDefinedFunctionsTest.sln**, and then click **Open**.
18. In **Solution Explorer**, expand **ProcessWantedOffenders**, and then double-click **Script.usql**. This USQL script uses the custom extractor to read data from the offences array in the offenderdata.json file. Note two REFERENCE ASSEMBLY statements at the top of the file. The first statement specifies the CustomExtractors assembly that contains the JsonExtractor class; the second statement references any dependent assemblies for this class (Newtonsoft.Json in this case). The U-SQL job generates two output files. The first output file contains a copy of the offences data in CSV format; the second output file contains a count of the occurrences of each different offence.
19. In the **Script.usql** pane, in the toolbar, set the **ADLA instance** to **(Local)**, set the **catalog** to **master**, and then click **Submit**.
20. Verify that the script completes successfully, and then press any Enter to close the app console window.
21. Close the **Compile View** pane.
22. In the **Local Run Results** pane, double-click the file **C:\\Users\\AdatumAdmin\\AppData\\Local\\USQLDataRoot\\Offenders.csv**. This is the offender data that was read from the JSON file, but converted to CSV format.
23. Close the **File Preview** pane.
24. In the Local Run Results pane, double-click the **C:\\Users\\AdatumAdmin\\AppData\\Local\\USQLDataRoot\OffenceFrequencies.csv** file. This file shows how many times each offence has been recorded.
25. Close the **File Preview** pane.
26. Close the **Script.usql** pane.

### Create and use a custom outputter

1. Switch to the instance of Visual Studio 2017 that has the CustomFunctions solution open.
2. In **Solution Explorer**, expand **CustomOutputters**, and then double-click **XmlOutputter.cs**, to open the file in the code editor window. Note the following points in this file:
    - The SimpleXMLOutputter class extends the IOutputter base class.
    - The Output function is an override that formats a line of data as a simple XML object; the object tag to use is specified by the constructor.
    - The class is tagged with the [SqlUserDefinedOutputter(AtomicFileProcessing = false)] attribute; each line of output is a separate XML document, so the output can be generated by using multiple vertices running in parallel.
    - The Close method flushes and closes the output stream.
3. On the **Build** menu, click **Build Solution**.
4. In **Solution Explorer**, right-click **CustomOutputters**, and then click **Register Assembly**.
5. In the **Assembly Registration** dialog box, set the **ADLA Account** to **(Local)**, leave the **Database** set to **master**, select the **Replace assembly if it already exists** check box, and then click **Submit.**
6. Switch to the instance of Visual Studio 2017 that has the **UserDefinedFunctionsTest** solution open.
7. In **Solution Explorer**, expand **OutputterTest**, and then double-click **Script.usql**. In this script, note the following points:
    - The REFERENCE ASSEMBLY adds a reference to the CustomOutputters assembly (containing the SimpleXMLOutputter class).
    - The testData variable contains a small set of sample employee data.
    - The OUTPUT statement writes the data in the testData variable to an XML file.
8. In **Solution Explorer**, right-click **OutputterTest**, and then click **Set as Startup Project**
9. In the **Script.usql** pane, in the toolbar, set the ADLA instance to **(Local)**, set the catalog to **master**, and then click **Submit**.
10. Verify that the script completes successfully, and then press Enter to close the app console window.
11. Close the **Compile View** pane.
12. On the **File** menu, click **Open** and then click **File**.
13. In the **Open File** dialog box, go to the **C:\\Users\\AdatumAdmin\\AppData\\Local\\USQLDataRoot** folder, click **All Files (\*.\*)**, click **EmployeeData.xml**, and then click **Open**.
14. Examine the data in the file. It should contain employee data in XML format.
15. Close the **EmployeeData.xml** file.

    > **Note**: You can't open the file from the **Local Run Results** pane in Visual Studio because the results window does not understand the XML format.

16. Close the **Script.usql** pane.

### Create and use a custom aggregator

1. Switch to the instance of Visual Studio 2017 that has the **CustomFunctions** solution open.
2. In **Solution Explorer**, expand **CustomAggregations**, and then double-click the **PositiveAVG.cs** file, to open the file in the code editor window. Note the following points:
    - The PositiveAVG class extends the IAggregator base class.
    - The Init method intializes the variables used by the aggregator.
    - The Accumulate method examines the value provided for each row and, if the value is greater than zero, it adds it to an accumulator variable; the function also maintains a count of the number of values accumulated.
    - The Terminate method, called after the final row has been read and accumulated, calculates the average and returns it.
    - The class is tagged with the [SqlUserDefinedReducer(IsRecursive = false)] attribute; this operation is not associative.
3. On the **Build** menu, click **Build Solution**.
4. In **Solution Explorer**, right-click **CustomAggregations**, and then click **Register Assembly**.
5. In the **Assembly Registration** dialog box, set the **ADLA Account** to **(Local)**, leave the **Database** set to **master**, select the **Replace assembly if it already exists** check box, and then click **Submit**.
6. Switch to the instance of Visual Studio 2017 that has the **UserDefinedFunctionsTest** solution open.
7. In Solution Explorer, right-click **AggregatorTest**, and then click **Set as Startup Project**.
8. Expand **AggregatorTest**, and then double-click **Script.usql**. Note the following points:
    - The REFERENCE ASSEMBLY adds a reference to the CustomAggregations assembly (that contains the PositiveAVG class).
    - The testData variable contains a small set of sample employee data, including employees with negative and zero salaries.
    - The two SELECT statements invoke the PositiveAvg aggregator; the first statement calculates the average positive salary across the organization, and the second statement breaks the results down by department. Thes two statements use the AGG function to run the aggregator.
9. In the **Script.usql** pane, in the toolbar, set the ADLA instance to **(Local)**, set the catalog to **master**, and then click **Submit**.
10. Verify that the script completes successfully, and then press Enter to close the app console window.
11. Close the **Compile View** pane.
12. In the **Local Run Results** pane, double-click the file **C:\\Users\\AdatumAdmin\\AppData\\Local\\USQLDataRoot\\AverageSalary.csv**. This file reports that the average positive salary across the company is 131633.33333333334.
13. Close the **File Preview** pane.
14. In the **Local Run Results** pane, double-click the file **C:\\Users\\AdatumAdmin\\AppData\\Local\\USQLDataRoot\\AverageSalaryByDepartment.csv**. This file contains three rows showing the average positive salary for each department:

    ```Text
    AverageSalary   Dept
    66900           1
    108000          2
    220000          3
    ```

15. Close the **File Preview** pane.
16. Close the **Script.usql** pane.
17. Switch to the instance of Visual Studio 2017 that has the **CustomFunctions** solution open.
18. In **Solution Explorer**, expand **CustomAggregations**, and then double-click **SmoothedRateOfPriceChange.cs**, to open the file in the code editor window. Note the following points:
    - The Accumulate method accumulates the values for the fluctuations in price and the flow of time for each stock.
    - The Terminate method, called after the final row has been read and calculated, calculates the rate of change in price over the specified time unit (day, hour, or minute)
    - The time unit is specified as a parameter to the constructor, where D is day, H is hour, and M is minute.
    - The class is tagged with the [SqlUserDefinedReducer(IsRecursive = false)] attribute; this operation is not associative.
19. Switch to the instance of Visual Studio 2017 that has the **UserDefinedFunctionsTest** solution open.
20. In **Solution Explorer**, right-click **PriceMovementAnalysis**, and then click **Set as Startup Project**.
21. Expand **PriceMovementAnalysis**, and then double-click **Script.usql**. Note the following points:
    - The REFERENCE ASSEMBLY adds a reference to the CustomAggregations assembly that contains the PositiveAvg class.
    - The stockData variable references the stock market movement data file used in the earlier demonstration.
    - The SELECT statement invokes the SmoothedRateOfPriceChange aggregator three times, to calculate the rate of price change by day, then by hour, and then by minute.
22. In the **Script.usql** pane, in the toolbar, set the ADLA instance to **(Local)**, set the catalog to **master**, and then click **Submit**.
23. Verify that the script completes successfully, and then press Enter close the app console window.
24. Close the **Compile View** pane.
25. In the **Local Run Results pane**, double-click the file **C:\\Users\\AdatumAdmin\\AppData\\Local\\USQLDataRoot\\StockPriceDynamics.csv**. This file summarizes the rate of change for the price for each stock, by day, hour, and minute.
26. Close both instances of Visual Studio.

## Demo 3: Incorporating R, Python and cognitive capabilities into a U-SQL job

### Scenario

In this demonstration, you will see how to:

- Incorporate R code into a U-SQL job. The R code will perform a simple analysis of stock price movements.
- Call Python code from a U-SQL job. The Python code repeats the simple analysis of stock proce movements.
- Add cognitive capabilities to a U-SQL job. This job examines a set of photographs and incorporates facial recognition and emotion detection.

### Incorporate R Code into a U-SQL job

1. Using **File Explorer**, go to the folder **E:\\Demofiles\\Mod06\\Demo3**, and then double-click **StockPriceData.csv** to open the file in Excel.
2. Examine the data in this file. It contains stock price movement data, comprising the stock ticker, new price, and quote time.
3. Close Excel.
4. Switch to the Azure portal.
5. Click **All resources**, and then click **stocksdla&lt;your name&gt;&lt;date&gt;**.
6. On the **stocksdla&lt;your name&gt;&lt;date&gt;** blade, under **Getting Started**, click **Sample scripts**.
7. On the **stocksdla&lt;your name&gt;&lt;date&gt; - Sample scripts** blade, click **Install U-SQL Extensions**, and then click **OK**.
8. Wait for the extentions to be installed before continuing.
9. On the **stocksdla&lt;your name&gt;&lt;date&gt;** blade, click **Data explorer**.
10. In the **stocksdla&lt;your name&gt;&lt;date&gt; - Data explorer**, in the **stocksdla&lt;your name&gt;&lt;date&gt; (default)** folder, click **Upload**.
11. On the **Upload files** blade, click the **Select a file** button.
12. In the **Open** dialog box, go to the folder **E:\\Demofiles\\Mod06\\Demo3**, click **StockPriceData.csv**, and then click **Open**.
13. On the **Upload files** blade, select the **Allow overwrite existing files** check box, and then click **Add selected files**.
14. When the file has been uploaded, close the **Upload files** blade.
15. On the Windows **Start** menu, type **Visual Studio 2017**, and then press Enter.
16. On the **File** menu, click **Open**, and then click **Project/Solution**.
17. In the **Open Project** dialog box, go to the **E:\\Demofiles\\Mod06\\Demo3\\Extensions** folder, click **Extensions.sln**, and then click **Open**. If the **Project Upgrade** dialog box appears, click **No**.
18. In **Solution Explorer**, expand **RExtensions**, and double-click **InlineR.usql**, to open the file in the code editor window.
19. In the code editor window, note the reference to the ExtR assembly which provides access to the R extensions. The script reads the stock price data, and uses the R Reducer to run a piece of R code over that data. The R code produces a basic statistical summary that lists the ticker, opening price, lowest price, highest price, and closing price for each stock.

    If some students are familiar with R, they might notice that this code uses only the functions available in the base R and ScaleR packages. To keep this demonstration manageable, it does not attempt to install and use the dplyr package that would likely be used in the real world!

20. In the **InlineR.usql** pane, in the toolbar, set the ADLA instance to **stocksdla&lt;your name&gt;&lt;date&gt**;, set the catalog to **master**, and then click **Submit**.
21. Verify that the script completes successfully.
22. On the **Job Graph** tab, double-click **StockPriceAnalysis.csv** and examine the results. For each ticker, the opening and closing prices are displayed, together with the highest and lowest prices.
23. Close the **File Preview** pane.
24. Close the **Job View** pane.
25. Close the **InlineR.usql** script pane.
26. In Solution Explorer, double-click **ScriptR.usql**, to open the file in the code editor window. This script performs the same task as the previous script, except that the R code is now deployed as a separate script.
27. On the **File** menu, click **Open**, and then cick **File**.
28. In the **Open File** dialog box, go to the **E:\\Demofiles\\Mod06\\Demo3\\Extensions** folder, click **All Files (\*.\*)**, click **StockSummaryScript.R**, and then click Open. This file contains the R code run by the job.
29. Switch to the Azure portal.
30. In the **stocksdla&lt;your name&gt;&lt;date&gt - Data explorer** blade, in the **stocksdla&lt;your name&gt;&lt;date&gt; (default)** folder, click **Upload**.
31. On the **Upload files** blade, click the **Select a file** button.
32. In the **Open**** dialog box, go to the folder **E:\\Demofiles\\Mod06\\Demo3\\Extensions**, click **StockSummaryScript.R**, and then click **Open**.
33. On the **Upload files** blade, click **Add selected** files.
34. After the file has uploaded, close the **Upload files** blade.
35. Return to Visual Studio.
36. In the **ScriptR.usql** pane, in the toolbar, set the ADLA instance to **stocksdla&lt;your name&gt;&lt;date&gt;**, set the catalog to **master**, and then click **Submit**.
37. Verify that the script completes successfully.
38. On the **Job Graph** tab, double-click **StockPriceAnalysis.csv**. The StockPriceAnalysis.csv contains the same results as before.
39. Close the **File Preview**, **Job View**, **ScriptR.usql**, and **StockSummaryScript.R** panes.

### Call Python code from a U-SQL job

1. In **Solution Explorer**, expand **PythonExtensions**, and then double-click **InlinePython.usql**, to open the file in the code editor pane. This script uses Python code to perform the same operation as the R scripts shown previously. The script references the **ExtPython** assembly and uses the Python reducer to run the code.
2. In the **InlinePython.usql** pane, in the toolbar, set the ADLA instance to **stocksdla&lt;your name&gt;&lt;date&gt;**, set the catalog to **master**, and then click **Submit**.
3. Verify that the script completes successfully.
4. On the **Job Graph** tab, double-click **StockPriceAnalysis.csv**. Verify that the StockPriceAnalysis.csv contains the same results as before
5. Close the **File Preview**, **Job View**, and **InlinePython.usql** panes.

### Add cognitive capabilities to a U-SQL job

1. Using **File Explorer**, go to the folder **E:\\Demofiles\\Mod06\\Demo3**, and double-click **MSC13_Everald_03.jpg**.
2. In the **How do you want to open this file?** dialog box, click **Photos**, and then click **OK**.
3. In **Photos**, click the right arrow to view the next JPG file, and then click the right arrow again to view the final JPG file in the folder.
4. Close **Photos**.
5. Switch to the Azure portal.
6. In the **stocksdla&lt;your name&gt;&lt;date&gt - Data explorer** blade, in the **stocksdla&lt;your name&gt;&lt;date&gt; (default)** folder, click **Upload**.
7. On the **Upload files** blade, click the **Select a file** button.
8. In the **Open** dialog box, go to the folder **E:\\Demofiles\\Mod06\\Demo3**. Using the Ctrl key to select multiple files, click **MSC13_Everald_03.jpg**, **MSC13_Louis_04.jpg**, and **OFC16_Meeting_002.jpg**, and then click **Open**.
9. On the **Upload files** blade, click **Add selected** files.
10. After the files have uploaded, close the **Upload files** blade.
11. Switch to Visual Studio.
12. In Solution Explorer, expand **Emotion**, and then double-click **Script.usql**, to open the file in the code editor pane. This script uses Cognitive Services to perform facial recognition and emotion analysis over a set of JPG files. The script uses the EmotionExtractor to invoke Cognitive Services, which returns a rowset containing information about each face detected, and the emotions it has perceived.
13. In the **Script.usql** pane, in the toolbar, set the ADLA instance to **stocksdla&lt;your name&gt;&lt;date&gt;**, set the catalog to **master**, and then click **Submit**.
14. Verify that the script completes successfully.
15. On the **Job Graph** tab, double-click **EmotionAnalysis.csv**. The file contains four rows; one row for each face detected in each file, together with the emotion displayed by that face (and a level of confidence in this determination). Note that the code only detects two faces in the third file; facial analysis does not always recognize faces that appear in profile in images.
16. Close the **File Preview**, **Job View**, and **Script.usql** script panes.

## Demo 4: Partitioning data in the ADLA catalog to optimize a U-SQL job

### Scenario

You want to use ADLA to analyze product sales data. Your sales data contains the following fields:

    - The US state in which the sale was made,
    - A product id, 
    - The price per unit, and
    - The number of units sold.

You need to determine which products sell better in which US states. You currently have 10 million rows of sales data stored in a CSV file. You decide that, given the volume of records, it mighht be beneficial to upload this data to the ADLA catalog and partition it to make processing more efficient.

In this demonstration, you will see how to:

- Partition data in the ADLA catalog.
- Assess the impact of incorrect data partitioning.
- Partition data in the ADLA catalog.

### Partition data in the ADLA catalog

1. In **Visual Studio 2017**, on the **File** menu, click **Open**, and then click **File**.
2. In the **Open File** dialog box, go to the folder **E:\\Demofiles\\Mod06\\Demo4**, click **All Documents (\*.\*)**, click **Sales.csv**, and then click **Open**. This file contains 10 million rows of sales data.
3. Cloase the **Sales.csv** file.
4. Switch to the Azure portal.
5. In the **stocksdla&lt;your name&gt;&lt;date&gt - Data explorer** blade, in the **stocksdla&lt;your name&gt;&lt;date&gt; (default)** folder, click **Upload**.
6. On the **Upload files** blade, click the **Select a file** button.
7. In the **Open** dialog box, go to the folder **E:\\Demofiles\\Mod06\\Demo4**, click **Sales.csv**, and then click **Open**.
8. On the **Upload files** blade, click **Add selected files**.
9. After the file has uploaded, close the **Upload files blade**. Note that the upload might take a few minutes.
10. Switch to Visual Studio 2017.
11. On the **File** menu, click **Open**, and then click **Project/Solution**.
12. In the **Open Project** dialog box, go to the **E:\\Demofiles\\Mod06\\Demo4** folder, click **Partitioning.sln**, and then click **Open**.
13. In **Solution Explorer**, double-click **CreateOrdinaryTable.usql**. Note the following statements in this script:
    - The EXTRACT statement reads the sales data from the CSV file.
    - A catalog named SalesData is created, if it doesn't already exist.
    - A table named OrdinarySales is created; this table has the same schema as the CSV file. The table is indexed and the data is distributed by using the product ID; there are no partitions.
    - The data from the CSV file is copied into the table.
14. In the **CreateOrdinaryTable.usql** pane, in the toolbar, set the ADLA instance to **stocksdla&lt;your name&gt;&lt;date&gt;**, set the database to **master**, and then click **Submit**.
15. Verify that the script completes successfully, and then close the **Job View** pane.
16. On the **View** menu, click **Cloud Explorer**.
17. In **Cloud Explorer**, expand the account that is associated with your Azure Pass, expand **Data Lake Analytics**, expand **stocksdla&lt;your name&gt;&lt;date&gt;**, expand **Databases**, and verify that the **SalesData** database has been created. If the **SalesData** database is not shown, right-click **Databases**, and then click **Refresh**.
18. Expand **SalesData**, expand **Tables**, and verify that the database contains the **dbo.OrdinarySales** table created by the script.
19. In **Solution Explorer**, double-click **CreatePartitionedTable.usql**. This script is nearly identical to the previous example, except that it creates a table named **Sales** that is partitioned by US state. The script creates partitions for each state before copying the data.
20. In the **CreatePartitionedTable.usql** pane, in the toolbar, set the ADLA instance to **stocksdla&lt;your name&gt;&lt;date&gt;**, set the database to **master**, and then click **Submit**.
21. Verify that the script completes successfully, and then close the **Job View** pane.
22. In **Cloud Explorer**, right-click **Tables**, and then click **Refresh**. Verify that the **dbo.Sales table** has been created.

### Assess the impact of incorrect data partitioning

1. In **Solution Explorer**, double-click **QueryNonPartitionedData.usql**. This script generates a report listing the total value of sales by product ID for sales made in NY.
2. In the **QueryNonPartitionedData.usql** pane, in the toolbar, set the ADLA instance to **stocksdla&lt;your name&gt;&lt;date&gt;**, set the database to **SalesData**, set the number of AUs to 5, and then click **Submit**.
3. When the job has completed, examine the job graph. Focus on the work performed by the **SV1 Extract** stage. This stage runs 10 vertices, and reads 143.10 MB of data to perform its processing; this is the entire table.
4. Leave the **Job View** open for the next part of the demo.
5. In **Solution Explorer**, double-click **QueryPartitionedData.usql**. This script is identical to the previous script, except that it uses the partitioned table to generate the report data.
6. In the **QueryPartitionedData.usql** pane, in the toolbar, set the ADLA instance to **stocksdla&lt;your name&gt;&lt;date&gt;**, set the database to **salesData**, set the number of AUs to 5, and then click **Submit**.
7. When the job has completed, review the job graph again. This time, the **SV1 Extract** stage runs only two vertices, and reads 19.08 MB of data to perform its processing—this is just over 20 percent of the table.
8. Leave this Job View window open (you will need both Job Views in the next part of the demo) and switch to the Azure portal.
9. In Data Explorer, note that there are two new files; **NYOrdinarySales.csv**, which was generated from the nonpartitioned data, and **NYSales.csv**, which was generated from the partitioned data. Both files should be the same size and contain the same data, although the order of the rows in each file might be different).

    **Question**:
    What conclusions could be drawn from comparing these two scripts, and their results?

    **Answers** (should include):
    - Partitioning the data leads to a dramatic decrease in I/O.
    - A greater number of vertices does not always indicate improved efficiency. In this case, the job that retrieved the partitioned data only used two vertices, because that is all it needed. The data was spread across fewer file extents, so the job did not require a large number of vertices to perform lots of unnecessary parallel I/O.

### Use the Job View in Visual Studio to help optimize resource use

1. Return to Visual Studio.
2. Click the **Job View** for the **QueryNonPartitionedData** job, and then click **Load Profile**. This action downloads additional information about how the job was run; this is stored as diagnostic information in your ADLA account.
3. Right-click the **SV1 Extract** stage, and then click **Show Vertex Execution View**. The bar chart showing the progress of the 10 vertices that were used to perform this stage. Only the top five vertices are marked with the **Creating and Queuing** phases, and the lower five are marked as **Waiting for Resources** at this point. This is because, by default, the job runs using five AUs, so a maximum of five vertices will execute simultaneously—the remaining vertices reuse AUs as they become available. The menu on the left enables you to focus on vertices that have failed or are running close to the AU resource limit.
4. Click the left arrow to return to the **Job View** for the **QueryNonPartitionedData** job.
5. Right-click the **SV1 Extract** stage, and then click **Show Stage Scatter View**. This view displays a scatter graph illustrating the amount of I/O performed by each vertex. Each vertex should have read 13-16 MB of data, and written approximately 0.003 MB. However, the time taken by the first five vertices is significantly greater than the time taken by the second five. There could be many reasons for this, including caching and other forms of buffering that occurred intially and that caused subsequent operations to perform I/O more efficently. However, the key point is that the amount of data read and written is roughly consistent across all vertices, so there is little sign of any skew.
6. Click one of the points on the graph, and then click **Vertex Detail View**. The diagram is divided into two sections. The left side is a depiction of the entire job, and the right side shows the processing performed by this vertex. There are three distinct operations: the data extraction when the data is read from the data source; a filter that applies the 'state == "NY"' predicate to the data; and a grouper that performs the group by operation. The results are passed to the next stage for combining with the data retrieved and processed by the other vertices. The summary at the top of the view gives the details for the amount of I/O performed, and the runtime used, by the vertex.
7. Close the **VertexDetailView** pane.
8. Click the left arrow to return to the **Job View** for the **QueryNonPartitionedData** job.
9. Right-click the **SV1 Extract** stage, and then click **Show Vertex Operator View.** This view shows the high-level operations performed by each vertex in this stage; the information is similar to the **Detail Vertex View**, except that it summarizes the row count of each operation across all the vertices.
10. Close the **View Vertex pane**.
11. Click the **Job View** for the **QueryPartitionedData** job, and then click **Load Profile**. Note the warning message that is displayed, and then click **View Diagnostics**.
12. On the **Diagnostics** pane, under **AU ANALYSIS**, the warning message indicates AU over-allocation; you could be paying for AUs that you are not using
13. Click the **Investigate** tab. The job was run with five AUs, but this job only used two of them. The **AU Usage** graph shows how many AUs were used by the job, and when.
14. At the bottom of the pane, set the **Select custom AUs value** slider to 2. Examine the figures in the **Custom** box over the slider. The number of **AU Hours** should be 0.005, and the **Run Time** should be 9 seconds.
15. Set the slider to 3 AUs. The run time should remain the same, but **AU Hours** increases to 0.008.
16. Set to the slider to 1 AU. The **AU Hours** drops back to 0.005, but the run time increases to 17 seconds. Using the slider enables you to see the probable effects of increasing or decreasing the number of AUs for the job. The statistics indicate that two AUs would be optimal.
17. Click **Job Graph** tab.
18. Right-click the **SV1 Extract** stage, and then click **Show Vertex Execution View**. This view confirms that only two vertices were used.
19. Click the left arrow to return to the **Job View** for the **QueryPartitionedData** job.
20. Right-click the **SV1 Extract** stage, and then click **Show Stage Scatter View**. The amount of I/O performed by each vertex is similar to that of the nonpartitioned job, in approximately the same time as each of the five slowest vertices. However, this time there are only two vertices, so the job is only performing 20 percent of the I/O of the previous job to obtain the same results.
21. Close both Job View panes.
22. Click the **QueryPartitionedData.usql** pane, set the number of AUs to 2, click **Submit** and wait for the job to complete.
23. In the **Job View** pane, click **Load Profile**. This time, there should be no warnings about overallocating AUs.
24. Close the **Job View** pane.

## Demo 5: Creating and using user-defined operators

### Scenario

In this demonstration, you will see how to create and run a:

- User-defined processor
- User-defined applier
- User-defined combiner
- User-defined reducer

### Create and run a user-defined processor

1. In **File Explorer**, go to the **E:\\Demofiles\\Mod06\\Demo5** folder, and copy the **Departments.tsv**, **EmployeeWorkHistory.tsv**, and **StockPriceData.csv** files to the clipboard.
2. Go to the **C:\\Users\\AdatumAdmin\\AppData\\Local\\USQLDataRoot** folder, and paste the files into this folder, replacing any existing files with the same names.
3. Switch to Visual Studio 2017.
4. On the **File** menu, click **Open**, then click **Project/Solution**.
5. In the **Open Project** dialog box, go to the **E:\\Demofiles\\Mod06\\Demo5\\CustomOperations** folder, click **CustomOperations.sln**, and then click **Open**.
6. In **Solution Explorer**, expand **CustomOperators**, and then double-click **CustomProcessor.cs**. This is the code for the custom processor described in the lesson notes. This code examines each input row (containing stock price data), and uses the logic shown in previous demos to detect whether the price change is anomalous; if so, the data is tagged with an 'X' in the Suspicious column that is output with the data.
7. Note the following points in the CustomProcessor.cs file:
    - The FlagRowsForInvestigationclass is tagged with the SqlUserDefinedProcessor attribute; this is optional, but good practice.
    - The FlagRowsForInvestigation class extends the IProcessor abstract base class.
    - The FlagRowsForInvestigation class overrides the Process method; this is where the row-by-row processing occurs.
8. On the **Build** menu, click **Build Solution**.
9. In **Solution Explorer**, right-click **CustomOperators**, and then click **Register Assembly**.
10. In the **Assembly Registration** dialog box, set the **ADLA Account** to **(Local)**, leave the **Database** set to **master**, select the **Replace assembly if it already exists** check box, and then click **Submit**.
11. On the WIndows **Start** menu, type **Visual Studio 2017**, and then press Enter, to open a second instance of Visual Studio 2017.
12. On the **File** menu, click **Open**, and then click **Project/Solution**.
13. In the **Open Project** dialog box, go to the **E:\\Demofiles\\Mod06\\Demo5\\UserDefinedOperatorsTest** folder, click **UserDefinedOperatorsTest.sln**, and then click **Open**. In the **Project Upgrade** dialog box, click **No**.
14. In **Solution Explorer**, expand **ProcessorTest**, and then double-click **Script.usql**, to open the file in the code editor pane. This USQL script uses the custom processor. Note the use of the PROCESS statement to invoke the processor.
15. In the **Script.usql** pane, in the toolbar, set the ADLA instance to **(Local)**, set the database to **master**, and then click **Submit**.
16. Verify that the script completes successfully, press Enter to close the app console window, and then close the **Compile View** and **Script.usql** panes.
17. On the **File** menu, click **Open**, and then click **File**.
18. In the **Open File** dialog box, go to the **C:\\Users\\AdatumAdmin\\AppData\\Local\\USQLDataRoot** folder, click **All Files (\*.\*)**, click **Movements.csv**, and then click **Open**. This file contains the stock price data that was read from the CSV file. A small number of rows have been tagged as suspicious—these are the rows that have "X" in the last column.
19. Close  **Movements.csv**.

### Create and run a user-defined applier

1. Switch to the instance of Visual Studio 2017 that has the **CustomOperations** solution open.
2. In **Solution Explorer**, double-click **CustomApplier.cs**. This code is a custom applier that examines each stock price movement, and outputs the opening price and the percentage price change since the last movement. This applier is designed to work with the CROSS APPLY operator in U-SQL. Note the following statements in CustomApplier.cs:
    - The class is tagged with the SqlUserDefinedApplier attribute; as before, this is optional, but good practice.
    - The GetStockAnalytics class extends the IApplier abstract base class.
    - The GetStockAnalytics class overrides the Apply method; this is where the row-by-row processing occurs.
3. Switch to the instance of Visual Studio 2017 that has the **UserDefinedOperatorsTest** solution open.
4. In **Solution Explorer**, right-click **ApplierTest**, and then click **Set as Startup Project**.
5. Expand **ApplierTest**, and then double-click **Script.usql**. Note the use of the applier in the CROSS APPLY clause of the SELECT statement.
6. In the **Script.usql** pane, in the toolbar, set the ADLA instance to **(Local)**, set the database to **master**, and then click **Submit**.
7. Verify that the script completes successfully, press Enter to close the app console window, and then close the **Compile View** and **Script.usql** panes.
8. In the **Local Run Results** pane, double-click **C:\\Users\\AdatumAdmin\\AppData\\Local\\USQLDataRoot\\PriceChanges.csv**. This file shows the opening price of each stock, and the percentage change since the previous price for each movement.
9. Close the **File Preview** pane.

### Create and run a user-defined combiner

1. In Visual Studio, on the **File** menu, click **Open**, and then click **File**.
2. In the **Open File** dialog box, go to the **C:\\Users\\AdatumAdmin\\AppData\\Local\\USQLDataRoot** folder, click **All Files (\*.\*)**, click **EmployeeWorkHistory.tsv**, and then click **Open**. This file contains employee data showing which employees work for which departments in a large organization. The final field contains a series of CSV values listing the roles that the employee has performed while working for a particular department.
3. Close the **EmployeeWorkHistory.tsv** file.
4. On the **File** menu, click **Open**, and then click **File**.
5. In the **Open File** dialog box, go to the folder **C:\\Users\\AdatumAdmin\\AppData\\Local\\USQLDataRoot**, click **All Files (\*.\*)**, click **Departments.tsv**, and then click **Open**. This file contains a list of department IDs and their names.
6. Close the **Departments.tsv** file.
7. Switch to the instance of Visual Studio 2017 that has the **CustomOperations** solution open.
8. In **Solution Explorer**, double-click **CustomCombiner.cs**. This code is a custom combiner that can be used to join employee and department data; the combiner parses the comma separated list of roles for each employee, and then outputs the data on separate rows, transforming the data into relational format. Note the following statements in CustomCombiner.cs:
    - The class is tagged with the SqlUserDefinedCombiner attribute with a mode of CombinerMode.Inner—the combiner performs an inner join.
    - The FindDepartmentRoles class extends the ICombiner abstract base class.
    - The FindDepartmentRoles class overrides the Combine method to perform the join operation and format rows for output.
9. Switch to the instance of Visual Studio 2017 that has the **UserDefinedOperatorsTest** solution open.
10. In **Solution Explorer**, right-click **CombinerTest**, and then click **Set as Startup Project**.
11. Expand **CombinerTest**, and then double-click **Script.usql**. Note the use of the combiner in the COMBINE statement, and that the relationship between the two tables is specified by the ON clause.
12. In the **Script.usql** pane, in the toolbar, set the ADLA instance to **(Local)**, set the database to **master**, and then click **Submit**.
13. Verify that the script completes successfully, press Enter to close the app console window, and then close the **Compile View** and **Script.usql** panes.
14. In the **Local Run Results** pane, double-click **C:\\Users\\AdatumAdmin\\AppData\\Local\\USQLDataRoot\\EmployeeRoles.csv**. Note the data in the EmpID, EmpName, DeptName, and Role columns; if an employee has several roles, each one is output as a new row.
15. Close the **File Preview** pane.

### Create and run a user-defined reducer

1. Switch to the instance of Visual Studio 2017 that has the **CustomOperations** solution open.
2. In **Solution Explorer**, double-click **CustomReducer.cs**. This code is a custom reducer that calculates the historical number of employees in each role in each department, and outputs the aggregated results. Note the following statements in CustomReducer.cs:
    - The class is tagged with the SqlUserDefinedReducer attribute with the IsRecursive parameter set to true; this is an associative operation.
    - The ReduceByRoleclass extends the IReducer abstract base class.
    - The ReduceByRoleclass overrides the Reduce method; this is where the aggregation occurs.
3. Switch to the instance of Visual Studio 2017 that has the **UserDefinedOperatorsTest** solution open.
4. In **Solution Explorer**, right-click **ReducerTest**, and then click **Set as Startup Project**.
5. Expand **ReducerTest**, and then double-click **Script.usql**. Note the use of the applier in the REDUCE statement.
6. In the **Script.usql** pane, in the toolbar, set the ADLA instance to **(Local)**, set the database to **master**, and then click **Submit**.
7. Verify that the script completes successfully, press Enter close the app console window, and then close the **Compile View** and **Script.usql** panes.
8. In the **Local Run Results** pane, double-click **C:\\Users\\AdatumAdmin\\AppData\\Local\\USQLDataRoot\\DepartmentRoleSummary.csv**.
9. Review the aggregated data for each department.
10. Close all instances of Visual Studio.

---

©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.