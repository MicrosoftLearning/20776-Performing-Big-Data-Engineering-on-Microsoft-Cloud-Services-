# Module 5: Processing big data using Azure Data Lake Analytics

- [Module 5: Processing big data using Azure Data Lake Analytics](#module-5-processing-big-data-using-azure-data-lake-analytics)
    - [Demo 1: Creating and running a Data Lake Analytics job](#demo-1-creating-and-running-a-data-lake-analytics-job)
        - [Scenario](#scenario)
        - [Setup](#setup)
        - [Create and run a job using the Azure portal](#create-and-run-a-job-using-the-azure-portal)
        - [Run a job using PowerShell](#run-a-job-using-powershell)
        - [Run a job using Data Lake Tools for Visual Studio](#run-a-job-using-data-lake-tools-for-visual-studio)
    - [Demo 2: Performing an analysis using U-SQL](#demo-2-performing-an-analysis-using-u-sql)
        - [Scenario](#scenario)
        - [Use an EXTRACT statement to combine multiple job inputs using virtual fields](#use-an-extract-statement-to-combine-multiple-job-inputs-using-virtual-fields)
        - [Perform parallel data saving to a Lake Store catalog and CSV file](#perform-parallel-data-saving-to-a-lake-store-catalog-and-csv-file)
        - [Test a job before deploying to the cloud](#test-a-job-before-deploying-to-the-cloud)
    - [Demo 3: Grouping and analyzing data](#demo-3-grouping-and-analyzing-data)
        - [Scenario](#scenario)
        - [Setup](#setup)
        - [Use windowing functions and create a catalog view](#use-windowing-functions-and-create-a-catalog-view)
        - [Create an external data source that references a SQL Database credential](#create-an-external-data-source-that-references-a-sql-database-credential)
        - [Use a federated query to join data from SQL Database with data retrieved from the ADLA catalog](#use-a-federated-query-to-join-data-from-sql-database-with-data-retrieved-from-the-adla-catalog)
  
## Demo 1: Creating and running a Data Lake Analytics job

### Scenario

You have a file, **StockPrices.csv**, that contains the stock price movements for a number of stocks recorded over a period of time. The data contains the stock ticker, the new price, and the time (the hour of the day) at which the stock's price changed. Stock prices can move up and down over time. You want to create a report that shows the maximum price achieved by each stock. You want the results to be ordered by ticker.

In this demonstration, you will see how to perform this task in three different ways. You will:

- Create and run a job using the Azure portal.
- Run a job using PowerShell.
- Run a job using the Data Lake Tools for Visual Studio.

### Setup

1. Ensure that the **MT17B-WS2016-NAT**, **20776A-LON-DC**, and **20776A-LON-DEV** virtual machines are running, and then log on to 20776A-LON-DEV as **ADATUM\\AdatumAdmin** with the password **Pa55w.rd**.
2. Using **File Explorer**, move to the folder **E:\\Demofiles\Mod05\\Demo1**, right-click **Setup.cmd**, and then click **Run as administrator**.
3. In the **User Account Control** dialog box, click **Yes**.
4. Using Internet Explorer, log in to the Azure portal using the account associated with your Azure pass.
5. In the Azure portal, click **+ Create a resource**, click **Storage**, and then click **Data Lake Storage Gen 1**.
6. On the **New Data Lake Storage Gen 1** blade, in the **Name** box, type **stockstore&lt;your name&gt;&lt;date&gt;**.
7. Under **Resource group**, click **Create new**, and then type **StockMarketRG**.
8. In the **Location** list, select your nearest location from the currently available Data Lake Store regions.
9. Leave all other settings at their defaults, and then click **Create**.
10. Wait until the storage has deployed before continuing with the demo.
11. In the Azure portal, click **All resources**, click **stockstore&lt;your name&gt;&lt;date&gt;**, and then click **Data Explorer**.
12. On the **stockstore&lt;your name&gt;&lt;date&gt;** blade, click **New Folder**.
13. In the **Create new folder** dialog box, type **StockMarket**, and then click **OK**.
14. Click the **StockMarket** folder, and then click **Upload**.
15. On the **Upload files** blade, click the **Select a file** button.
16. In the **Open** dialog box, go to the **E:\\Demofiles\\Mod05\\Demo1** folder, click **StockPrices.csv**, and then click **Open**.
17. On the **Upload files** blade, click **Add selected files**.
18. After the file has uploaded, close the **Upload files** blade.
19. On the **stockstore&lt;your name&gt;&lt;date&gt;** blade, click **StockPrices.csv**. This file contains some information about stock price movements (tickers, prices, and hour of day). The file contains over 19000 records.
20. Close the **File Preview** blade.

### Create and run a job using the Azure portal

1. In the Azure portal, click **+ Create a resource**, click **Analytics**, and then click **Data Lake Analytics**.
2. On the **New Data Lake Analytics Account** blade, in the Name box, type **stocksdla&lt;your name&gt;&lt;date&gt;**.
3. Under **Resource group**, click **Use existing**, and then click **StockMarketRG**.
4. In the **Location** list, select the same location as you used for the Data Lake Store.
5. Under **Data Lake Storage Gen 1**, click **Configure required settings**.
6. On the **Select Data Lake Storage Gen 1** blade, click **stockstore&lt;your name&gt;&lt;date&gt;**
7. Leave all other settings at their defaults, and then click **Create**.
8. Wait until the account has deployed before continuing with the demo.
9. Click **All resources**, click **stocksdla&lt;your name&gt;&lt;date&gt;**, and then click **+ New Job**.
10. On the **New Job** blade, in the **Job name** box, type **Ticker price**.
11. In the **AUs** box, type 2.
12. In the code pane, type the following U-SQL code:

    ```SQL
    @priceData =
        EXTRACT Ticker string,
                Price int,
                HourOfDay int
        FROM "/StockMarket/StockPrices.csv"
        USING Extractors.Csv(skipFirstNRows: 1);

    @maxPrices =
        SELECT Ticker, MAX(Price) AS MaxPrice
        FROM @priceData
        GROUP BY Ticker;

    OUTPUT @maxPrices
        TO "/output/MaxPrices.csv"
        USING Outputters.Csv(outputHeader: true);
    ```

    You can copy this code from the file **E:\\Demofiles\\Mod05\\Demo1\\USQLcode.txt**.

13. On the **New Job** blade, click **Submit**.
14. On the **Ticker price** blade, notice the **Preparing**, and **Queued** steps.
15. When the job enters the Running phase, notice that the job details show how the job has been broken down into vertices (there will only be one vertex for this simple job).
16. On the **Ticker price** blade, click **Data**, click **Output**, and then click **MaxPrices.csv**. This file contains the results of the job—specifically, the maximum price for each stockticker.
17. Close the **File Preview** blade.
18. Click **All resources**, click **stockstore&lt;your name&gt;&lt;date&gt;**, and then click **Data Explorer**.
19. On the **stockstore&lt;your name&gt;&lt;date&gt;** blade, click **output**, and notice that this is where the results file is located (as specified in the OUTPUT statement in the U-SQL code).
20. Next to **MaxPrices.csv**, click the ellipses (...), and then click **Delete File**.
21. In the **Delete** dialog box, click **OK**.

### Run a job using PowerShell

1. On the Windows **Start** menu, type **PowerShell ISE**, right-click **Windows PowerShell ISE**, and then click **Run as administrator**.
2. In the **User Account Control** dialog box, click **Yes**.
3. On the **File** menu, click **Open**.
4. In the **Open** dialog box, go to the folder **E:\\Demofiles\\Mod05\\Demo1**, click **SubmitJob.ps1**, and then click **Open**.
5. Examine the script. This script logs in to your Azure account and retrieves the details of you ADLA account. The script then submits a USQL job using the code in the StockPriceJob.usql file and waits for the job to complete before downloading the results from your Data Lake Storage account to the MaxPrices.csv file in the Demo1 folder. The file StockPriceJob.usql contains the same code used in the previous demo. You can examine this file by using Notepad if you wish:

    ```PowerShell
    # Log in to Azure
    Login-AzureRmAccount

    $adla = "&lt;name of your ADLA account&gt;"
    $adls = "<name of your Data Lake Store>"
    $rg = "StockMarketRG"

    # Verify that the ADLA account exists
    Get-AdlAnalyticsAccount -ResourceGroupName $rg -Name $adla

    # Submit an ADLA job
    $scriptpath = "E:\Demofiles\Mod05\Demo1\StockPriceJob.usql"
    $job = Submit-AdlJob -AccountName $adla –ScriptPath $scriptpath -Name "Demo"

    # Repeatedly view the status of the job
    $job = Get-AdlJob -AccountName $adla -JobId $job.JobId

    # Wait for the job to complete
    Wait-AdlJob -Account $adla -JobId $job.JobId

    # Download the results
    Export-AdlStoreItem -AccountName $adls -Path "/output/MaxPrices.csv" -Destination "E:\Demofiles\Mod05\Demo1\MaxPrices.csv"
    ```

6. Edit the script and replace **&lt;name of your ADLA account&gt;** with **stocksdla&lt;your name&gt;&lt;date&gt;**.
7. Replace **&lt;name of your Data Lake Store&gt;** with **stockstore&lt;your name&gt;&lt;date&gt;**.
8. On the **File** menu, click **Save**.
9. Highlight lines 1-9 using the mouse, and then on the toolbar, click **Run Selection**.
10. In the **Sign in to your account** dialog box, sign in using the details of the Microsoft account that is associated with your Azure Learning Pass subscription.
11. Examine the details of the Data Lake Analytics account that are returned by the **Get-AdlAnalyticsAccount** cmdlet.
12. In the script, highlight lines 11-22, and then on the toolbar, click **Run Selection**.
13. When the script has finished, examine the job details that have been returned.
14. Open the file **E:\\Demofiles\\Mod05\\Demo1\\MaxPrices.csv** using Excel. This file contains the same results as the previous demo.
15. Close Excel, and then close the PowerShell ISE.

### Run a job using Data Lake Tools for Visual Studio

1. Start Visual Studio 2017.
2. On the **File** menu, click **New**, and then click **Project**.
3. In the **New Project** dialog box, in the **Templates** list, expand **Azure Data Lake**, click **U-SQL (ADLA)**, and then click **OK**.
4. In the Script.usql pane, type the following code (you can copy this code from the file **E:\\Demofiles\\Mod05\\Demo1\\StockPriceJob.usql)**:

    ```SQL
    @priceData =
        EXTRACT Ticker string,
                Price int,
                HourOfDay int
    FROM "/StockMarket/StockPrices.csv"
    USING Extractors.Csv(skipFirstNRows: 1);

    @maxPrices =
        SELECT Ticker, MAX(Price) AS MaxPrice
        FROM @priceData
        GROUP BY Ticker;

    OUTPUT @maxPrices
    TO "/output/MaxPrices.csv"
    USING Outputters.Csv(outputHeader: true);
    ```

5. On the **View** menu, click **Cloud Explorer**.
6. In **Cloud Explorer**, if the account associated with your Azure Learning Pass subscription folder is not listed, perform the following steps:

    1. Click the **Azure Account settings** icon, and then click **Add an account**.
    2. In the **Sign in to your account** dialog box, sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.
    3. In **Cloud Explorer**, select your Azure Learning Pass subscription, and then click **Apply**.

7. In the **Script.usql** pane, in the **(Local)** list, click **stocksdla&lt;your name&gt;&lt;date&gt;**, and then click **Submit**. Note that Visual Studio Tools for Azure Data Lake Analytics provides a similar interface to that used in the Azure portal.
8. When the job has completed, on the **Job Graph** tab, double-click MaxPrices.csv. Note that, to see the file name in the Job Graph pane, you might need to close other panes, such as Solution Explorer.
9. Wait while the file is downloaded, and then review the results displayed in **File Preview** window. THe results should be the same as those generated by the previous tasks.
10. Close Visual Studio. Do not save the project if prompted to do so.

## Demo 2: Performing an analysis using U-SQL

### Scenario

You have several days worth of stock price movement data, held as a set of files in Azure Data Lake storage. The data is organized into subfolders by year, month, and day. You want to merge the data into a single file for later processing, and also store a copy of the data in the Azure Data Lake Catalog. You decide to create an Azure Data Lake Analytics job to perform these tasks, but you want to create and test this job locally before deploying it to the cloud.

In this demonstration, you will see how to:

- Use an EXTRACT statement to combine multiple job inputs using virtual fields.
- Perform parallel data saving to a Lake Store catalog and CSV file.
- Test a job, before deploying to the cloud.

### Use an EXTRACT statement to combine multiple job inputs using virtual fields

1. In **File Explorer**, go to the folder **C:\\Users\\AdatumAdmin\\AppData\\Local\\USQLDataRoot**, and then create a new folder called **Stocks**.

    > **Note** AppData is a hidden folder so it might not appear in **File Explorer** by default. TO show hidden items in File Explorer, click the **View** menu, and then check the **Hidden items** check box.

2. In File Explorer, go to the folder **E:\\Demofiles\\Mod05\\Demo2\\Stocks**, and copy the **2017** folder tree to the clipboard. This folder tree contains several days worth of stock price movement data, organized by month and day.
3. In File Explorer, go to the folder **C:\\Users\\AdatumAdmin\\AppData\\Local\\USQLDataRoot\\Stocks**, and paste the **2017** folder tree.
4. In File Explorer, go to the folder **C:\\Users\\AdatumAdmin\\AppData\\Local\\USQLDataRoot\Stocks\\2017\\08\\27**, and double-click **Prices.csv** to open the file in Excel. For each day, the Prices file contains the Ticker, Price, and QuoteTime for a price change.
5. Close Microsoft Excel.
6. Start Visual Studio 2017.
7. On the **File** menu, click **OPen**, and then click **Project/Solution**.
8. In the **Open Project** dialog box, go to the folder **E:\\Demofiles\\Mod05\\Demo2\\StockMarketProcessor**, click **StockMarketProcessor.sln**, and then click **Open**.
9. In **Solution Explorer**, double-click **Script.usql** and note the following points in the USQL script:
    - EXTRACT statement. The file path references the Year, Month, and Day virtual fields to walk the subtree under the Stocks folder.
    - DROP/CREATE DATABASE statements. These create a new database in the ADLA catalog.
    - CREATE TABLE statement. This creates a table in the new database. Note the index.
    - INSERT INTO statement. This statement copies the input data to the table in the database.
    - SELECT statement that creates the @result rowset. This simply retrieves all the data from each row.
    - OUTPUT statement. This statement copies all of the data to a single CSV file. At the end, this file will contain the data merged from the original input files.

### Perform parallel data saving to a Lake Store catalog and CSV file

1. In the **Script.usql** pane, ensure that the **ADLA instance** in the toolbar is set to **(Local)**, and then click **Submit**. This will run the job defined by the script using the local instance of ADLA running on the virtual machie; this is the quickest (and cheapest way) to test a script. The job will open a command window that displays the progress of the job. When complete, the job should report the **Success** status.
2. When the job has press Enter to close the command window.
3. In the job graph, notice that the job has read all the input files (there are four of them) and combined them into a single stream. The job then performs two parallel tasks: one that sends data to the catalog and the other that writes data to the file on disk. Note that, to see the details in the job graph, you might need to zoom in, and close other panes in Visual Studio.
4. In **File Explorer**, go to the folder **C:\\Users\\AdatumAdmin\\AppData\\Local\\USQLDataRoot\\Stocks**. This folder should contain the **CombinedPrices.csv** file created by the job.
5. Open the **CombinedPrices.csv** file using Excel. Note the date in the first row, scroll to the end, and then note the date in the last row. The data should be in the same order as it was in the original files (ordered by date and time).
6. Close Excel.
7. Switch to Visual Studio.
8. In **Server Explorer**, expand **Azure**, expand **Data Lake Analytics**, expand **(Local)**, expand **U-SQL Databases**, expand **StockPrices** (this is the database created in the catalog), expand **Tables**, right-click **dbo.StockPriceMovements**, and then click **Preview**. The preview runs another Azure Data Lake Analytics job, which fetches the data in the table and returns it as a TSV file; the job might take a few seconds to return the data. The job displays the first 100 rows by default; you can fetch additional rows, or filter the results if you want.

### Test a job before deploying to the cloud

1. In **Cloud Explorer**, expand your Azure subscription, expand **Data Lake Analytics**, expand **stocksdla&lt;your name&gt;&lt;date&gt;**, expand **Linked Storages**, right-click **stockstore&lt;your name&gt;&lt;date&gt;**, and then click **Open File Explorer**.
2. In **Azure Data Lake Explorer**, right-click the files and folders pane, and then click **New Folder**.
3. In the **NewFolder** box, type **Stocks**, and press Enter.
4. In the files and folders pane, double-click **Stocks**.
5. Right-click the files and folders pane again, and click **New Folder**.
6. In the **NewFolder** box, type **2017**, and press Enter.
7. In the files and folders pane, double-click **2017**.
8. Right-click the files and folders pane, and click **New Folder**.
9. In the **NewFolder** box, type **08**, and press Enter.
10. In the files and folders pane, double-click **08**.
11. Right-click the files and folders pane, and click **New Folder**.
12. In the **NewFolder** box, type **27**, and press Enter.
13. Repeat steps 11 and 12 to create folders called **28**, **29**, and **30**.
14. In the files and folders pane, double-click **27**.
15. Right-click the files and folders pane, and then click **Upload**.
16. In the **Upload File** dialog box, go to the folder **E:\\Demofiles\\Mod05\\Demo2\\Stocks\\2017\\08\\27**, click **Prices.csv**, and then click **Open**.
17. In **Azure Data Lake Explorer**, in the breadcrumb trail, click **08**.
18. Repeat steps 14 to 17 to upload the **Prices.csv** files for folders **28**, **29**, and **30**.

    > **Note**: You need to perform these manual steps as there currently isn't a way to upload folder trees into Data Lake Store through Data Explorer (you could have used AzCopy into Blob storage, and then AdlCopy as an alternative).

19. On the **Script.usql** tab, in the **Script.usql** pane, change the **ADLA instance** in the toolbar from **(Local)** to **stocksdla&lt;your name&gt;&lt;data&gt;**, and then click **Submit**. The job will be deployed and, this time, will run in the cloud.
20. When the job has completed, in Visual Studio, click the **Azure Data Lake Explorer** tab.
21. In **Azure Data Lake Explorer**, in the breadcrumb trail, click the **Stocks** folder. The **CombinedPrices.csv** file has been created in the **Stocks** folder (you might need to click **Refresh** to see it).
22. In **Server Explorer**, under **Data Lake Anayltics**, expand **stocksdla&lt;your name&gt;&lt;date&gt;**, expand **U-SQL Databases**, expand **StockPrices**, expand **Tables**, right-click **dbo.StockPriceMovements**, and then click **Preview by Running a job**. 
23. In the **Preview by Running a Job** dialog box, click **Submit**. This time the preview job runs in the cloud instead of locally.
24. When the job has completed, in the **Job Graph** tab, doule-click **StockPrices.dbo.StockPriceMovements_Preview.tsv**. The results should be the same as the previous task.

    > **Note**: Running Azure Data Lake Analytics jobs in the cloud incurs costs, which is why running jobs locally first is a best practice during development and testing.

25. Close Visual Studio.

## Demo 3: Grouping and analyzing data

### Scenario

You want to investigate the volatility of each stock item. To do this, you decide to analyze the stock price movement data and calculate how much the price for each stock item moves. The prices are already available in the ADLA catalog (created by the previous demo), so you elect to create a view that partitions this data by ticker and determines the price differential between each price change, together with the lowest and highest price for each stock.

Additionally, you want to take the price movement analysis data and construct a report. The catalog view only contains limited information about each stock. The details for a stock item (such as its name), are held in a SQL database, so you need to join the data in the SQL database with that in the ADLA catalog.

In this demonstration, you will see how to:

- Use windowing functions and create a catalog view.
- Create an external data source that references a SQL Database credential.
- Use a federated query to join data from a SQL Database with data retrieved from the ADLA catalog.

### Setup

1. Complete the previous demo.
2. Ensure that the **MT17B-WS2016-NAT**, **20776A-LON-DC**, and **20776A-LON-DEV** virtual machines are running, and then log on to 20776A-LON-DEV as **ADATUM\\AdatumAdmin** with the password **Pa55w.rd**.
3. Using **File Explorer**, move to the folder **E:\\Demofiles\Mod05\\Demo3**, right-click **Setup.cmd**, and then click **Run as administrator**.
4. In the **User Account Control** dialog box, click **Yes**.
5. Using Internet Explorer, log in to the Azure portal using the account associated with your Azure pass.
6. In the Azure portal, click **+ Create a resource**, click **Databases**, and then click **SQL Database**.
7. On the **SQL Database** blade, in the **Database name** box, type **StockDB**.
8. Under **Resource group**, click **Use existing**, and then click **StockMarketRG**.
9. In the **Select source** list, select **Blank database**.
10. Click **Server**.
11. On the **New server** blade, enter the following details, and then click **Select**:
    - **Server name**: stockdbs&lt;your name&gt;&lt;date&gt;
    - **Server admin login**: student
    - **Password**: Pa55w.rd
    - **Confirm password**: Pa55w.rd
    - **Location**: select the same location as you used for the Data Lake Store that you created in Demo 1
    - Leave all other settings at their default value
12. On the **SQL Database** blade, click **Pricing tier**.
13. On the **Configure** blade, click **Basic**, and then click **Apply**.
14. On the **SQL Database** blade, leave all other settings at their defaults, and then click **Create**.
15. Wait until the database and server have deployed before continuing with the demo preparation.
16. Click **All resources**, and then click **stockdbs&lt;your name&gt;&lt;date&gt;**.
17. On the **stockdbs&lt;your name&gt;&lt;date&gt;** blade, under **Security**, click **Firewall and virtual networks**, click **Add client IP**, click **Save**, and then click **OK**.
18. Click **All resources**, and then click **StockDB (stockdbs&lt;your name&gt;&lt;date&gt;/StockDB)**.
19. On the **StockDB** blade, click **Query editor**.
20. On the **Query editor (preview)** blade, accept the default login details, and then click **OK**.
21. Click line 1, type the following command, and then click **Run**. Verify that the query runs successfully. This query creates a new table named StockNames that will hold the ticker and full stock names for each stock item:

    ```SQL
    CREATE TABLE StockNames(
        Ticker CHAR(4) NOT NULL PRIMARY KEY,
        Name VARCHAR(200) NOT NULL)
    ```

    You can copy this query from the from the file **E:\\Demofiles\\Mod05\\CreateSqlTable.txt**.

22. On the Windows **Start** menu, type **Microsoft SQL Server Management Studio 17**, and then press Enter.
23. In the **Connect to Server** dialog box, in the **Server name** box, type **stockdbs&lt;your name&gt;&lt;date&gt;.database.windows.net**.
24. In the **Authentication** list, select **SQL Server Authentication**.
25. In the **Login** box, type **student**.
26. In the **Password** box, type **Pa55w.rd**, and then click **Connect**.
27. In **SQL Server Management Studio**, in **Object Explorer**, expand **Databases**, right-click **StockDB**, point to **Tasks**, and then click **Import Data**.
28. In the **SQL Server Import and Export Wizard**, on the **Welcome to SQL Server Import and Export Wizard** page, click **Next**.
29. On the **Choose a Data Source** page, in the **Data source** list, click **Flat File Source**, and then click **Browse**.
30. In the **Open** dialog box, go to the **E:\\Demofiles\\Mod05\\Demo3** folder, change the file type to **CSV files (*.csv)**, click **StockNames.csv**, and then click **Open**.
31. On the **Choose a Data Source** page, clear the **Column names in the first data row** check box, and then click **Next**.
32. On the second **Choose a Data Source** page (previewing the data in the file), click **Next**.
33. On the **Choose a Destination** page, in the **Destination** list, click **SQL Server Native Client 11.0**.
34. In the **Server name** box type **stockdbs&lt;your name&gt;&lt;date&gt;.database.windows.net**, and then click **Use SQL Server Authentication**.
35. In the **User name** box, type **student**, in the **Password** box, type **Pa55w.rd**, in the **Database** list, click **StockDB**, and then click **Next**.
36. On the **Select Source Tables and Views** page, click **Next**.
37. On the **Save and Run Package** page, check the **Run immediately** check box, and then click **Next**.
38. On the **Complete the Wizard** page, click **Finish**.
39. Wait for the data to be imported; the process should upload 100 rows. If you get data truncation warnings, you can ignore them.
40. On the **The execution was successful** page, click **Close**.
41. In **SQL Server Management Studio**, expand **Databases**, expand **StockDB**, expand **Tables**, right-click **dbo.StockNames**, and then click **Select Top 1000 Rows**.
42. Verify that the data has been uploaded to the table. The stock names are random strings.
43. Close SQL Server Management Studio.

### Use windowing functions and create a catalog view

1. Start Visual Studio 2017.
2. In Visual Studio, on the **File** menu, click **Open**, and then click **Project/Solution**.
3. In the **Open Project** dialog box, go to the **E:\\Demofiles\\Mod05\\Demo3\\MarketPricesAnalyzer** folder, click **MarketPricesAnalyzer.sln**, and then click **Open**.
4. In Solution Explorer, double-click **Analysis.usql**, and note the following points:
    - The PriceDifferentials view is created in the ADLA catalog. It assumes that you created and populated the StockPriceMovements table in the previous demo.
    - The PriceDifferentialsView uses windowing with the MAX, and MIN to display the high/low prices for each stock, and the LAG function is used to calculate the difference in price between quotes.
    - The @priceDifferences rowset uses the view to generate the results.
5. In the **Analysis.usql** pane, change the **ADLA instance** in the toolbar from **(Local)** to **stocksdla&lt;your name&gt;&lt;data&gt;**, and then click **Submit**. The job will be deployed and run in the cloud.
6. When the job has completed, in **Cloud Explorer**, expand your Azure subscription, expand **Data Lake Analytics**, expand **stocksdla&lt;your name&gt;&lt;date&gt;**, expand **Linked Storage**, and then double-click **stockstore&lt;your name&gt;&lt;date&gt;**.
7. In **Azure Data Lake Explorer**, in the files and folders pane, double-click **Stocks**. The Analysis.csv file should be listed in the **Stocks** folder (you might need to click **Refresh** to see it).
8. In **Azure Data Lake Explorer**, in the files and folders pane, double-click **Analysis.csv**. This file contains six columns for ticker, current price, quote time, gain/loss from previous price, and maximum and minimum prices recorded for the stock.
9. Close Visual Studio.

### Create an external data source that references a SQL Database credential

1. On the Windows **Start** menu, type **PowerShell ISE**, right-click **Windows PowerShell ISE**, and then click **Run as administrator**.
2. In the **User Account Control** dialog box, click **Yes**.
3. On the **File** menu, click **Open**.
4. In the **Open** dialog box, go to teh folder **E:\\Demofiles\\Mod05\\Demo3**, click **ConfigureFederatedDatabase.ps1**, and then click **Open**. This script creates a credential in the ADLA catalog that enables ADLA jobs to connect to your Azure SQL Database server by using the **New-AzureRmDataLakeAnalyticsCatalogCredential** cmdlet. When you run this cmdlet, you will be prompted for the SQL Server username and password to be stored as part of the credential (it is encrypted in the ADLA catalog). The script also shows the **Remove-AzureRmDataLakeAnalyticsCatalogCredential** command (commented out). This is how you remove a credential from the catalog.

    ```PowerShell
    Install-Module AzureRM -AllowClobber;
    Import-Module AzureRM;

    Login-AzureRmAccount;

    New-AzureRmDataLakeAnalyticsCatalogCredential -AccountName "&lt;name of your ADLA account&gt;" -DatabaseName "StockPrices" -CredentialName "StocksDBCredential" -Credential (Get-Credential) -DatabaseHost "&lt;name of your Azure SQL Database Server&gt;.database.windows.net" -Port 1433;

    # Remove-AzureRmDataLakeAnalyticsCatalogCredential -AccountName "&lt;name of your ADLA account&gt;" -DatabaseName "StockPrices" -Name "StocksDBCredential";
    ```

5. Edit the script, and replace **&lt;name of your ADLA account&gt;** with **stocksdla&lt;your name&gt;&lt;date&gt;**.
6. Replace **&lt;name of your Azure SQL Database Server&gt;** with **stockdbs&lt;your name&gt;&lt;date&gt;**.
7. On the toolbar, click **Save**.
8. On the toolbar, click **Run Script**.
9. In the **Untrusted repository** dialog box, click **Yes to All**.
10. In the **Sign in to your account** dialog box, enter the details of the Microsoft account that is associated with your Azure Learning Pass subscription, and then sign in.
11. In the **cmdlet Get-Credential at command pipe** dialog box, in the **User name** box, type **student**, in the **Password** box, type **Pa55w.rd**, and then click **OK**. If a warning message appears about the output type for the cmdlet being incorrect, you can ignore it.

### Use a federated query to join data from SQL Database with data retrieved from the ADLA catalog

1. Switch to Visual Studio.
2. In **Solution Explorer**, double-click Federated.usql, and note the following points:

    - The USE DATABASE StockPrices statement switches the context to the StockPrices ADLA catalog. It means that you don't need to keep specifying StockPrices in the subsequent statements.
    - The DROP DATA SOURCE IF EXISTS RemoteStocksDB statement is self-explanatory.
    - The CREATE DATA SOURCE statement creates a new data source named RemoteStocksDB. This data source connects to your Azure SQL Database using the credential you created with PowerShell.
    - The @results rowset joins the data in the ADLA catalog with data retrieved from the Azure SQL Database (stock names).

3. In the **Federated.usql** pane, change the **ADLA instance** in the toolbar from **(Local)** to **stocksdla&lt;your name&gt;&lt;data&gt;**, and then click **Submit**. The job will be deployed and run in the cloud.
4. When the job has completed, on the **Job Graph** tab, double-click **StockData.csv**. This file contains five columns for stock name, ticker, current price, quote time, and gain/loss from the previous price. The stock name has been retrieved from the SQL database.

---

©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.