# Module 7: Implementing Azure SQL Data Warehouse

- [Module 7: Implementing Azure SQL Data Warehouse](#module-7-implementing-azure-sql-data-warehouse)
    - [Lab: Implementing SQL Data Warehouse](#lab-implementing-sql-data-warehouse)
        - [Scenario](#scenario)
        - [Objectives](#objectives)
        - [Lab Setup](#lab-setup)
    - [Exercise 1: Create and configure a new SQL Data Warehouse](#exercise-1-create-and-configure-a-new-sql-data-warehouse)
        - [Scenario](#scenario)
        - [Task 1: Install AzCopy and AdlCopy](#task-1-install-azcopy-and-adlcopy)
        - [Task 2: Create a new database server](#task-2-create-a-new-database-server)
        - [Task 3: Create a new SQL Data Warehouse](#task-3-create-a-new-sql-data-warehouse)
        - [Task 4: Explore the SQL Data Warehouse using SQL Server Management Studio](#task-4-explore-the-sql-data-warehouse-using-sql-server-management-studio)
        - [Task 5: Scale the SQL Data Warehouse](#task-5-scale-the-sql-data-warehouse)
    - [Exercise 2: Design and configure SQL Data Warehouse tables](#exercise-2-design-and-configure-sql-data-warehouse-tables)
        - [Scenario](#scenario)
        - [Task 1: Design tables and indexes for a SQL Data Warehouse application](#task-1-design-tables-and-indexes-for-a-sql-data-warehouse-application)
        - [Task 2: Use SQL Server Management Server to create data warehouse tables and indexes](#task-2-use-sql-server-management-server-to-create-data-warehouse-tables-and-indexes)
    - [Exercise 3: Import static data into SQL Data Warehouse](#exercise-3-import-static-data-into-sql-data-warehouse)
        - [Scenario](#scenario)
        - [Task 1: Stage data in Data Lake Store prior to SQL Data Warehouse import](#task-1-stage-data-in-data-lake-store-prior-to-sql-data-warehouse-import)
        - [Task 2: Stage data in an on-premises SQL Server database prior to SQL Data Warehouse import](#task-2-stage-data-in-an-on-premises-sql-server-database-prior-to-sql-data-warehouse-import)
        - [Task 3: Import data from a local CSV file into SQL Data Warehouse](#task-3-import-data-from-a-local-csv-file-into-sql-data-warehouse)
        - [Task 4: Import data from Data Lake Store into SQL Data Warehouse](#task-4-import-data-from-data-lake-store-into-sql-data-warehouse)
        - [Task 5: Import data from an on-premises SQL Server database into SQL Data Warehouse](#task-5-import-data-from-an-on-premises-sql-server-database-into-sql-data-warehouse)
    - [Exercise 4: Stream dynamic data to SQL Data Warehouse](#exercise-4-stream-dynamic-data-to-sql-data-warehouse)
        - [Scenario](#scenario)
        - [Task 1: Configure an Azure Stream Analytics job to output to SQL Data Warehouse](#task-1-configure-an-azure-stream-analytics-job-to-output-to-sql-data-warehouse)
        - [Task 2: Configure a Visual Studio app to use the Stream Analytics job](#task-2-configure-a-visual-studio-app-to-use-the-stream-analytics-job)
        - [Task 3: View Stream Analytics job data in SQL Data Warehouse](#task-3-view-stream-analytics-job-data-in-sql-data-warehouse)
        - [Task 4: Lab cleanup](#task-4-lab-cleanup)

## Lab: Implementing SQL Data Warehouse

### Scenario

You work for Adatum as a data engineer, and you have been asked to build a traffic surveillance system for traffic police. This system must be able to analyze significant amounts of dynamically streamed data—captured from speed cameras and automatic number plate recognition (ANPR) devices—and then crosscheck the outputs against large volumes of reference data holding vehicle, driver, and location information. Fixed roadside cameras, hand-held cameras (held by traffic police), and mobile cameras (in police patrol cars) are used to monitor traffic speeds and raise an alert if a vehicle is travelling too quickly for the local speed limit. The cameras also have built-in ANPR software that read vehicle registration plates.

In this phase of the project, you will consolidate the data storage for the traffic surveillance system by using SQL Data Warehouse as a single data location for static, or rarely updated, information including stolen vehicle data, vehicle owner data, and speed camera location data. You will also configure the traffic surveillance system to use the same SQL Data Warehouse to hold dynamic data streamed live from the speed cameras.

### Objectives

After completing this lab, you will be able to:

- Create and configure a new SQL Data Warehouse.
- Design and configure SQL Data Warehouse tables.
- Import static data into SQL Data Warehouse.
- Stream dynamic data to SQL Data Warehouse.

### Lab Setup

Estimated time: 90 minutes
Virtual machine: **20776A-LON-DEV**
User name: **ADATUM\\AdatumAdmin**
Password: **Pa55w.rd**

Before starting this lab, performing the following steps:

1. Using Internet Explorer, go to **https://go.microsoft.com/fwlink/?linkid=2014060**.
2. In the Internet Explorer message box, click **Run**.
3. In the **Microsoft SQL Server Data Tools** dialog box, on the **Welcome** page, click **Next**.
4. In the **Install tools to this Visual Studio 2017 instance** list, click Visual Studio Enterprise 2017.
5. Under **Install tools for these SQL Server features**, select all the check boxes, and then click **Install**.
6. In the **User Account Control** dialog box, click **Yes**.
7. On the **Setup Completed** page, click **Restart** to reboot the virtual machine.
8. Log on to the virtual machine as **ADATUM\\AdatumAdmin** with the password **Pa55w.rd**.
9. Start Internet Explorer and go to **https://www.microsoft.com/en-us/download/details.aspx?id=54798**, and then click **Download**.
10. On the **Choose the download you want** page, select the **SsisAzureFeaturePack\_2017\_x86.msi** check box, and then click **Next**.
11. In the Internet Explorer message box, click **Run**.
12. In the **Microsoft SQL Server Feature Pack Setup** dialog box, on the **Welcome to the Microsoft SQL Server 2017 Integration Services Feature Pack for Azure (x86) Setup Wizard** page, click **Next**.
13. On the **Please read the Microsoft SQL Server 2017 Integration Services Feature Pack for Azure (x86) License Agreement** page, select the **I accept the terms in the License Agreement** check box, and then click **Install**.
14. In the **User Account Control** dialog box, click **Yes**.
15. On the **Completed the Microsoft SQL Server 2017 Integration Services Feature Pack for Azure (x64) Setup Wizard** page, click **Finish**.
16. Close Internet Explorer.

> **Note:** This lab also uses the following resources from Lab 5 and earlier:
>
> - **Resource group**: CamerasRG
> - **Data Lake Store**: adls&lt;_your name_&gt;&lt;_date_&gt;
> - **Azure Stream Analytics job**: CaptureTrafficData
> - **Event Hub**: camerafeeds&lt;_your name_&gt;&lt;_date_&gt;

## Exercise 1: Create and configure a new SQL Data Warehouse

### Scenario

You are going to consolidate the data storage for the traffic surveillance system by using SQL Data Warehouse as a single data location for both static and dynamic data. In this exercise, you will create a new data warehouse for holding traffic data—this warehouse will run on a new database server. You will then use SQL Server Management Studio to explore the data warehouse, and use scaling to set a performance level.

The main tasks for this exercise are as follows:

1. Install AzCopy and AdlCopy
2. Create a new database server
3. Create a new SQL Data Warehouse
4. Explore the SQL Data Warehouse using SQL Server Management Studio
5. Scale the SQL Data Warehouse

> **Note**: If you have completed Lab 4, you do not need to perform Exercise 1, Task 1.

### Task 1: Install AzCopy and AdlCopy

1. Ensure that the **MT17B-WS2016-NAT**, **20776A-LON-DC**, **20776A-LON-SQL**, and **20776A-LON-DEV** virtual machines are running, and then log on to **20776A-LON-DEV** as **ADATUM\\AdatumAdmin** with the password **Pa55w.rd**.
2. Download and install **AzCopy** from **https://aka.ms/downloadazcopy**.
3. Add: **C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\AzCopy** to the **Path** environment variable.
4. Download and install **AdlCopy** from **https://www.microsoft.com/en-us/download/details.aspx?id=50358**.
5. Add **%HOMEPATH%\\Documents\\AdlCopy** to the **Path** environment variable.

### Task 2: Create a new database server

1. Using the Azure portal, create a new SQL server (logical server), using the following details:
    - **Server name**: trafficserver&lt;_your name_&gt;&lt;_date_&gt;
    - **Server admin login**: student
    - **Password**: Pa55w.rd
    - **Resource group (use existing)**: CamerasRG
    - **Location**: select the same location as you have used for Data Lake Stores in previous labs
2. Ensure that the **Allow azure services to access server** check box is selected.
3. Configure the server firewall with your client IP address, and ensure that **Allow access to Azure services** in set to **ON**.
4. Wait until the server has deployed before continuing with the lab.

### Task 3: Create a new SQL Data Warehouse

1. Using the Azure portal, create a new SQL Data Warehouse, with the following details:
    - **Database name**: trafficwarehouse
    - **Resource group (Use existing)**: CamerasRG
    - **Select source**: Blank database
    - **Server**: trafficserver&lt;_your name_&gt;&lt;_date_&gt;
    - **Performance level**: Gen1: DW100
    - Leave all other settings at their defaults
2. Wait until the data warehouse has deployed before continuing with the lab.

### Task 4: Explore the SQL Data Warehouse using SQL Server Management Studio

1. Use Microsoft SQL Server Management Studio to connect to your database server, using the following connection details:
    - **Server name**: trafficserver&lt;_your name_&gt;&lt;_date_&gt;.database.windows.net
    - **Authentication**: SQL Server Authentication
    - **Login**: student
    - **Password**: Pa55w.rd
2. In Object Explorer, verify that the **trafficwarehouse** data warehouse is listed.
3. Create a **New Query**, and run the following SELECT statement:

    ```SQL
    SELECT *
    FROM sys.dm_pdw_nodes
    GO
    ```

     You can copy the SQL statements in this exercise from the file **E:\\Labfiles\\Lab07\\Exercise1\\Exercise1.sql**.

4. Verify that this select statement lists a single **CONTROL** node and a single **COMPUTE** node.
5. Make a note of the value in the **pdw\_node\_id** column of the **COMPUTE** node.
6. Run the following SELECT statement:

    ```SQL
    SELECT *
    FROM sys.pdw_distributions
    GO
    ```

7. Verify that you see 60 distributions (databases) listed; these distributions should all belong to the COMPUTE node (use the value recorded for the pdw\_node\_id column in the previous query to verify this).

### Task 5: Scale the SQL Data Warehouse

1. Use the Azure portal to set the **Performance level** of **trafficwarehouse** to **DW400**.
2. Wait until the data warehouse has resumed after the scaling operation before continuing with the lab; this might take several minutes.
3. When the data warehouse has resumed, return to Microsoft SQL Server Management Studio, reconnect to the data warehouse and repeat the following SELECT statement:

    ```SQL
    SELECT *
    FROM sys.dm_pdw_nodes
    GO
    ```

4. Verify that, this time, you see four COMPUTE nodes, because you have scaled performance by a factor of four.
5. Execute the following SELECT statement:

    ```SQL
    SELECT *
    FROM sys.pdw_distributions
    GO
    ```

6. Verify that you still see 60 distributions but they are now spread across the compute nodes (15 distributions per node).
7. Use the Azure portal to set the **Performance level** of **trafficwarehouse** back to **DW100**.
8. Wait until the data warehouse has resumed after the scaling operation before continuing with the lab; this might take several minutes. It’s important to use an appropriate performance level, as SQL Data Warehouse is an expensive resource, and should only be scaled as needed.

>**Result**: In this exercise, you created a new database server, and a new data warehouse. You explored the data warehouse using SQL Server Management Studio. Finally, you scaled the data warehouse.

## Exercise 2: Design and configure SQL Data Warehouse tables

### Scenario

You want to consolidate the data storage for the traffic surveillance system by using SQL Data Warehouse into a single data location for static and dynamic data. In this exercise, you will design and create the tables and indexes that are required to support the traffic monitoring system.

The main tasks for this exercise are as follows:

1. Design tables and indexes for a SQL Data Warehouse application
2. Use SQL Server Management Server to create data warehouse tables and indexes

### Task 1: Design tables and indexes for a SQL Data Warehouse application

The traffic monitoring system uses the following information:

- **Traffic camera locations**. There are 500 traffic cameras, recording traffic speeds and capturing vehicle registrations. The location of each camera rarely changes.
- **Vehicle speeds**. This data is continually streamed from the traffic cameras. It comprises the identity of the camera, speed limit (this can vary with time), vehicle speed, and vehicle registration. This data is used to perform analysis of traffic patterns, identify hotspots, and so on. It’s also used to send fixed penalty fines and summonses to owners of vehicles caught speeding, and to notify traffic patrols if a suspect (stolen or false) vehicle registration is captured.
- **Vehicle/owner data**. This dataset contains the current vehicle ownership records for every registered vehicle. This data includes the registration number, together with the name (title, forename, surname), and address (lines 1-4) for every vehicle. Currently, there are approximately 7.7 million vehicles registered, but new vehicles are registered often and older vehicles unregistered as they are written off.
- **Stolen vehicles**. This dataset contains the registration number, date reported stolen, and date recovered (or null if the vehicle is still missing) for every vehicle reported stolen. This data is historical—it’s appended to (and updated as vehicles are recovered), but data is never deleted. There are currently 1.2 million stolen vehicle records (approximately 2 percent of vehicles [150,000] are reported stolen each year, and there are currently eight years of data on record—2010-2017 inclusive). The data is expected to grow at a similar rate in the future.

**Question 1:**
What distribution policy would be most appropriate for each type of data?

**Question 2:**
How might your partition the data (if at all)?

### Task 2: Use SQL Server Management Server to create data warehouse tables and indexes

1. Using Microsoft SQL Server Management Studio, write and execute CREATE TABLE statements for each of the required tables. The tables should have the following names and columns (name and type):

    ```Text
    Table: VehicleSpeed
    ----------------------------
    CameraID: VARCHAR(10)
    SpeedLimit: INT
    Speed: INT
    VehicleRegistration: VARCHAR(7)
    WhenDate: DATETIME
    WhenMonth: INT (required for partitioning)

    Table: CameraLocation
    --------------------------------
    CameraID: VARCHAR(10)
    GPSLocationX: FLOAT
    GPSLocationY: FLOAT

    Table: VehicleOwner
    ----------------------------
    VehicleRegistration: VARCHAR(7)
    Title: VARCHAR(30)
    Forename: VARCHAR(30)
    Surname: VARCHAR(30)
    AddressLine1: VARCHAR(50)
    AddressLine2: VARCHAR(50)
    AddressLine3: VARCHAR(50)
    AddressLine4: VARCHAR(50)

    Table:StolenVehicle
    ---------------------------
    VehicleRegistration: VARCHAR(7)
    DateStolen: DATETIME
    DateRecovered: DATETIME
    YearStolen: INT (required for partitioning)
    ```

    Note the following:
    - All columns are mandatory, except for the DateRecovered column in the StolenVehicle table, which must allow nulls.
    - The VehicleSpeed and CameraLocation tables must be created as HEAPs.
    - You should use a CLUSTERED COLUMNSTORE index for the other two tables.

    You can copy example statements from the file **E:\\Labfiles\\Lab07\\Exercise2\\Exercise2.sql**

2. Use Object Explorer to verify that the **dbo.VehicleSpeed**, **dbo.CameraLocation**, **dbo.VehicleOwner**, and **dbo.StolenVehicle** tables have been created.

>**Result**: In this exercise, you designed tables and indexes for a data warehouse application, and used SQL Server Management Server to create the required data warehouse tables and indexes.

## Exercise 3: Import static data into SQL Data Warehouse

### Scenario

In this exercise, you will consolidate the data storage for the traffic surveillance system by using SQL Data Warehouse as a single data location for static, or rarely updated, information including stolen vehicle data, vehicle owner data, and speed camera location data. You will also import the static data required by the system into the warehouse. There are three data sources for this exercise:

- Stolen vehicle data in Data Lake Store.
- Vehicle owner data in an on-premises SQL Server database.
- Speed camera location data in a local CSV file.

You will first upload the stolen vehicle data to Data Lake Store (using AzCopy and Adlcopy), as a temporary staging location. You will then create a local SQL Server database for holding vehicle owner data, again as a staging location. You will then upload speed camera location data directly into the data warehouse from a local CSV file using AzCopy, PolyBase, and CTAS (dropping the existing table first, and using CTAS with the same options that were used to create the table in the first place). You will then import the staged stolen vehicle data from Data Lake Store into SQL Data Warehouse—leaving the existing table in place—and then use INSERT INTO to append data to the table.

Finally, you will import the staged vehicle/owner data from the on-premises SQL Server database by using an ADO.NET source and destination with SQL Server Integration Services—again leaving the existing table in place.

The main tasks for this exercise are as follows:

1. Stage data in Data Lake Store prior to SQL Data Warehouse import
2. Stage data in an on-premises SQL Server database prior to SQL Data Warehouse import
3. Import data from a local CSV file into SQL Data Warehouse
4. Import data from Data Lake Store into SQL Data Warehouse
5. Import data from an on-premises SQL Server database into SQL Data Warehouse

### Task 1: Stage data in Data Lake Store prior to SQL Data Warehouse import

**Note**: This task is only necessary if you have not performed exercise 3 of module 6 which uses the same data. If you have already staged the stolen vehicle data in Data Lake Store, then skip to Task 2 to stage data in an on-premises SQL Server database.

1. Use the Azure portal to create a new Blob storage account, using the following details:
    - **Resource group (use existing)**: CamerasRG
    - **Name**: vehicledata&lt;_your name_&gt;&lt;_date_&gt;
    - **Location**: select the same location as you used for the data warehouse in Exercise 1
    - **Account kind**: Blob storage
    - Leave all other details at their defaults
2. Wait until the storage account has been successfully created before continuing with the exercise.
3. Add a new blob container named **stolen** to the account.
4. Make a note of the storage access key for the storage account.
5. View the **E:\\Labfiles\\Lab07\\Exercise3\\StolenVehicles** folder on the desktop, and verify that it contains eight years of stolen vehicle data, organized in subfolders by year/month/day; there are 2,914 separate CSV files.
6. Use the AzCopy command to upload the files and folders under the **E:\\Labfiles\\Lab07\\Exercise3\\StolenVehicles** folder to the blob container. Use the **/S** parameter to indicate that AzCopy should recursively traverse subfolders; you copy the command from **E:\\Labfiles\\Lab07\\Exercise3\\AzCopyCmd1.txt**. Replace **&lt;storage account name&gt;** with **vehicledata&lt;_your name_&gt;&lt;_date_&gt;**, and replace **&lt;storage key&gt;** with the key you noted previously:

    ```CMD
    azcopy /Source:"E:\Labfiles\Lab07\Exercise3\StolenVehicles" /Dest:https://<storage account name>.blob.core.windows.net/stolen /DestKey:<storage key> /S
    ```

   The copy process might take several minutes to complete. Wait until all files have been copied before continuing with the exercise.

7. Use Cloud Explorer in Visual Studio to examine the stolen blob container.
8. In Cloud Explorer, if you do not have an Azure Pass folder, you will need to add the Microsoft account that is associated with your Azure Learning Pass subscription.
9. Verify that the files and subfolders have been uploaded.
10. Use the Azure portal to create a new folder called **Stolen** in your Data Lake Store (adls&lt;your name&gt;&lt;date&gt;).
11. Use the AdlCopy command to transfer the files from Blob storage to the Stolen folder in your Data Lake Store; you copy the following command from **E:\\Labfiles\\Lab07\\Exercise3\\AdlCopyCmd.txt**. Replace **&lt;storage account name&gt;** with **vehicledata&lt;_your name_&gt;&lt;_date_&gt;**, replace **&lt;Data Lake Store name&gt;** with **adls&lt;_your name_&gt;&lt;_date_&gt;**, and replace **&lt;storage key&gt;** with the blob store key you noted previously:

    ```CMD
    adlcopy /source https://<storage account name>.blob.core.windows.net/stolen/ /dest adl://<Data Lake Store name>.azuredatalakestore.net/Stolen/ /sourcekey <storage key>
    ```

    If a **Visual Studio** dialog box appears prompting you to log in, sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.

    > **Note**: Depending on the region and the location of the Blob storage account (ideally they should be in the same region, but might not be), AdlCopy will take from two to 20 minutes to copy the data. Ignore the stats that indicate the percentage of files copied—it sits at 0.00% until complete then jumps to 100%, and may copy the data in three phases (files 1 to 1000, then files 1001 to 2000, and then the remainder).

12. Use Data Explorer in the Azure portal to examine the Data Lake Store and verify that all the files and folders have been copied across.

### Task 2: Stage data in an on-premises SQL Server database prior to SQL Data Warehouse import

1. On the desktop, use **File Explorer** to move to the **E:\\Labfiles\\Lab03\\Exercise3** folder.
2. Double-click **ownerdata.part01.exe**.
3. In the **WinRAR self-extracting archive** dialog box, in the **Destination Folder** box, type **E:\\Labfiles\\Lab07\\Exercise3**, and then click **Extract**.
4. Wait while the **ownerdata.csv** file is extracted from the archive. This file contains the sample vheicle opwner data that you will stage in the SQL Server database.
5. Using Microsoft SQL Server Management Studio, connect to the **LON-SQL** server using Windows Authentication.
6. Use Object Explorer to create a new database named **VehicleInfo** using the default options.
7. In a new query window, run the following commands:

    ```SQL
    USE VehicleInfo
    GO

    -- Create VehicleOwner table
    CREATE TABLE VehicleOwner
    (
        VehicleRegistration VARCHAR(7) NOT NULL,
        Title VARCHAR(30) NOT NULL,
        Forename VARCHAR(30) NOT NULL,
        Surname VARCHAR(30) NOT NULL,
        AddressLine1 VARCHAR(50) NOT NULL,
        AddressLine2 VARCHAR(50) NOT NULL,
        AddressLine3 VARCHAR(50) NOT NULL,
        AddressLine4 VARCHAR(50) NOT NULL
    )
    GO
    ```

    You can copy these commands from the file **E:\\Labfiles\\Lab07\\Exercise3\\SqlCmd1.txt)**.

8. At a command prompt, move to the **E:\\Labfiles\\Lab07\\Exercise3** folder, and run the following command (you copy this from **E:\\Labfiles\\Lab07\\Exercise3\\BcpCmd1.txt**):

    ```CMD
    bcp VehicleInfo.dbo.VehicleOwner in ownerdata.csv /T /SLON-SQL /c /t,
    ```

    Ensure that you include the comma at the end of the line.

9. The bcp command should upload more than 7.7 million rows to the database (it’s fairly quick, and should take no more than five minutes).
10. Return to Microsoft SQL Server Management Studio and run the following query:

    ```SQL
    SELECT TOP(1000) *
    FROM VehicleOwner
    GO
    ```

    You can copy this query from the file  **E:\\Labfiles\\Lab07\\Exercise3\\SqlCmd2.txt**.

    This command should display the first 1,000 rows of data; the names and addresses contain random strings, but the vehicle registrations tie in with those of the stolen vehicle data (and the data generated by the speed cameras that you will use in Exercise 4).

11. Close the query window, without saving any changes, and disconnect from LON-SQL.

### Task 3: Import data from a local CSV file into SQL Data Warehouse

1. Use the Azure portal to add a new blob container named **locationdata** to the **vehicledata&lt;_your name_&gt;&lt;_date_&gt;** Blob storage account.
2. Make a note of the storage access key for the storage account.
3. Use the AzCopy command to upload the **CameraData.csv** file in the **E:\\Labfiles\\Lab07\\Exercise3** folder to the **locationdata** container; you copy the following command from **E:\\Labfiles\\Lab07\\Exercise3\\AzCopyCmd2.txt**. Replace **&lt;storage account name&gt;** with **vehicledata&lt;_your name_&gt;&lt;_date_&gt;**, and replace **&lt;storage key&gt;** with the key you noted in Step 2:

    ```CMD
    azcopy /Source:"E:\Labfiles\Lab07\Exercise3\" /Pattern:CameraData.csv /Dest:https://<storage account name>.blob.core.windows.net/locationdata /DestKey:<storage key>
    ```

    This file contains the locations of the traffic cameras as GPS coordinates.

4. In Microsoft SQL Server Management Studio, re-establish the database connection with the SQL Data Warehouse.
5. In Object Explorer, connect to the **trafficwarehouse** database, and then execute the following command to create an encryption key for encrypting Blob storage credentials:

    ```SQL
    CREATE MASTER KEY ENCRYPTION BY PASSWORD='Pa55w.rd'
    GO
    ```

    You can find all the commands for this task in the file **E:\\Labfiles\\Lab07\\Exercise3\\SpeedCameraLocationScript.sql**.

6. Execute the following command in the script to create a database scoped credential for accessing the **vehicledata** storage account; the identity is the name of the storage account, and the secret is the storage account key. Replace **&lt;storage account name&gt;** with **vehicledata&lt;your name&gt;&lt;date&gt;**, and replace **&ltstorage key&gt;** with the key you copied to the clipboard earlier:

    ```SQL
    CREATE DATABASE SCOPED CREDENTIAL CredentialsToBlobStorage
    WITH IDENTITY = '<storage account name>',
    SECRET = '<storage key>';
    GO
    ```

7. Execute the following command create an external data source named **LocationDataSource** that connects to the **locationdata** container in the **vehicledata** Blob storage account using this credential. Replace **&lt;storage account name&gt;** with **vehicledata&lt;your name&gt;&lt;date&gt;**:

    ```SQL
    CREATE EXTERNAL DATA SOURCE LocationDataSource
    WITH (
        TYPE = HADOOP,
        LOCATION = 'wasbs://locationdata@<storage account name>.blob.core.windows.net',
        CREDENTIAL = CredentialsToBlobStorage
    )
    GO
    ```

8. Execute the following command to define the file format of the data to be read from this data source; it is CSV data:

    ```SQL
    CREATE EXTERNAL FILE FORMAT CommaSeparatedFileFormat
    WITH (
        FORMAT_TYPE = DelimitedText,
        FORMAT_OPTIONS (FIELD_TERMINATOR =',')
    )
    GO
    ```

9. Execute the following command to create an external table named **ExternalLocationData** that uses the data source and file format to read the location data from Blob storage. Remember that the file in Blob storage is named **CameraData.csv**. The table should contain the following three fields:
    - CameraID: VARCHAR(10) NOT NULL
    - GPSLocationX: FLOAT NOT NULL
    - GPSLocationY: FLOAT NOT NULL

    ```SQL
    CREATE EXTERNAL TABLE ExternalLocationData (
        CameraID VARCHAR(10) NOT NULL,
        GPSLocationX FLOAT NOT NULL,
        GPSLocationY FLOAT NOT NULL)
    WITH (
        LOCATION='CameraData.csv',
        DATA_SOURCE = LocationDataSource,
        FILE_FORMAT = CommaSeparatedFileFormat,
        REJECT_TYPE = VALUE,
        REJECT_VALUE = 0
    )
    GO
    ```

10. Execute the following command to verify that the external table has been created correctly, and use it to read the camera locations; the query should return 500 rows (one row for each camera):

    ```SQL
    SELECT *
    FROM ExternalLocationData
    GO
    ```

11. Execute the following command to drop the existing **CameraLocation** table from the data warehouse; you will rebuild it using CTAS in the next step:

    ```SQL
    DROP TABLE CameraLocation
    GO
    ```

12. Execute the following command to use CTAS to rebuild and populate the **CameraLocation** table; remember that this table should be implemented as a heap using the REPLICATE distribution policy:

    ```SQL
    CREATE TABLE CameraLocation
    WITH
    (
        HEAP,
        DISTRIBUTION = REPLICATE
    )
    AS SELECT CameraID, GPSLocationX, GPSLocationY
    FROM ExternalLocationData
    GO
    ```

13. Execute the following command to verify that the **CameraLocation** table has been recreated and populated; the following query should return the same 500 rows found in the external table:

    ```SQL
    SELECT *
    FROM CameraLocation
    GO
    ```

14. Close the query window, without saving any changes. Leave the connection to the data warehouse open.

### Task 4: Import data from Data Lake Store into SQL Data Warehouse

1. Use the Azure portal to create a new Application Registration in Azure Active Directory, with the following configuration:
    - **Name**: ADLSToPolyBase2
    - **Application type**: Web app/ API
    - **Sign-on URL**: https://ADLSToPolyBase2/Dummy

    You will use this application to import the stolen vehicle data that you previously uploaded to ADLS into the SQL Data Warehouse using Polybase

    > **Note**: The actual URL entered on this blade is immaterial because you do not actually build or deploy an app at this location; it’s merely acting as an identifier in this example.

2. Make a note of the **Application ID** generated for the app (referred to as **&lt;application ID&gt;** later).
3. Create a new application key named **Key1** that expires in one year. Save the key and make a note of the value (referred to as **&lt;key&gt;** later; you might want to save the Application ID and Key values to Notepad for reference later).
4. In the **Properties** blade of your Azure Active Directory account, locate your **Directory ID** (referred to as **&lt;directory ID&gt;** later), and make a note of this ID, or save it to Notepad.
5. Go to Data explorer for your Data Lake Store (adls&lt;your name&gt;&lt;date&gt;), and add the following access permissions to the root folder for ADLSToPolyBase2:
    - **Read**, **Write**, and **Execute** permissions, for **This folder and all children**.

   > **IMPORTANT**. Wait while permissions are assigned to **ADLSPolyBase**; notice the assignments to each file and folder is displayed in the **Assigning permissions to ...** box, in the **Access** blade. Do not continue with the exercise until you see the notification that the permissions assignment has successfully completed (this might take several minutes).

6. On the desktop, in Microsoft SQL Server Management Studio, open the file **E:\\Labfiles\\Lab07\\Exercise3\\StolenVehicleDataScript.sql**.
7. Ensure that the **trafficserver*&lt;your name&gt;&lt;date&gt;** server is selected, and then execute the following command in the script to create another database scoped credential. This credential will be used for accessing the ADLA account. Replace **&lt;application ID&gt;**, **&lt;directory ID&gt;**, and **&lt;key&gt;** with the values you noted in Steps 2 and 3:

    ```SQL
    CREATE DATABASE SCOPED CREDENTIAL ADLCredential
    WITH
        IDENTITY = '<application ID>@https://login.windows.net/<directory ID>/oauth2/token',
        SECRET = '<key>'
    GO
    ```

8. Execute the following command in the script to create an external data source named **StolenVehicleDataSource** that connects to the Data Lake storage account using this credential. Replace **&lt;Data Lake Store name&gt;** with **adls&lt;_your name_&gt;&lt;_date_&gt;**:

    ```SQL
    CREATE EXTERNAL DATA SOURCE StolenVehicleDataSource
    WITH (
        TYPE = HADOOP,
        LOCATION = 'adl://<Data Lake Store name>.azuredatalakestore.net',
        CREDENTIAL = ADLCredential
    )
    GO
    ```

9. Execute the following command in the script to define the file format of the data to be read from this data source; it is CSV data but some of the strings are delimited with quotes that should be not included in the data:

    ```SQL
    CREATE EXTERNAL FILE FORMAT DelimitedCsvTextFileFormat
    WITH (
        FORMAT_TYPE = DelimitedText,
        FORMAT_OPTIONS (
            FIELD_TERMINATOR = ',',
            STRING_DELIMITER = '"'
    ));
    GO
    ```

10. Execute the following command in the script to create an external table named **ExternalStolenVehicleData** that uses the data source. The table should contain the following three fields:
    - VehicleRegistration: VARCHAR(7) NOT NULL
    - DateStolen: VARCHAR(25) NOT NULL
    - DateRecovered: VARCHAR(25) NULL

    ```SQL
    CREATE EXTERNAL TABLE ExternalStolenVehicleData (
        VehicleRegistration VARCHAR(7) NOT NULL,
        DateStolen DATETIME NOT NULL,
        DateRecovered DATETIME NULL )
    WITH (
        LOCATION='/Stolen/',
        DATA_SOURCE = StolenVehicleDataSource,
        FILE_FORMAT = DelimitedCsvTextFileFormat,
        REJECT_TYPE = PERCENTAGE,
        REJECT_VALUE = 5,
        REJECT_SAMPLE_VALUE = 1000
    );
    GO
    ```

    It might take a couple of minutes to create this table. Note that:
    - DateRecovered must allow NULL values.
    - The date fields are defined as VARCHAR rather than DATETIME in the external table. When you transfer the data into the data warehouse you will convert this data, and also extract the year from the DateStolen field to use as the partition key.
    - Not all of the rows in each file in ADLS contain valid data (there are some header rows, and there could also be some other forms of corruption, given the volume of the data). Specify that you will allow up to 5 percent of the rows retrieved by a query to be discarded if they do not appear to contain valid data. Sample over every 1,000 rows.

11. Execute the following command in the script to verify that the external table has been created correctly, by reading the first 1,000 rows of stolen vehicle data:

    ```SQL
    SELECT TOP(1000) *
    FROM ExternalStolenVehicleData
    GO
    ```

    This query is likely to be slow (it might take three or four minutes).

12. Execute the following command in the script. This command to populates the **StolenVehicle** table, converts the VARCHAR date columns in the external table into DATETIME values, extracts the year from the DateStolen column in the external table, and stores the result in the **YearStolen** column in the StolenVehicle table:

    ``` SQL
    INSERT INTO StolenVehicle(VehicleRegistration, DateStolen, DateRecovered, YearStolen)
    SELECT VehicleRegistration, CONVERT(DATETIME, DateStolen), CONVERT(DATETIME, DateRecovered), DATEPART(yyyy, CONVERT(DATETIME, DateStolen))
    FROM ExternalStolenVehicleData
    GO
    ```

    This operation will take some time to perform (three or four minutes); additionally, you should see that 2,914 rows are rejected (these are the rows containing headers rather than data)—this is normal.

13. Execute the following command in the script to verify that the **StolenVehicle** table has been populated. The query should return 1,000 rows, but will be much faster than querying the data from ADLS via PolyBase:

    ```SQL
    SELECT TOP(1000) *
    FROM StolenVehicle
    GO
    ```

14. Close the SQL Server Management Studio, without saving any changes.

### Task 5: Import data from an on-premises SQL Server database into SQL Data Warehouse

1. In Visual Studio 2017, create a new Integration Service Project called **ImportVehicleOwnerData**.
2. When the project has been created, drag a **Data Flow Task** from the SSIS Toolbox onto the Design window.
3. Click the **Data Flow** tab at the top of the Design pane.
4. In the **SSIS Toolbox**, expand **Other Sources**, and then drag the **ADO.NET Source** onto the Data Flow window.
5. Open the **ADO.NET Source** component to display the **ADO.NET Source Editor**.
6. Click **New** to display the **Configure ADO.NET Connection Manager** dialog box.
7. Click **New** to display the **Connection Manager** dialog box, and enter the following details:
    - **Server name**: LON-SQL
    - **Connect to a database**: VehicleInfo
8. In the **Connection Manager** dialog box, click **OK**.
9. In the **Configure ADO.NET Connection Manager** dialog box, click **OK**.
10. In the **ADO.NET Source Editor** dialog box, in the **Name of the table or the view drop-down** **list** box, click **dbo.VehicleOwner**, click **"dbo"."VehicleOwner"**, and then click **Columns**.
11. Verify that all of the columns in the **External Column** table are mapped to the corresponding columns in the **Output Column** table, and then click **OK**, and then click **OK**.
12. In the **SSIS Toolbox**, expand **Other Destinations**, and then drag the **ADO.NET Destination** onto the Data Flow window below the existing ADO.NET source.
13. Click the **ADO.NET Source** component, and then click and drag the blue connector to the **ADO.NET Destination**.
14. Open the **SQL Server Destination** component to display the **ADO.NET Destination Editor**.
15. By the **Connection Manager** drop-down list box, click **New** to display the **Configure ADO.NET Connection Manager** dialog box.
16. Click **New** to display the **Connection Manager** dialog box, and enter the following details, and then click **OK**:
    - **Server Name**: trafficserver&lt;_your name_&gt;&lt;_date_&gt;.database.windows.net
    - **Authentication**: SQL Server Authentication
    - **Username**: student
    - **Password**: Pa55w.rd
    - Select **Save my password**
    - **Database Name**: trafficwarehouse
17. In the **Configure ADO.NET Connection Manager** dialog box, click **OK**.
18. In the **ADO.NET Destination Editor** window, in the **Use a table or view** drop-down list box, click **"dbo"."VehicleOwner"**, and then click **Mappings**.
19. Verify that all of the columns in the source table are mapped to the corresponding columns in the destination table, and then click **OK**.
20. Save the project.
21. On the **Debug** menu, click **Start Debugging** to run the package and perform the upload. This upload transfers 7.7 million rows, and can take a few minutes to complete.
22. Stop debugging, and close Visual Studio.

>**Result**: In this exercise, you:
>
> - Staged data in Data Lake Store prior to SQL Data Warehouse import.
> - Staged data in an on-premises SQL Server database prior to SQL Data Warehouse import.
> - Imported data from a local CSV file directly into SQL Data Warehouse.
> - Imported data from Data Lake Store into SQL Data Warehouse.
> - Imported data from an on-premises SQL Server database into SQL Data Warehouse.

## Exercise 4: Stream dynamic data to SQL Data Warehouse

### Scenario

In this exercise, you will consolidate the data storage for the traffic surveillance system by using SQL Data Warehouse as a single data location for dynamic data streamed live from the speed cameras. You will stream dynamic data from speed cameras into SQL Data Warehouse from a Stream Analytics job, leaving the existing table in place. You will then configure a Visual Studio app to use this Stream Analytics job, and then view the Stream Analytics job data in SQL Data Warehouse.

The main tasks for this exercise are as follows:

1. Configure an Azure Stream Analytics job to output to SQL Data Warehouse
2. Configure a Visual Studio app to use the Stream Analytics job
3. View Stream Analytics job data in SQL Data Warehouse
4. Lab cleanup

### Task 1: Configure an Azure Stream Analytics job to output to SQL Data Warehouse

1. Using the Azure portal, open the **CaptureTrafficData** Azure Stream Analytics job; remember that this job captures data from the speed cameras and writes it to Data Lake storage.
2. Add a SQL Database output to the job, using the following details:
    - **Output alias**: DataWarehouse
    - **Select SQL Database from your subscriptions**: selected
    - **Database**: trafficwarehouse
    - **Username**: student
    - **Password**: Pa55w.rd
    - **Table**: VehicleSpeed
    - Leave all other settings at their defaults
3. Wait until the output has been successfully created before continuing with the lab.
4. Edit the query for the job, and add the following SELECT statement:

    ```SQL
    SELECT
        CameraID, SpeedLimit, Speed, VehicleRegistration, Time AS WhenDate, DATEPART(month, Time) AS WhenMonth
    INTO
        DataWarehouse
    FROM
        CameraDataFeed4
    ```

    You can copy this query from the file **E:\\Labfiles\\Lab07\\Exercise4\\ASAquery.txt**.

    Note that the names of the columns or aliases in the SELECT statement must match the names of the columns in the table in the SQL Data Warehouse; remember that the VehicleSpeed table uses the WhenMonth column to partition the data.

5. Start the **TrafficAnalytics** job.
6. Wait until the job has been successfully started before continuing with the lab.

### Task 2: Configure a Visual Studio app to use the Stream Analytics job

1. In the Azure portal, find and note the primary access key for the **camerafeeds&lt;your name&gt;&lt;date&gt;** namespace.
2. Start Visual Studio, and open the **SpeedCameraDevice** solution in the **E:\\Labfiles\\Lab07\\Exercise4\\SpeedCameraDevice** folder.
3. Edit the **app.config** file and in the **Microsoft.ServiceBus.ConnectionString** setting, replace the text **YourNamespace** with **camerafeeds&lt;your name&gt;&lt;date&gt;**, and replace the text **YourPrimaryKey** with the primary access key for the event hub. Note that this version of the application captures data from 500 speed cameras.
4. Set **SpeedCameraDriver** as the startup project, and then build the solution.
5. Verify that the app compiles successfully then start the app. The app opens a console window displaying generated speed camera data that is being sent to the event hub.

### Task 3: View Stream Analytics job data in SQL Data Warehouse

1. Start Microsoft SQL Server Management Studio, and connect to **trafficserver&lt;_your name_&gt;&lt;_date_&gt;.database.windows.net** as **student**, with the password **Pa55w.rd**.
2. Use Object Explorer to run a **Select Top 1000 Rows** query against **dbo.VehicleSpeed**.
3. Verify that the first 1000 rows of data are shown. These speed records should have the current date.
4. Run the following query:

   ```SQL
   SELECT COUNT(*)
   FROM dbo.VehicleSpeed
   ```

   This query returns the number of records in the VehicleSpeed table. Repeat this query several times to verify that new rows are being continually added.

### Task 4: Lab cleanup

1. Stop the **SpeedCameraDevice** application.
2. Close Visual Studio and SQL Server Management Studio.
3. Stop the **CaptureTrafficData** Stream Analytics job.

   > **VERY IMPORTANT**: Pause the **trafficwarehouse** data warehouse; if you don't do this, you will quickly run out of Azure credits! Remember that resources are billed for all the time that the data warehouse is running, even if it is not actively performing a task. If you are not going to use the data warehouse for a while, you should always click **Pause** to stop the warehouse and release resources. You will not be billed for the time the data warehouse is paused.

>**Result**: In this exercise, you configured a Stream Analytics job to output data to SQL Data Warehouse, configured a Visual Studio app to use the Stream Analytics job, and viewed data from a Stream Analytics that was sent to SQL Data Warehouse.

---

©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
