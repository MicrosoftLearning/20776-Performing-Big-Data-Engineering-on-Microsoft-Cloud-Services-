# Module 7: Implementing Azure SQL Data Warehouse

- [Module 7: Implementing Azure SQL Data Warehouse](#module-7-implementing-azure-sql-data-warehouse)
    - [Lab: Implementing SQL Data Warehouse](#lab-implementing-sql-data-warehouse)
    - [Exercise 1: Create and configure a new SQL Data Warehouse](#exercise-1-create-and-configure-a-new-sql-data-warehouse)
        - [Task 1: Install AzCopy and AdlCopy](#task-1-install-azcopy-and-adlcopy)
        - [Task 2: Create a new database server](#task-2-create-a-new-database-server)
        - [Task 3: Create a new SQL Data Warehouse](#task-3-create-a-new-sql-data-warehouse)
        - [Task 4: Explore the SQL Data Warehouse using SQL Server Management Studio](#task-4-explore-the-sql-data-warehouse-using-sql-server-management-studio)
        - [Task 5: Use scaling with SQL Data Warehouse](#task-5-use-scaling-with-sql-data-warehouse)
    - [Exercise 2: Design and configure SQL Data Warehouse tables](#exercise-2-design-and-configure-sql-data-warehouse-tables)
        - [Task 1: Design tables and indexes for a SQL Data Warehouse application](#task-1-design-tables-and-indexes-for-a-sql-data-warehouse-application)
        - [Task 2: Use SQL Server Management Server to create data warehouse tables and indexes](#task-2-use-sql-server-management-server-to-create-data-warehouse-tables-and-indexes)
    - [Exercise 3: Import static data into SQL Data Warehouse](#exercise-3-import-static-data-into-sql-data-warehouse)
        - [Task 1: Stage data in Data Lake Store prior to SQL Data Warehouse import](#task-1-stage-data-in-data-lake-store-prior-to-sql-data-warehouse-import)
        - [Task 2: Stage data in an on-premises SQL Server database prior to SQL Data Warehouse import](#task-2-stage-data-in-an-on-premises-sql-server-database-prior-to-sql-data-warehouse-import)
        - [Task 3: Import data from a local CSV file into SQL Data Warehouse](#task-3-import-data-from-a-local-csv-file-into-sql-data-warehouse)
        - [Task 4: Import data from Data Lake Store into SQL Data Warehouse](#task-4-import-data-from-data-lake-store-into-sql-data-warehouse)
        - [Task 5: Import data from an on-premises SQL Server database into SQL Data Warehouse](#task-5-import-data-from-an-on-premises-sql-server-database-into-sql-data-warehouse)
    - [Exercise 4: Stream dynamic data to SQL Data Warehouse](#exercise-4-stream-dynamic-data-to-sql-data-warehouse)
        - [Task 1: Configure an Azure Stream Analytics job to output to SQL Data Warehouse](#task-1-configure-an-azure-stream-analytics-job-to-output-to-sql-data-warehouse)
        - [Task 2: Configure a Visual Studio app to use the Stream Analytics job](#task-2-configure-a-visual-studio-app-to-use-the-stream-analytics-job)
        - [Task 3: View Stream Analytics job data in SQL Data Warehouse](#task-3-view-stream-analytics-job-data-in-sql-data-warehouse)
        - [Task 4: Lab cleanup](#task-4-lab-cleanup)

## Lab: Implementing SQL Data Warehouse

Before starting this lab, performing the following steps:

1. Using Internet Explorer, go to **https://go.microsoft.com/fwlink/?linkid=2014060**.
2. In the Internet Explorer message box, click **Run**.
3. In the **Microsoft SQL Server Data Tools** dialog box, on the **Welcome** page, click **Next**.
4. In the **Install tools to this Visual Studio 2017 instance** list, click Visual Studio Enterprise 2017.
5. Under **Install tools for these SQL Server features**, select all the check boxes, and then click **Install**.
6. In the **User Account Control** dialog box, click **Yes**.
7. On the **Setup Completed** page, click **Restart** to reboot the virtual machine.
8. Log on to the virtual machine as **ADATUM\\AdatumAdmin** with the password **Pa55w.rd**.
9. Start Internet Explorer and go to **https://www.microsoft.com/en-us/download/details.aspx?id=54798**, and then click **Download**.
10. On the **Choose the download you want** page, select the **SsisAzureFeaturePack\_2017\_x86.msi** check box, and then click **Next**.
11. In the Internet Explorer message box, click **Run**.
12. In the **Microsoft SQL Server Feature Pack Setup** dialog box, on the **Welcome to the Microsoft SQL Server 2017 Integration Services Feature Pack for Azure (x86) Setup Wizard** page, click **Next**.
13. On the **Please read the Microsoft SQL Server 2017 Integration Services Feature Pack for Azure (x86) License Agreement** page, select the **I accept the terms in the License Agreement** check box, and then click **Install**.
14. In the **User Account Control** dialog box, click **Yes**.
15. On the **Completed the Microsoft SQL Server 2017 Integration Services Feature Pack for Azure (x64) Setup Wizard** page, click **Finish**.
16. Close Internet Explorer.

## Exercise 1: Create and configure a new SQL Data Warehouse

### Task 1: Install AzCopy and AdlCopy

1. Ensure that the **MT17B-WS2016-NAT**, **20776A-LON-DC**, **20776A-LON-SQL**, and **20776A-LON-DEV** virtual machines are running, and then log on to **20776A-LON-DEV** as **ADATUM\\AdatumAdmin** with the password **Pa55w.rd**.
2. Start **Internet Explorer** and go to **https://aka.ms/downloadazcopy**
3. In the Internet Explorer message box, click Run.
4. In the **Microsoft Azure Storage AzCopy** dialog box, on the **Welcome to the Microsoft Azure Storage AzCopy Setup Wizard** page, click **Next**.
5. On the **End-User License Agreement** page, select the **I accept the terms in the License Agreement** check box, and then click **Next**.
6. On the **Destination Folder** page, click **Next**.
7. On the **Ready to install Microsoft Azure Storage AzCopy** page, click **Install**.
8. In the **User Account Control** dialog box, click **Yes**.
9. On the **Completed the Microsoft Azure Storage AzCopy Setup Wizard** page, click **Finish**.
10. Right-click the Windows **Start** button, click **System**, and then click **Advanced system settings**.
11. In the **System Properties** dialog box, click **Environment Variables**.
12. In the **Environment Variables** dialog box, under **User variable for AdatumAdmin**, click **Path**, and then click **Edit**.
13. In the **Edit User Variable** dialog box, in the **Variable value** box, at the end of the existing text, type **C:\Program Files (x86)\Microsoft SDKs\Azure\AzCopy;**, and then click **OK**.
14. Leave the **Environment Variables** dialog box open.
15. In Internet Explorer, open a new tab, and go to **https://www.microsoft.com/en-us/download/details.aspx?id=50358**.
16. On the **AdlCopy** page, click **Download**.
17. In the Internet Explorer message box, click **Run**.
18. In the **AdlCopy** dialog box, on the **Welcome to the AdlCopy Setup Wizard** page, click **Next**.
19. On the **License Agreement** page, click **I Agree**, and then click **Next**.
20. On the **Select Installation Folder** page, click **Next**.
21. On the **Confirm Installation** page, click **Next**.
22. In the **User Account Control** dialog box, click **Yes**.
23. On the **Installation Complete** page, click **Close**.
24. Return to the **Environment Variables** dialog box.
25. Under **User variable for AdatumAdmin**, click **Path**, and then click **Edit**.
26. In the **Edit environment variable** dialog box, click **New**.
27. In the text box, type **%HOMEPATH%\Documents\AdlCopy**, and then click **OK**.
28. In the **Environment Variables** dialog box, click **OK**.
29. In the **System Properties** dialog box, click **OK**.
30. Close the System window.

### Task 2: Create a new database server

1. In Internet Explorer, go to the Azure portal at https://portal.azure.com, and sign in using the account associated with your Azure Training Pass.
2. In the Azure portal, click **+ Create  resource**, in the **Search marketplace** box, type **sql server**, and then click **SQL server (logical server)**.
3. On the **SQL server (logical server)** blade, click **Create**.
4. On the **SQL Server (logical server only)** blade, enter the following details:
    - **Server name**: trafficserver&lt;_your name_&gt;&lt;_date_&gt;
    - **Server admin login**: student
    - **Password**: Pa55w.rd
    - **Confirm password**: Pa55w.rd
5. Under **Resource group**, click **Use existing**, and then click **CamerasRG**.
6. In the **Location** list, select the same location as you have used for Data Lake Stores in previous labs.
7. Ensure that the **Allow azure services to access server** check box is selected.
8. Leave all other settings at their defaults, and click **Create**.
9. Wait until the server has deployed before continuing with the lab.
10. In the Azure portal, click **All resources**, and then click **trafficserver&lt;_your name_&gt;&lt;_date_&gt;**.
11. On the **trafficserver&lt;_your name_&gt;&lt;_date_&gt;** blade, under **Security**, click **Firewall and virtual networks**, and then click **Add client IP**.
12. Ensure that **Allow access to Azure services** in set to **ON**, click **Save**, and then click **OK**.

### Task 3: Create a new SQL Data Warehouse

1. In the Azure portal, click **+ Create a resource**, click **Databases**, and then click **SQL Data Warehouse**.
2. On the **SQL Data Warehouse** blade, in the **Database name** box, type **trafficwarehouse**.
3. Under **Resource group**, click **Use existing**, and then click **CamerasRG**.
4. In the **Select source** list, click **Blank database**.
5. Click **Server**.
6. On the **Server** blade, click **trafficserver&lt;_your name_&gt;&lt;_date_&gt;**.
7. On the **SQL Data Warehouse** blade, click **Performance level**.
8. On the **Configure performance** blade, on the **Gen1** tab, drag the **Scale your system** slider to the left and set it to **DW100**, and then click **Apply**.
9. On the **SQL Data Warehouse** blade, leave all other settings at their defaults, and click **Create**.
10. Wait until the data warehouse has deployed before continuing with the lab.

### Task 4: Explore the SQL Data Warehouse using SQL Server Management Studio

1. On the desktop, on the Windows **Start** menu, type **Microsoft SQL Server Management Studio**, and then press Enter.
2. In the **Connect to Server** dialog box, in the **Server name** box, type **trafficserver&lt;_your name_&gt;&lt;_date_&gt;.database.windows.net**.
3. In the **Authentication** list, click **SQL Server Authentication**.
4. In the **Login** box, type **student**.
5. In the **Password** box, type **Pa55w.rd**, and then click **Connect**.
6. In Object Explorer, expand **Databases**, and verify that the **trafficwarehouse** data warehouse is listed.
7. Right-click **trafficwarehouse**, and then click **New Query**.
8. In the SQL Editor, type the following statement, and then click **Execute**:

    ```SQL
    SELECT *
    FROM sys.dm_pdw_nodes
    GO
    ```

    You can copy the SQL statements in this exercise from the file **E:\\Labfiles\\Lab07\\Exercise1\\Exercise1.sql**.

9. Verify that this select statement lists a single **CONTROL** node and a single **COMPUTE** node.
10. Make a note of the value in the **pdw\_node\_id** column of the **COMPUTE** node.
11. In the SQL Editor, replace the existing code with the following statement, and then click **Execute**:

    ```SQL
    SELECT *
    FROM sys.pdw_distributions
    GO
    ```

12. Verify that you see 60 distributions (databases) listed; these distributions should all belong to the **COMPUTE** node (use the value recorded for the **pdw\_node\_id** column in the previous query to verify this).

### Task 5: Use scaling with SQL Data Warehouse

1. Switch to the Azure portal.
2. In the Azure portal, click **All resources**, and then click **trafficwarehouse**.
3. On the **trafficwarehouse** blade, under **COMMON TASKS**, click **Scale**.
4. On the **trafficwarehouse - Scale** blade, set the **Performance level** to **DW400**, click **Save**, and then click **Yes**.
5. Wait until the data warehouse has resumed after the scaling operation before continuing with the lab; this might take several minutes.
6. Switch to Microsoft SQL Server Management Studio.
7. In Object Explorer, click the **Connect Object Explorer** button.
8. In the **Connect to Server** dialog box, in the **Server name** box, type **trafficserver&lt;_your name_&gt;&lt;_date_&gt;.database.windows.net**.
9. In the **Authentication** list, click **SQL Server Authentication**.
10. In the **Login** box, type **student**.
11. In the **Password** box, type **Pa55w.rd**, and then click **Connect**.
12. In Object Explorer, click **trafficwarehouse**.
13. In the SQL Editor, replace the existing code with the following statement, and then click **Execute**:

    ```SQL
    SELECT *
    FROM sys.dm_pdw_nodes
    GO
    ```

14. Verify that this time you see four **COMPUTE** nodes, because you have scaled performance by a factor of four.
15. In the SQL Editor, replace the existing code with the following statement, and then click **Execute**:

    ```SQL
    SELECT *
    FROM sys.pdw_distributions
    GO
    ```

16. Verify that you still see 60 distributions, but they are now spread across the compute nodes (15 distributions per node).
17. Switch to the Azure portal.
18. On the **trafficwarehouse - Scale** blade, set the **Performance level** back to **DW100**, click **Save**, and then click **Yes**.
19. Wait until the data warehouse has resumed after the scaling operation before continuing with the lab; this might take several minutes. It’s important to use an appropriate performance level because SQL Data Warehouse is an expensive resource, and should only be scaled as needed.
20. Close the **trafficwarehouse - Scale** blade.

>**Result**:  In this exercise, you created a new database server, and a new data warehouse. You explored the data warehouse using SQL Server Management Studio. Finally, you scaled the data warehouse.

## Exercise 2: Design and configure SQL Data Warehouse tables

### Task 1: Design tables and indexes for a SQL Data Warehouse application

The traffic monitoring system uses the following information:

- **Traffic camera locations**. There are 500 traffic cameras, recording traffic speeds and capturing vehicle registrations. The location of each camera rarely changes.
- **Vehicle speeds**. This data is continually streamed from the traffic cameras. It comprises the identity of the camera, speed limit (this can vary with time), vehicle speed, and vehicle registration. This data is used to perform analysis of traffic patterns, identify hotspots, and so on. It’s also used to send fixed penalty fines and summonses to owners of vehicles caught speeding, and to notify traffic patrols if a suspect (stolen or false) vehicle registration is captured.
- **Vehicle/owner data**. This dataset contains the current vehicle ownership records for every registered vehicle. This data includes the registration number, together with the name (title, forename, surname), and address (lines 1-4) for every vehicle. Currently, there are approximately 7.7 million vehicles registered, but new vehicles are registered often and older vehicles unregistered as they are written off.
- **Stolen vehicles**. This dataset contains the registration number, date reported stolen, and date recovered (or null if the vehicle is still missing) for every vehicle reported stolen. This data is historical—it’s appended to (and updated as vehicles are recovered), but data is never deleted. There are currently 1.2 million stolen vehicle records (approximately 2 percent of vehicles [150,000] are reported stolen each year, and there are currently eight years of data on record—2010-2017 inclusive). The data is expected to grow at a similar rate in the future.

**Question 1:**
What distribution policy would be most appropriate for each type of data?

**Answer**:
**Vehicle speeds**. Hashed by camera ID. This is high volume data that is likely to grow quickly. Hashing by camera ID will help to spread the load across the data warehouse but ensure that it is found quickly by analytics that examine traffic patterns.
**Traffic camera locations**. Replicated (small amounts of data that might need to be joined frequently with vehicle speed data).
**Vehicle/owner data**. Round robin. This data has a relatively slow-moving footprint. It is often retrieved by vehicle registration (making an argument for hashing by registration number), but queries that determine ownership by address are also frequently performed.
**Stolen vehicles**: Hashed by registration number. Most of the queries that reference this data retrieve it by registration number.

**Question 2:**
How might your partition the data (if at all)?

**Answer**:

- **Vehicle speeds**. Partition by month. Most queries to this data are likely to be about recent incidents, so the current month's data (and possibly the previous month) are the most likely to be accessed. The data also grows by time, so it’s easy for the data warehouse to determine in which partition new data should go (the current month). Also, older data is eventually archived by month more easily.
- **Traffic camera locations**. No partitions.
- **Vehicle/owner data**. No partitions.
- **Stolen vehicles**. Partition by year. Most records for unrecovered vehicles will be for the current/previous year. Older data is more likely to indicate a recovered date.

### Task 2: Use SQL Server Management Server to create data warehouse tables and indexes

1. In Microsoft SQL Server Management Studio, in Object Explorer, click the **Connect Object Explorer** button.
2. In the **Connect to Server** dialog box, in the **Server name** box, type **trafficserver&lt;_your name_&gt;&lt;_date_&gt;.database.windows.net**.
3. In the **Authentication** list, click **SQL Server Authentication**.
4. In the **Login** box, type **student**.
5. In the **Password** box, type **Pa55w.rd**, and then click **Connect**.
6. In Object Explorer, right-click **trafficwarehouse**, and then click **New Query**.
7. In the SQL Editor, type the following statements, and then click **Execute**:

    ```SQL
    CREATE TABLE VehicleSpeed
    (
        CameraID VARCHAR(10) NOT NULL,
        SpeedLimit INT NOT NULL,
        Speed INT NOT NULL,
        VehicleRegistration VARCHAR(7) NOT NULL,
        WhenDate DATETIME NOT NULL,
        WhenMonth INT NOT NULL
    )
    WITH
    (
        HEAP,
        DISTRIBUTION = HASH(CameraID),
        PARTITION (WhenMonth RANGE FOR VALUES(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12))
    )
    GO

    CREATE TABLE CameraLocation
    (
        CameraID VARCHAR(10) NOT NULL,
        GPSLocationX FLOAT NOT NULL,
        GPSLocationY FLOAT NOT NULL
    )
    WITH
    (
        HEAP,
        DISTRIBUTION = REPLICATE
    )
    GO

    CREATE TABLE VehicleOwner
    (
        VehicleRegistration VARCHAR(7) NOT NULL,
        Title VARCHAR(30) NOT NULL,
        Forename VARCHAR(30) NOT NULL,
        Surname VARCHAR(30) NOT NULL,
        AddressLine1 VARCHAR(50) NOT NULL,
        AddressLine2 VARCHAR(50) NOT NULL,
        AddressLine3 VARCHAR(50) NOT NULL,
        AddressLine4 VARCHAR(50) NOT NULL
    )
    WITH
    (
        CLUSTERED COLUMNSTORE INDEX,
        DISTRIBUTION = ROUND_ROBIN
    )
    GO

    CREATE TABLE StolenVehicle
    (
        VehicleRegistration VARCHAR(7) NOT NULL,
        DateStolen DATETIME NOT NULL,
        DateRecovered DATETIME NULL,
        YearStolen INT NOT NULL
    )
    WITH
    (
        CLUSTERED COLUMNSTORE INDEX,
        DISTRIBUTION = HASH(VehicleRegistration),
        PARTITION (YearStolen RANGE FOR VALUES(2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017))
    )
    GO
    ```

     You can copy the SQL statements for this exercise from the file **E:\\Labfiles\\Lab07\\Exercise2\\Exercise2.sql**.

8. In Object Explorer, expand **trafficwarehouse**, expand **Tables**, and verify that the **dbo.VehicleSpeed**, **dbo.CameraLocation**, **dbo.VehicleOwner**, and **dbo.StolenVehicle** tables have been created.

>**Result**: In this exercise, you designed tables and indexes for a data warehouse application, and used SQL Server Management Server to create the required data warehouse tables and indexes.

## Exercise 3: Import static data into SQL Data Warehouse

### Task 1: Stage data in Data Lake Store prior to SQL Data Warehouse import

> **Note**: This task is only necessary if you have not performed exercise 3 of module 6 which uses the same data. If you have already staged the stolen vehicle data in Data Lake Store, then skip to the next task to stage data in an on-premises SQL Server database.

1. In the Azure portal, click **+ Create a resource**, click **Storage**, and then click **Storage account - blob, file, table, queue**.
2. On the **Create Storage account** blade, by **Resource group**, click **Use existing**, and then click **CamerasRG**
3. In the **Storage account name** box, type **vehicledata&lt;_your name_&gt;&lt;_date_&gt;**.
4. In the **Location** list, select the same location as you used for the data warehouse in Exercise 1.
5. In the **Account kind** list, click **Blob storage**.
6. Leave all other details at their defaults, and click **Review + create**, and then click **Create**.
7. Wait until the storage account has been successfully created before continuing with the exercise.
8. Click **All resources**, and then click **vehicledata&lt;_your name_&gt;&lt;_date_&gt;**.
9. On the **vehicledata&lt;_your name_&gt;&lt;_date_&gt;** blade, under **BLOB SERVICE**, click **Blobs**, and then click **+ Container**.
10. In the **New container** dialog box, in the **Name** box, type **stolen**, and then click **OK**.
11. On the **vehicledata&lt;_your name_&gt;&lt;_date_&gt; -** **Containers** blade, under **SETTINGS**, click **Access keys**.
12. Next to **key1**, click the **Click to copy** button, to copy the key to the clipboard.
13. On the desktop, right-click the Windows **Start** button, and then click **Command Prompt (Admin)**.
14. In the **User Account Control** dialog box, click **Yes**.
15. At the command prompt, type the following command, and then press Enter:

    ```CMD
    dir E:\Labfiles\Lab07\Exercise3\StolenVehicles /s
    ```

    Note that the **StolenVehicles** folder contains eight years of stolen vehicle data, organized in subfolders by year/month/day; there are 2,914 separate CSV files.

16. At the command prompt, type the following command. Replace **&lt;storage account name&gt;** with **vehicledata&lt;_your name_&gt;&lt;_date_&gt;**, replace **&lt;storage key&gt;** with the key you copied to the clipboard, and then press Enter:

    ```CMD
    azcopy /Source:"E:\Labfiles\Lab07\Exercise3\StolenVehicles" /Dest:https://<storage account name>.blob.core.windows.net/stolen /DestKey:<storage key> /S
    ```

    You can copy this command from the file **E:\\Labfiles\\Lab07\\Exercise3\\AzCopyCmd1.txt**.

    The copy process might take several minutes to complete. Wait until all files have been copied before continuing with the exercise.

17. On the Windows **Start** menu, type **Visual Studio 2017**, and then press Enter.
18. In Visual Studio, on the **View** menu, click **Cloud Explorer**.
19. In Cloud Explorer, if the account associated with your Azure Learning Pass subscription folder is not listed, perfrm the following steps:
    1. Click the **Azure** **Account settings** icon, and then click **Add an account**.
    2. In the **Sign in to your account** dialog box, sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.
    3. In Cloud Explorer, select your Azure Learning Pass subscription, and then click **Apply**..
20. Under your Azure Learning Pass subscription folder, expand **Storage Accounts**, expand **vehicledata&lt;_your name_&gt;&lt;_date_&gt;**, expand **Blob Containers**, and then double-click **stolen**.
21. Verify that the files and subfolders have been uploaded.
22. Switch to the Azure portal.
23. Click **All resources**, click **adls&lt;_your name_&gt;&lt;_date_&gt;**, and then click **Data explorer**.
24. In the Data explorer blade, click **New Folder**.
25. In the **Create new folder** box, type **Stolen**, and then click **OK**.
26. Return to the command prompt.
27. At the command prompt, type the following command. Replace **&lt;storage account name&gt;** with **vehicledata&lt;_your name_&gt;&lt;_date_&gt;**, replace **&lt;Data Lake Store name&gt;** with **adls&lt;_your name_&gt;&lt;_date_&gt;**, replace **&lt;storage key&gt;** with the blob store key you copied to the clipboard, and then press Enter:

    ```CMD
    adlcopy /source https://<storage account name>.blob.core.windows.net/stolen/ /dest adl://<Data Lake Store name>.azuredatalakestore.net/Stolen/ /sourcekey <storage key>
    ```

    You can copy this command from the file **E:\\Labfiles\\Lab07\\Exercise3\\AdlCopyCmd.txt**.

    If a **Visual Studio** dialog box appears prompting you to log in, sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.

    Depending on the region and the location of the Blob storage account (ideally they should be in the same region, but might not be), AdlCopy will take from two to 20 minutes to copy the data. Ignore the stats that indicate the percentage of files copied—it sits at 0.00% until complete then jumps to 100%, and may copy the data in three phases (files 1 to 1000, then files 1001 to 2000, and then the remainder).

28. Switch to the Azure portal.
29. In the Data explorer blade for the ADLS account, click the **Stolen** folder, and verify that all the files and folders have been uploaded.

### Task 2: Stage data in an on-premises SQL Server database prior to SQL Data Warehouse import

1. On the desktop, use **File Explorer** to move to the **E:\\Labfiles\\Lab03\\Exercise3** folder.
2. Double-click **ownerdata.part01.exe**.
3. In the **WinRAR self-extracting archive** dialog box, in the **Destination Folder** box, type **E:\\Labfiles\\Lab07\\Exercise3**, and then click **Extract**.
4. Wait while the **ownerdata.csv** file is extracted from the archive. This file contains the sample data that you will stage in the SQL Server database.
5. Switch to Microsoft SQL Server Management Studio.
6. In Object Explorer, click the **Connect Object Explorer** button.
7. In the **Connect to Server** dialog box, in the **Server name** box, type **LON-SQL**.
8. In the **Authentication** list, click **Windows Authentication**, and then click **Connect**.
9. In Object Explorer, expand **LON-SQL**, right-click **Databases**, and then click **New Database**.
10. In the **New Database** dialog box, in the **Database name** box, type **VehicleInfo**, and then click **OK**.
11. In Object Explorer, expand **Databases**, right-click **VehicleInfo**, and click **New Query**.
12. In the SQL Editor, type the following commands, and then click **Execute**:

    ```SQL
    USE VehicleInfo
    GO
    -- Create VehicleOwner table
    CREATE TABLE VehicleOwner
    (
        VehicleRegistration VARCHAR(7) NOT NULL,
        Title VARCHAR(30) NOT NULL,
        Forename VARCHAR(30) NOT NULL,
        Surname VARCHAR(30) NOT NULL,
        AddressLine1 VARCHAR(50) NOT NULL,
        AddressLine2 VARCHAR(50) NOT NULL,
        AddressLine3 VARCHAR(50) NOT NULL,
        AddressLine4 VARCHAR(50) NOT NULL
    )
    GO
    ```

    You can copy these commands from the file **E:\\Labfiles\\Lab07\\Exercise3\\SqlCmd1.txt**.

13. Switch to the command prompt.
14. At the command prompt, run the following commands:

    ```CMD
    E:
    cd E:\Labfiles\Lab07\Exercise3
    bcp VehicleInfo.dbo.VehicleOwner in ownerdata.csv /T /SLON-SQL /c /t,
    ```

    Ensure that you include the comma at the end of the line. The bcp command should upload more than 7.7 million rows to the database (it’s fairly quick, and should take no more than five minutes).

    You can copy these commands from the file  **E:\\Labfiles\\Lab07\\Exercise3\\BcpCmd1.txt**.

15. Switch to Microsoft SQL Server Management Studio.
16. In the SQL Editor, replace the existing code with the following statement, and then click **Execute**:

    ```SQL
    SELECT TOP(1000) *
    FROM VehicleOwner
    GO
    ```

    You can copy this command from the file  **E:\\Labfiles\\Lab07\\Exercise3\\SqlCmd2.txt**.

    This command should display the first 1,000 rows of data; the names and addresses contain random strings, but the vehicle registrations tie in with those of the stolen vehicle data (and the data generated by the speed cameras that you will use in Exercise 4).

17. Close the SQL Editor window, without saving any changes.
18. In Object Explorer, right-click **LON-SQL**, and then click **Disconnect**.

### Task 3: Import data from a local CSV file into SQL Data Warehouse

1. Switch to the Azure portal.
2. Click **All resources**, and then click **vehicledata&lt;_your name_&gt;&lt;_date_&gt;**.
3. On the **vehicledata&lt;_your name_&gt;&lt;_date_&gt;** blade, under **BLOB SERVICE**, click **Blobs**, and then click **+ Container**.
4. In the **New container** dialog box, in the **Name** box, type **locationdata**, and then click **OK**.
5. On the **vehicledata&lt;_your name_&gt;&lt;_date_&gt;** blade, under **SETTINGS**, click **Access keys**.
6. Next to **key1**, click the **Click to copy** button, to copy the key to the clipboard.
7. ON the desktop, switch to the command prompt.
8. At the command prompt, type the following command. Replace **&lt;storage account name&gt;** with **vehicledata&lt;_your name_&gt;&lt;_date_&gt;**, replace **&lt;storage key&gt;** with the key you copied to the clipboard, and then press Enter:

    ```CMD
    azcopy /Source:"E:\Labfiles\Lab07\Exercise3\" /Pattern:CameraData.csv /Dest:https://<storage account name>.blob.core.windows.net/locationdata /DestKey:<storage key>
    ```

    You can copy this command from the file **E:\\Labfiles\\Lab07\\Exercise3\\AzCopyCmd2.txt**.

9. Switch to Microsoft SQL Server Management Studio.
10. In Object Explorer, click **Connect Object Explorer**.
11. In the **Connect to Server** dialog box, in the **Server name** box, type **trafficserver&lt;_your name_&gt;&lt;_date_&gt;.database.windows.net**.
12. In the **Authentication** list, click **SQL Server Authentication**.
13. In the **Login** box, type **student**.
14. In the **Password** box, type **Pa55w.rd**, and then click **Connect**.
15. In Object Explorer, expand **trafficserver&lt;_your name_&gt;&lt;_date_&gt;.database.windows.net**, expand **Databases**, right-click **trafficwarehouse**, and then click **New Query**.
16. In the **Microsoft SQL Server Management Studio** message box, click **Yes, Always**.
17. In the SQL Editor, type the following command and then click **Execute**:

    ```SQL
    CREATE MASTER KEY ENCRYPTION BY PASSWORD='Pa55w.rd'
    GO
    ```

    You can find all the commands for this task in the file **E:\\Labfiles\\Lab07\\Exercise3\\SpeedCameraLocationScript.sql**.

18. In the SQL Editor, highlight your edited version of the preceding command, and on the toolbar, click **Execute**.
19. In the SQL Editor, type the following command. Replace **&lt;storage account name&gt;** with **vehicledata&lt;your name&gt;&lt;date&gt;**, and replace **&ltstorage key&gt;** with the key you copied to the clipboard earlier:

    ```SQL
    CREATE DATABASE SCOPED CREDENTIAL CredentialsToBlobStorage
    WITH IDENTITY = '<storage account name>',
    SECRET = '<storage key>';
    GO
    ```
20. In the SQL Editor, highlight your edited version of the preceding command, and on the toolbar, click **Execute**.
21. In the SQL Editor, type the following command. Replace **&lt;storage account name&gt;** with **vehicledata&lt;your name&gt;&lt;date&gt;**:

    ```SQL
    CREATE EXTERNAL DATA SOURCE LocationDataSource
    WITH (
        TYPE = HADOOP,
        LOCATION = 'wasbs://locationdata@<storage account name>.blob.core.windows.net',
        CREDENTIAL = CredentialsToBlobStorage
    )
    GO
    ```

22. In the SQL Editor, highlight your edited version of the preceding command, and on the toolbar, click **Execute**.
23. In the SQL Editor, type the following command, highlight it, and then click **Execute**:

    ```SQL
    CREATE EXTERNAL FILE FORMAT CommaSeparatedFileFormat
    WITH (
        FORMAT_TYPE = DelimitedText,
        FORMAT_OPTIONS (FIELD_TERMINATOR =',')
    )
    GO
    ```

24. In the SQL Editor, type the following command, highlight it, and then click **Execute**:

    ```SQL
    CREATE EXTERNAL TABLE ExternalLocationData (
        CameraID VARCHAR(10) NOT NULL,
        GPSLocationX FLOAT NOT NULL,
        GPSLocationY FLOAT NOT NULL)
    WITH (
        LOCATION='CameraData.csv',
        DATA_SOURCE = LocationDataSource,
        FILE_FORMAT = CommaSeparatedFileFormat,
        REJECT_TYPE = VALUE,
        REJECT_VALUE = 0
    )
    GO
    ```

25. In the SQL Editor, type the following command, highlight it, and then click **Execute**:

    ```SQL
    SELECT *
    FROM ExternalLocationData
    GO
    ```

26. In the SQL Editor, type the following command, highlight it, and then click **Execute**:

    ```SQL
    DROP TABLE CameraLocation
    GO
    ```

27. In the SQL Editor, type the following command, highlight it, and then click **Execute**:

    ```SQL
    CREATE TABLE CameraLocation
    WITH
    (
        HEAP,
        DISTRIBUTION = REPLICATE
    )
    AS SELECT CameraID, GPSLocationX, GPSLocationY
    FROM ExternalLocationData
    GO
    ```

28. In the SQL Editor, type the following command, highlight it, and then click **Execute**:

    ```SQL
    SELECT *
    FROM CameraLocation
    GO
    ```

29. Close the SQL Editor window, without saving any changes. Leave the connection to the data warehouse open.

### Task 4: Import data from Data Lake Store into SQL Data Warehouse

1. Switch to the Azure portal.
2. Click **Azure Active Directory**
3. On the Azure Active Directory blade, under **MANAGE**, click **App registrations**, and then click **New application registration**.
4. On the **Create** blade, enter the following configuration, and then click **Create**:
    - **Name**: ADLSToPolyBase2
    - **Application type**: Web app / API
    - **Sign-on URL**: https://ADLSToPolyBase2/Dummy

    You will use this application to import the stolen vehicle data that you previously uploaded to ADLS into the SQL Data Warehouse using Polybase.

    > **Note**: The actual URL entered on this blade is immaterial because you don’t actually build or deploy an app at this location; it’s merely acting as an identifier in this example.

5. In the **ADLSToPolyBase2 Registered app** blade, next to **Application ID**, and then click the **Click to copy** button.
6. On the desktop, on the Windows **Start** menu, type **Notepad**, and then press Enter.
7. In Notepad, type **&lt;application ID&gt;**, and then press Enter.
8. On the **Edit** menu, click **Paste**, to store the copied Application ID.
9. On the **File** menu, click **Save**, and save the file as **App\_details.txt** in your **Documents** folder.
10. Return to the Azure portal.
11. In the **ADLSToPolyBase2 Registered app** blade, click **Settings**.
12. On the **Settings** blade, under **API ACCESS**, click **Keys**.
13. On the **Keys** blade, enter the following information, and then click **Save**:
    - **Description**: Key1
    - **Expires**: In 1 year
14. Select the **VALUE** for **Key1**, then right-click, and click **Copy**.
15. Switch to Notepad.
16. Press CTRL+END, press Enter twice, type **Key1**, and then press Enter.
17. On the **Edit** menu, click **Paste**, to store the API key.
18. Return to the Azure portal.
19. Close the **Keys**, **Settings**, **Settings**, and **ADLSToPolyBase2** blades.
20. On the **Azure Active Directory** blade, under **MANAGE**, click **Properties**.
21. On the **Properties** blade, next to **Directory ID**, click the **Click to copy** button.
22. Switch to Notepad.
23. Press CTRL+END, press Enter twice, type **Directory ID**, and then press Enter.
24. On the **Edit** menu, click **Paste**, to store the Directory ID.
25. In Notepad, on the **File** menu, click **Save**.
26. Return to the Azure portal.
27. Click **All resources**, click **adls&lt;_your name_&gt;&lt;_date_&gt;**, and then click **Data explorer**.
28. In the Data explorer blade, ensure that the root folder (**adls&lt;_your name_&gt;&lt;_date_&gt;**) is open, and then click **Access**.
29. On the **Access** blade, click **+ Add**.
30. On the **Select User or Group** blade, click **ADLSToPolyBase2**, and then click **Select**.
31. On the **Select Permissions** blade, select the **Read**, **Write**, and **Execute** check boxes, click **This folder and all children**, and then click **OK**.

    > **IMPORTANT**. Wait while permissions are assigned to **ADLSPolyBase**; notice that the assignments to each file and folder is displayed in the **Assigning permissions to ...** box, in the **Access** blade. Do not continue with the exercise until you see the notification that the permissions assignment has successfully completed (this might take several minutes).

32. Close the **Access** blade.
33. On the desktop, switch to Microsoft SQL Server Management Studio.
34. On the **File** menu, click **Open**, and then click **File**.
35. In the **Open File** dialog box, go to **E:\\Labfiles\\Lab07\\Exercise3**, click **StolenVehicleDataScript.sql**, and then click **Open**.
36. In Object Explorer, ensure that **trafficserver&lt;_your name_&gt;&lt;_date_&gt;** is selected.
37. In the SQL Editor, locate the following command:

    ```SQL
    CREATE DATABASE SCOPED CREDENTIAL ADLCredential
    WITH
        IDENTITY = '<application ID>@https://login.windows.net/<directory ID>/oauth2/token',
        SECRET = '<key>'
    GO
    ```

38. Edit the command, replacing **&lt;application ID&gt;**, **&lt;directory ID&gt;**, and **&lt;key&gt;** with the values you saved to **App\_details.txt** in Notepad.
39. In the SQL Editor, highlight your edited version of the preceding command, and on the toolbar, click **Execute**.
40. In the SQL Editor, locate the following command:

    ```SQL
    CREATE EXTERNAL DATA SOURCE StolenVehicleDataSource
    WITH (
        TYPE = HADOOP,
        LOCATION = 'adl://<Data Lake Store name>.azuredatalakestore.net',
        CREDENTIAL = ADLCredential
    )
    GO
    ```

41. Edit the command, replacing **&lt;Data Lake Store name&gt;** with **adls&lt;_your name_&gt;&lt;_date_&gt;**.
42. In the SQL Editor, highlight your edited version of the preceding command, and on the toolbar, click **Execute**.
43. In the SQL Editor, highlight the following command, and then click **Execute**:

    ```SQL
    CREATE EXTERNAL FILE FORMAT DelimitedCsvTextFileFormat
    WITH (
        FORMAT_TYPE = DelimitedText,
        FORMAT_OPTIONS (
            FIELD_TERMINATOR = ',',
            STRING_DELIMITER = '"'
    ));
    GO
    ```

44. In the SQL Editor, highlight the following command, and then click **Execute**:

    ```SQL
    CREATE EXTERNAL TABLE ExternalStolenVehicleData (
        VehicleRegistration VARCHAR(7) NOT NULL,
        DateStolen DATETIME NOT NULL,
        DateRecovered DATETIME NULL )
    WITH (
    LOCATION='/Stolen/',
        DATA_SOURCE = StolenVehicleDataSource,
        FILE_FORMAT = DelimitedCsvTextFileFormat,
        REJECT_TYPE = PERCENTAGE,
        REJECT_VALUE = 5,
        REJECT_SAMPLE_VALUE = 1000
    );
    GO
    ```

    It might take a couple of minutes to create this table. While you are waiting, note the following configuration details:
    - DateRecovered must allow NULL values.
    - The date fields are defined as VARCHAR rather than DATETIME in the external table. When you transfer the data into the data warehouse, you will convert this data, and also extract the year from the DateStolen field to use as the partition key.
    - Not all of the rows in each file in ADLS contain valid data (there are some header rows, and there could also be some other forms of corruption, given the volume of the data). Specify that you will allow up to 5 percent of the rows retrieved by a query to be discarded if they do not appear to contain valid data. Sample over every 1,000 rows.
  
45. In the SQL Editor, highlight the following command, and then click **Execute**:

    ```SQL
    SELECT TOP(1000) *
    FROM ExternalStolenVehicleData
    GO
    ```

    This query is likely to be slow (three or four minutes).

46. In the SQL Editor, highlight the following command, and then click **Execute**:

    ```SQL
    INSERT INTO StolenVehicle(VehicleRegistration, DateStolen, DateRecovered, YearStolen)
    SELECT VehicleRegistration, CONVERT(DATETIME, DateStolen), CONVERT(DATETIME, DateRecovered), DATEPART(yyyy, CONVERT(DATETIME, DateStolen))
    FROM ExternalStolenVehicleData
    GO
    ```

    This operation will take some time to perform (three or four minutes). Additionally, you should see that 2,914 rows are rejected (these are the rows containing headers rather than data). This is normal.

47. In the SQL Editor, highlight the following command, and then click **Execute**:

    ```SQL
    SELECT TOP(1000) *
    FROM StolenVehicle
    GO
    ```

48. Close the SQL Server Management Studio without saving any changes.

### Task 5: Import data from an on-premises SQL Server database into SQL Data Warehouse

1. Switch to Visual Studio
2. On the **File** menu, click **New**, and then click **Project**.
3. In the **New Project** dialog box, expand **Business Intelligence**, click **Integration Services**, and then click **Integration Service Project**.
4. In the **Name** box, type **ImportVehicleOwnerData**, and then click **OK**.
5. In the **SSIS Toolbox**, double-click **Data Flow Task**, to add this object to the Design pane.
6. At the top of the Design pane, click the **Data Flow** tab.
7. In the **SSIS Toolbox**, expand **Other Sources**, double-click **ADO NET Source**, to add this object to the Data Flow Task pane.
8. In the **Design** pane, double-click **ADO NET Source**.
9. In the **ADO.NET Source Editor** dialog box, click **New**.
10. In the **Configure ADO.NET Connection Manager** dialog box, click **New**.
11. In the **Configure ADO.NET Connection Manager** dialog box, in the **Server name** box, type **LON-SQL**.
12. In the **Connect to a database** section, click **Select or enter a database name**, in the box, type **VehicleInfo**, and then click **Test Connection**.
13. In the **Connection Manager** dialog box, click **OK**.
14. In the **Connection Manager** dialog box, click **OK**.
15. In the **Configure ADO.NET Connection Manager** dialog box, click **OK**.
16. In the **ADO.NET Source Editor** dialog box, in the **Name of the table or the view** list, click **"dbo"."VehicleOwner"**, and then click **Columns**.
17. Verify that all of the columns in the **External Column** table are mapped to the corresponding columns in the **Output Column** table, and then click **OK**.
18. In the **SSIS Toolbox**, expand **Other Destinations**, double-click **ADO NET Destination**, and then drag this object to the Data Flow Task pane under the **ADO NET Source object**.
19. In the **Data Flow Task** pane, click **ADO NET Source**, and then click and drag the blue connector to **ADO NET Destination**.
20. In the **Data Flow Task** pane, double-click **ADO NET Destination**.
21. In the **ADO.NET Destination Editor** dialog box, next to the **Connection manager** list, click **New**.
22. In the **Configure ADO.NET Connection Manager** dialog box, click **New**.
23. In the **Connection Manager** dialog box, in the **Server name** box, type **trafficserver&lt;_your name_&gt;&lt;_date_&gt;.database.windows.net**.
24. In the **Authentication** list, click **SQL Server Authentication**.
25. In the **User name** box, type **student**, in the **Password** box, type **Pa55w.rd**, and then select the **Save my password** check box.
26. In the **Select or enter a database name** box, type **trafficwarehouse**, and then click **OK**.
27. In the **Configure ADO.NET Connection Manager** dialog box, click **OK**.
28. In the **ADO.NET Destination Editor** dialog box, in the **Use a table or view** list, click **"dbo"."VehicleOwner"**, and then click **Mappings**.
29. Verify that all of the columns in the source table are mapped to the corresponding columns in the destination table, and then click **OK**.
30. On the **File** menu, click **Save All**.
31. On the **Debug** menu, click **Start Debugging** to run the package and perform the uploa. Wait until the upload completes before continuing.
32. On the **Debug** menu, click **Stop Debugging**.
33. Close Visual Studio.

>**Result**: In this exercise, you :
>
> - Staged data in Data Lake Store prior to SQL Data Warehouse import.
> - Staged data in an on-premises SQL Server database prior to SQL Data Warehouse import.
> - Imported data from a local CSV file directly into SQL Data Warehouse.
> - Imported data from Data Lake Store into SQL Data Warehouse.
> - Imported data from an on-premises SQL Server database into SQL Data Warehouse.

## Exercise 4: Stream dynamic data to SQL Data Warehouse

### Task 1: Configure an Azure Stream Analytics job to output to SQL Data Warehouse

1. In the Azure portal, click **All resources**, and then click the **CaptureTrafficData** Azure Stream Analytics job; remember that this job captures data from the speed cameras and writes it to Data Lake storage.
2. On the **CaptureTrafficData** blade, under **JOB TOPOLOGY**, click **Outputs**, click **+ Add**, and then click **SQL Database**.
3. On the **New output** blade, enter the following details, and then click **Save**:
    - **Output alias**: DataWarehouse
    - **Select SQL Database from your subscriptions**: selected
    - **Database**: trafficwarehouse
    - **Username**: student
    - **Password**: Pa55w.rd
    - **Table**: VehicleSpeed
4. Wait until the output has been successfully created before continuing with the lab.
5. On the **CaptureTrafficData - Outputs** blade, under **JOB TOPOLOGY**, click **Query**, and then add the following after the existing query:

    ``` SQL
    SELECT
        CameraID, SpeedLimit, Speed, VehicleRegistration, Time AS WhenDate, DATEPART(month, Time) AS WhenMonth
    INTO
        DataWarehouse
    FROM
        CameraDataFeed4
    ```

    Note that the names of the columns or aliases in the SELECT statement must match the names of the columns in the table in the SQL Data Warehouse; remember that the VehicleSpeed table uses the WhenMonth column to partition the data.

    You can copy this query from the file **E:\\Labfiles\\Lab07\\Exercise4\\ASAquery.txt**.

6. Click **Save**, and then click **Yes**.
7. On the **CaptureTrafficData - Query** blade, click **Overview**, and then click **Start**.
8. On the **Start job** blade, click **Now**, and then click **Start**.
9. Wait until the job has been successfully started before continuing with the lab.

### Task 2: Configure a Visual Studio app to use the Stream Analytics job

1. In the Azure portal, click **All resources**, and then click **camerafeeds&lt;your name&gt;&lt;date&gt;**.
2. On the **camerafeeds&lt;your name&gt;&lt;date&gt;** blade, under **SETTINGS**, click **Shared access policies**, and then click **RootManageSharedAccessKey**.
3. Click the **Click to copy** button by the **Primary key** box.
4. On the desktop, on the Windows **Start** menu, type **Visual Studio 2017**, and then press Enter.
5. In Visual Studio, on the **File** menu, click **Open**, and then click **Project/Solution**.
6. In the **Open Project** dialog box, go to the **E:\\Labfiles\\Lab07\\Exercise4\\SpeedCameraDevice** folder, click **SpeedCameraDevice.sln**, and then click **Open**.
7. In Solution Explorer, double-click the **App.config** file.
8. In App.config, in the **appSettings** section, in the **ServiceBusConnectionString** key, replace **YourNamespace** with **camerafeeds&lt;your name&gt;&lt;date&gt;**, and replace **YourPrimaryKey** with the event hub primary key in the clipboard.  Note that this version of the application captures data from 500 speed cameras.
9. In Solution Explorer, right-click **SpeedCameraDriver**, and then click **Set as StartUp Project**.
10. On the **Build** menu, click **Build Solution**.
11. Verify that the app compiles successfully, and then click **Start**. The app opens a console window displaying generated speed camera data that is being sent to the event hub.

### Task 3: View Stream Analytics job data in SQL Data Warehouse

1. On the Start menu, type **Microsoft SQL Server Management Studio**, and then press Enter.
2. In the **Connect to Server** dialog box, in the **Server name** box, type **trafficserver&lt;_your name_&gt;&lt;_date_&gt;.database.windows.net**.
3. In the **Authentication** list, click **SQL Server Authentication**.
4. In the **Login** box, type **student**.
5. In the **Password** box, type **Pa55w.rd**, and then click **Connect**.
6. In Object Explorer, expand **Databases**, expand **trafficwarehouse**, expand **Tables**, right-click **dbo.VehicleSpeed**, and then click **Select Top 1000 Rows**.
7. Verify that the first 1000 rows of data appear. These speed records should have the current date.
8. In Object Explorer, right-click **trafficwarehouse**, and then click **New Query**.
9. In the Query Editor, enter the following query:

    ```SQL
    SELECT COUNT(*)
    FROM dbo.VehicleSpeed
    ```

    This query returns the number of records in the VehicleSpeed table.
10. Click **Execute** and observe the number of rows in the table. Repeat this process several times to verify that new rows are being continually added.

### Task 4: Lab cleanup

1. Switch to the Visual Studio app window for **SpeedCameraDevice**, and press Enter to stop the app.
2. Close Visual Studio.
3. Close SQL Server Management Studio without saving changes.
4. Switch to the Azure portal.
5. Click **All resources**, and then click **CaptureTrafficData**.
6. On the **CaptureTrafficData** blade, click **Stop**, and then click **Yes**.
7. In the Azure portal, click **All resources**, and then click **trafficwarehouse**.
8. On the **trafficwarehouse** blade, click **Pause**, and then click **Yes**.

    Remember that resources are billed for all the time that the data warehouse is running, even if it is not actively performing a task. If you are not going to use the data warehouse for a while, you should click **Pause** to stop the warehouse and release resources. You will not be billed for the time the data warehouse is paused. If you don't do this, you will quickly run out of Azure credits!

>**Result**: In this exercise, you configured a Stream Analytics job to output data to SQL Data Warehouse, configured a Visual Studio app to use the Stream Analytics job, and viewed data from a Stream Analytics that was sent to SQL Data Warehouse.

---

©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
