# Module 3: Performing Custom Processing in Azure Stream Analytics

- [Module 3: Performing Custom Processing in Azure Stream Analytics](#module-3-performing-custom-processing-in-azure-stream-analytics)
    - [Demo 1: Creating a UDF and using it in a Stream Analytics job](#demo-1-creating-a-udf-and-using-it-in-a-stream-analytics-job)
        - [Scenario](#scenario)
        - [Setup](#setup)
        - [Create a new event hub namespace](#create-a-new-event-hub-namespace)
        - [Add an event hub to the new namespace](#add-an-event-hub-to-the-new-namespace)
        - [Retrieve the primary connection string for the namespace](#retrieve-the-primary-connection-string-for-the-namespace)
        - [Create a new Stream Analytics job](#create-a-new-stream-analytics-job)
        - [Add an input to the Stream Analytics job for the event hub](#add-an-input-to-the-stream-analytics-job-for-the-event-hub)
        - [Add an output to the Stream Analytics job that saves data to Blob storage](#add-an-output-to-the-stream-analytics-job-that-saves-data-to-blob-storage)
        - [Add a JavaScript UDF function to the Stream Analytics job](#add-a-javascript-udf-function-to-the-stream-analytics-job)
        - [Configure a Stream Analytics job query](#configure-a-stream-analytics-job-query)
        - [Configure an app to generate simulated stock market data](#configure-an-app-to-generate-simulated-stock-market-data)
        - [Start the Stream Analytics job](#start-the-stream-analytics-job)
        - [Start the app and generate simulated data](#start-the-app-and-generate-simulated-data)
        - [Use Azure Cloud Explorer to download results from Blob storage and examine them](#use-azure-cloud-explorer-to-download-results-from-blob-storage-and-examine-them)
        - [Stop the Stream Analytics job and stop the app](#stop-the-stream-analytics-job-and-stop-the-app)
    - [Demo 2: Calling a Machine Learning model from a Stream Analytics job](#demo-2-calling-a-machine-learning-model-from-a-stream-analytics-job)
        - [Scenario](#scenario)
        - [Setup](#setup)
        - [Edit the existing Stream Analytics job to add input and output, and modify the query](#edit-the-existing-stream-analytics-job-to-add-input-and-output-and-modify-the-query)
        - [Start the StockMarketProcessor Stream Analytics job](#start-the-stockmarketprocessor-stream-analytics-job)
        - [Simulate stock market activity and generate price data](#simulate-stock-market-activity-and-generate-price-data)
        - [Stop the Stream Analytics job and stop the app](#stop-the-stream-analytics-job-and-stop-the-app)
        - [Download the training data from the Data Lake Store](#download-the-training-data-from-the-data-lake-store)
        - [Create a Machine Learning workspace](#create-a-machine-learning-workspace)
        - [Upload training data to a new](#upload-training-data-to-a-new)
        - [Create and run a new experiment](#create-and-run-a-new-experiment)
        - [Prepare the trained model](#prepare-the-trained-model)
        - [Expose the trained model as a web service](#expose-the-trained-model-as-a-web-service)
        - [Reconfigure the Stream Analytics job to use the web service](#reconfigure-the-stream-analytics-job-to-use-the-web-service)
        - [Rerun the simulation app to use “live” data](#rerun-the-simulation-app-to-use-live-data)
        - [Demo clean up](#demo-clean-up)
  
## Demo 1: Creating a UDF and using it in a Stream Analytics job

### Scenario

This demonstration analyses the prices of stocks in a stockmarket, and attempts to work out which stocks are more volatile than others.

The **MostVolatile** UDF function returns the stock ticker, the number of price changes, and the maximum and minimum prices for the most volatile stock item from all the stock market price changes recorded in a given time window.

The definition of “most volatile” is:

- The price must have changed at least 10 times in the time window.
- The difference between the maximum and minimum prices must be greater than that of all other stocks that have changed prices at least five times.
- In the event of a tie, the number of price changes decides which is the most volatile.

### Setup

> **Important**: Due to the CPU-intensive nature of some of the tasks performed by the applications in these demos, it is recommended that you assign at least 4 virtual processors to the 20776A-LON-DEV VM before starting these demos. To do this, perform the following steps:
>   1. Shutdown the 20776A-LON-DEV VM if it is currently running
>   2. Using Hyper-V Manager, right-click the 20776A-LON-DEV VM, and then click **Settings**
>   3. Click **Processor**
>   4. In the **Processor** pane, increase **Number of virtual processors** to 4
>   5. Click **OK**
>   6. Restart the 20776A-LON-DEV VM

1. Ensure that the **MT17B-WS2016-NAT**, **20776A-LON-DC**, and **20776A-LON-DEV** virtual machines are running, and then log on to 20776A-LON-DEV as **ADATUM\\AdatumAdmin** with the password **Pa55w.rd**.
2. Using Internet Explorer, log in to the Azure portal using the account associated with your Azure pass.
3. In the Azure portal, click **+ Create a resource**, click **Storage**, and then click **Storage account - blob, file, table, queue**.
4. On the **Create storage account** blade, enter the following details, leave other settings at their default value, and then click **Create**:
    - **Name**: stocks&lt;your_initials&gt;&lt;day number of month&gt;. For example, stocksjs30
    - **Account kind**: Storage (general purpose v1)
    - **Location**: specify your nearest location
    - **Resource group**: Use existing, StocksRG
5. Wait for the storage account to be created before continuing.

### Create a new event hub namespace

1. In the Azure portal, click **+ Create a resource**, click **Internet of Things**, and then click **Event Hubs**.
2. On the **Create Namespace Event Hubs** blade, enter the following details and then click **Create**:
    - **Name**: stockmarket&lt;your_initials&gt;&lt;day number of month&gt;
    - **Pricing tier**: Standard
    - **Resource group**: Use existing, and select StocksRG.
    - **Location**: select the same location as you used for the Data Lake Store.
    - **Throughput Units**: 2
3. Wait until the namespace has deployed before continuing with the demo.

### Add an event hub to the new namespace

1. Click **All resources**, and then click **stockmarket&lt;your_initials&gt;&lt;day number of month&gt;**.
2. On the **stockmarket&lt;your_initials&gt;&lt;day number of month&gt;** blade, click **+ Event Hub**.
3. On the **Create Event Hub blade**, enter the following details, and then click **Create**:
    - **Name**: stockmarkethub
    - **Partition Count**: 16
    - **Message Retention**: 1
    - **Capture**: Off
4. Wait until the event hub has deployed before continuing with the demo.
5. On the **stockmarket&lt;your_initials&gt;&lt;day number of month&gt;** blade, under **Entities**, click **Event Hubs**.
6. In the Event Hubs list, click **stockmarkethub** (you might need to refresh the page to see the hub).
7. On the **stockmarkethub** blade, click **+ Consumer group**.
8. On the **Create consumer group** blade, in the **Name** box, type **stockmarket**, and then click **Create**.
9. Close the **stockmarkethub** blade.

### Retrieve the primary connection string for the namespace

1. On the stockmarket&lt;your_initials&gt;&lt;day number of month&gt; - Event Hubs Instance** blade, under **Settings**, click **Shared access policies**, and then click **RootManageSharedAccessKey**.
2. Next to the **Primary Key**, click the **Click to copy** button to copy the primary key to the clipboard.
3. On the **Start** menu, type **Notepad**, and then press Enter.
4. In Notepad, type **Event hub primary key**, press Enter to start a new line, and then paste the primary key.
5. Save the Notepad file as **Config_details.txt** in the **E:\\Demofiles\\Mod03** folder.

### Create a new Stream Analytics job

1. In the Azure portal, click **+ Create a resource**, click **Analytics**, and then click **Stream Analytics job**.
2. On the **New Stream Analytics job** blade, enter the following details, and then click **Create**:
    - **Job name**: StockMarketProcessor.
    - **Resource group**: Use existing, and select StocksRG.
    - **Location**: select the same location as you used for the event hub namespace
    - **Streaming units**: 3
3. Wait until the Stream Analytics job has deployed before continuing with the demo.

### Add an input to the Stream Analytics job for the event hub

1. In the Azure portal, click **All resources**, and then click **StockMarketProcessor**.
2. On the **StockMarketProcessor** blade, under **Job Topology**, click **Inputs**, click **+ Add stream input**, and then click **Event Hub**.
3. On the **Event Hub New input** blade, enter the following details, and then click **Save**:
    - **Input alias**: StockMarket
    - **Provide Event Hub settings manually**: selected
    - **Service Bus namespace**: stockmarket&lt;your_initials&gt;&lt;day number of month&gt;
    - **Event Hub name**: stockmarkethub
    - **Event hub policy name**: **RootManageSharedAccessKey**
    - **EVent Hub policy key**: use the key that you saved earlier in the **Config_details.txt** file
    - **Event hub consumer group**: stockmarket
4. Wait until the input has been successfully created before continuing with the demo.

### Add an output to the Stream Analytics job that saves data to Blob storage

1. On the **StockMarketProcessor - Inputs** blade, under **Job Topology**, click **Outputs**, click **+ Add**, and then click **Blob storage**.
2. On the **Blob stotae New output** blade, enter the following details, and then click **Save**:
    - **Output alias**: VolatileStocks
    - **Select Blob storage from your subscriptions**: selected
    - **Storage account**: stocks&lt;your_initials&gt;&lt;day number of month&gt;
    - **Container**: Create new, and name the container **volatilestocks**
    - **Path pattern**: leave empty
    - **Event serialization format**: CSV
3. Wait until the output has been successfully created before continuing with the demo

### Add a JavaScript UDF function to the Stream Analytics job

1. On the **StockMarketProcessor - Outputs** blade, under **Job Topology**, click **Functions**, click **+ Add**, and then click **Javascript UDF**.
2. On the **Javascript function New function** blade, in the **Function Alias** box, type **MostVolatile**.
3. In the **Output Type** list, click **record**.
4. Replace the template function code text with the following code. You can copy this code from the file **E:\\Demofiles\\Mod03\\UDF.txt**.

    ```JavaScript
    // UDF that returns the most volatile stock item recorded during the current time window
    function main(stockPriceChanges) {
      var stockItems = [];

      // Iterate through the records in the array and organize the data as a collection of price changes for each distinct ticker
        stockPriceChanges.forEach(
          function(item, index, array) {

            // Retrieve the data of interest from the array item
            var ticker = item.Ticker;
            var price = item.Price;

            // Check whether an existing record already exists in the stockItems array for this ticker
            var existingRecord = findStockItemRecords(ticker, stockItems);

            // If not, create a new record
            if (!existingRecord) {
                stockItems.push({Ticker: ticker, Prices: []});
                existingRecord = stockItems[stockItems.length - 1];
            }

          // Add the price information to the record for this ticke
          existingRecord.Prices.push(price);
        }
      );

      var mostVolatileTicker = "";
      var maxPrice = 0;
      var minPrice = 0;
      var numChanges = 10; // Minimum number of price changes to consider

      // Now find which item is the most volatile
      for (var i = 0; i < stockItems.length; i++) {
        var itemPriceChanges = stockItems[i].Prices.length;
        if (itemPriceChanges >= numChanges) {
          numChanges = itemPriceChanges;
          var possMaxPrice = stockItems[i].Prices[0];
          var possMinPrice = stockItems[i].Prices[0];
          for (var j = 1; j < itemPriceChanges; j++) {
            if (stockItems[i].Prices[j] >= possMaxPrice) {
              possMaxPrice = stockItems[i].Prices[j]; 
            }

            if (stockItems[i].Prices[j] <= possMaxPrice) {
              possMinPrice = stockItems[i].Prices[j]; 
            }

            if (possMaxPrice - possMinPrice >= maxPrice - minPrice) {
              maxPrice = possMaxPrice;
              minPrice = possMinPrice;
              mostVolatileTicker = stockItems[i].Ticker;
            }
          }
        }
      }

      return {Ticker: mostVolatileTicker, NumChanges: numChanges, MaxPrice: maxPrice, MinPrice: minPrice};
    }

    // Check to see whether a record for the specified ticker already exists, and if so, return it
    function findStockItemRecords(ticker, array) {

      for(var i = 0; i < array.length; i++) {
        if (array[i].Ticker === ticker) {
          return array[i];
        }
      }

      return null;
    }
    ```

5. Click **Save**.

### Configure a Stream Analytics job query

1. On the **StockMarketProcessor - Functions** blade, under **Job Topology**, and then click **Query**.
2. On the **StockMarketProcessor - Query** blade, replace the template query text with the following code:

    ```SQL
    WITH
        MarketData AS
            (SELECT UDF.MostVolatile(Collect())
                FROM
                    StockMarket
                GROUP BY
                    TumblingWindow(Minute, 2))

    SELECT
        System.Timestamp AS Time, GetRecordPropertyValue(MostVolatile, 'Ticker') AS Ticker, GetRecordPropertyValue(MostVolatile, 'NumChanges') AS NumChanges, GetRecordPropertyValue(MostVolatile, 'MaxPrice') AS MaxPrice, GetRecordPropertyValue(MostVolatile, 'MinPrice') AS MinPrice
    INTO
        VolatileStocks
    FROM
        MarketData
    ```

    This query calls the **MostVolatile** UDF to find the volatile stock during each 2 minute time window. You can copy this query from the file **E:\\Demofiles\\Mod03\\Demo1Cmd.txt**.

3. Click **Save**, and then click **Yes**.

### Configure an app to generate simulated stock market data

1. On the Windows **Start** menu, type **Visual Studio 2017**, and then press Enter.
2. On the **File** menu, point to **Open**, and then click **Project/Solution**.
3. In the **Open Project** dialog box, go to the **E:\\Demofiles\\Mod03\\StockMarketSimulation** folder, click **StockMarketSimulation.sln**, and then click **Open**.
4. In Solution Explorer, double-click the **App.config** file.
5. In the App.config file in the **appSettings** section, in the **Endpoint** value, replace the text **YourNamespace** with **stockmarket&lt;your_initials&gt;&lt;day number of month&gt;**, and replace the text **YourPrimaryKey** with the primary key you copied to Config_details.txt.
6. On the **Build** menu, click **Build StockMarketSimulation**.
7. Verify that the app compiles successfully, but don’t start it running yet.

### Start the Stream Analytics job

1. Return to the Azure portal.
2. On the **StockMarketProcessor - Query** blade, click **Overview**, and then click **Start**.
3. On the **Start job** blade, click **Now**, and then click **Start**.
4. Wait until the job has successfully started before continuing with the demo.

### Start the app and generate simulated data

1. Switch to Visual Studio, and press F5 to run the application.
2. Verify that the app opens a console window displaying generated stock market ticker data that is being sent to the event hub. Leave the app running for several minutes.
3. In the Visual Studio app window, press Enter to stop the app.

### Use Azure Cloud Explorer to download results from Blob storage and examine them

1. In Visual Studio, on the **View** menu, click **Cloud Explorer**.
2. In **Cloud Explorer**, click the **Azure Accounts settings** icon, and then click **Add an account**.
3. In the **Sign in to your account** dialog box, sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.
4. In **Cloud Explorer**, select the subscription check box for your subscription, and then click **Apply**.
5. Under your subscription, expand **Storage Accounts**, expand **stocks&lt;your_initials&gt;&lt;day number of month&gt;**, expand **Blob Containers**, and then double-click **volatilestocks**.
6. In the **volatilestocks [Container]** pane, click the CSV file, and then click the **Save As** icon.
7. In the **Save As** dialog box, go to the **E:\\Demofiles\\Mod03** folder, and then click **Save**.
8. Using **File Explorer**, go to the **E:\Demofiles\Mod03** folder, and then double-click the CSV file.
9. In Microsoft Excel, note that the stocks that are listed meet the definition of a volatile stock; the price has changed at least 10 times, and there is a significant difference between the maximum and minimum prices.
10. Close Microsoft Excel, without saving any changes.

### Stop the Stream Analytics job and stop the app

1. Return to the Azure portal.
2. On the **StockMarketProcessor** blade, click **Stop**, and then click **Yes**.
3. Switch to Visual Studio, and close the **volatilestocks [Container]** and **Cloud Explorer** panes.

## Demo 2: Calling a Machine Learning model from a Stream Analytics job

> **Note:** This demonstration assumes that you have completed demo 1.

### Scenario

In this demonstration, you will see how to create and use a machine learning experiment that analyzes data retrieved by Stream Analytics.

This demonstration continues the stock market scenario. Stream Analytics captures the tickers and new price each time a price change occurs. The Stream Analytics job uses machine learning to detect whether a price is unusual for the stock (much higher than might be expected given the price history of the stock).

### Setup

1. Ensure that the **MT17B-WS2016-NAT**, **20776A-LON-DC**, and **20776A-LON-DEV** virtual machines are running, and then log on to 20776A-LON-DEV as **ADATUM\\AdatumAdmin** with the password **Pa55w.rd**.
2. Log in to the Azure portal using the account associated with your Azure pass.
3. In the Azure portal, click **+ Create a resource**, click **Storage**, and then click **Data Lake Storage Gen1**.
4. On the **New Data Lake Storage Gen1** blade, enter the following details, and then click **Create**:
    - **Name**: adls&lt;your_initials&gt;&lt;day number of month&gt;
    - **Resource group**: Create new, and name it StocksRG
    - **Location**: Your nearest location
    - **Pricing Package**: Pay-as-you-go
5. Wait until until the Data Lake Storage account has been created before continuing.

### Edit the existing Stream Analytics job to add input and output, and modify the query

1. In the Azure portal, click **All resources**, and then click **StockMarketProcessor**.
2. On the **StockMarketProcessor Stream Analytis job** blade, under **Job Topology**, click **Outputs**, click **+ Add**, and then click **Data Lake Store**.
3. On the **Data Lake Store New output** blade, enter the following details, and then click **Save**:
    - **Output alias**: StockMarketData
    - **Select Data Lake Store from your subscriptions**: selected
    - **Account Name**: adls&lt;your_initials&gt;&lt;day number of month&gt;
    - **Path prefix pattern**: StockMarketTrainingData/
    - **Event serialization format**: CSV
    - Click **Authorize**
4. Wait until the output has been successfully created before continuing with the demo.
5. On the **StockMarketProcessor - Outputs** blade, under **Job Topology**, click **Query**.
6. In the **StockMarketProcessor - query** blade, replace the existing query text with the following query:

    ```SQL
    SELECT
      Ticker, Price, DATEPART(hour, QuoteTime) AS HourOfDay
    INTO
      StockMarketData
    FROM
      StockMarket
    ```

    You can copy the query from the file **E:\\Demofiles\\Mod03\\Demo2Cmd1.txt**.
    This query captures the hour of the day as market volatility could be time-related (prices might fall or rise quickly at the start of the trading day, for example).

7. Click **Save**, and then click **Yes**.

### Start the StockMarketProcessor Stream Analytics job

1. On the **StockMarketProcessor - Outputs** blade, click **Overview**, and then click **Start**.
2. On the **Start job blade**, click **Now**, and then click **Start**.
3. Wait until the job has successfully started before continuing with the demo.

### Simulate stock market activity and generate price data

1. Using **File Explorer**, go to the **E:\\Demofiles\\Mod03** folder, and then delete the file **StockData.txt**.

    This file contains historical data from the previous run of the app, and it is important to remove this file initially. In training mode, the app captures the randomly generated tickers and writes them to this file. In "non-training mode", the app reads the tickers from this file. This ensures that the same tickers are used in both modes. If you don't delete the file before doing the training run, the file will contain the data from multiple runs.

2. Switch to Visual Studio, and press F5 to start the StockMarketSimulation app.
3. Verify that the app opens a console window displaying generated stock market ticker data that is being sent to the event hub.
4. Leave the app to run for a few minutes while the StockMarketProcessor Stream Analytics job captures the stock market data.

### Stop the Stream Analytics job and stop the app

1. In the StockMarketSimulation app window, press Enter to stop the app.
2. Switch to the Azure portal.
3. On the **StockMarketProcessor** blade, click **Stop**, and then click **Yes**.

### Download the training data from the Data Lake Store

1. In the Azure portal, click **All resources**, and then click **adls&lt;your_initials&gt;&lt;day number of month&gt;**.
2. On the **adls&lt;your_initials&gt;&lt;day number of month&gt;** blade, click **Data Explorer**.
3. In the **Data Explorer** blade, click **StockMarketTrainingData**, and then click the CSV file to view the contents in the **File Preview** blade. Verify that the file contains records with the tickers, prices, and times.

    > **Note:** The File Preview blade only shows the first few records in the file, not the entire contents of the file.

4. Click **Download**.
5. In the **Download** blade, click **Start Download** now.
6. In the Internet Explorer message box, click the **Save** drop-down arrow, and then click **Save as**.
7. In the **Save As** dialog box, go to the **E:\\Demofiles\\Mod03** folder, in the **File name** box, type **StockMarketData**, and then click **Save**.
8. Close the Internet Explorer message box.
9. Close the **Download** blade.

### Create a Machine Learning workspace

1. In the Azure portal, click **+ Create a resource**.
2. In the **Search the Marketplace** box, type **Machine Learning Studio Workspace**, and then press Enter.
3. On the **Everything** blade, click **Machine Learning Studio Workspace**.
4. On the **Machine Learning Studio Workspace** blade, click **Create**.
5. On the **Machine Learning Studio workspace** blade, enter the following details, and then click **Create**:
    - **Workspace name**: StockMarket
    - **Resource group**: Use existing, and select StocksRG.
    - **Location**: select the nearest location to the one you used for the Data Lake Store.
    - **Storage account**: Create new, and name the account stocks2&lt;your_initials&gt;&lt;day number of month&gt;.
    - **Workspace pricing tier**: Standard
    - **Web service plan**: Create new, and accept the default name.
    - **Web service pricing plan tier**: S1 Standard
6. Wait until the workspace has been deployed before continuing.
7. Click **All resources**, and then click the **StockMarket** workspace.
8. On the **StockMarket Machine Learning Studio workspace** blade, under **Additional Links**, click **Launch Machine Learning Studio**.
9. On the **Microsoft Azure Machine Learning Studio** page, click **Sign in here**. If prompted, sign in using your Machine Learning account credentials.If you don't already have a Machine Learning account, click **Sign up here** and follow the instructions.

### Upload training data to a new  

1. On the **experiments** page, click **+ NEW**, click **DATASET**, and then click **FROM LOCAL FILE**.
2. In the **Upload a new dataset** dialog box, click **Choose file**.
3. In the **Open** dialog box, go to the **E:\\Demofiles\\Mod03** folder, click **StockMarketData.csv**, and then click **Open**.
4. In the **Upload a new dataset** dialog box, in the **SELECT A TYPE FOR THE NEW DATASET** list, click **Generic CSV File with a header (.csv)**, and then click **OK** (the tick icon).
5. Wait while the file is uploaded.

### Create and run a new experiment

1. On the **experiments** page, click **+ NEW**, click **EXPERIMENT**, and then click **Blank Experiment**.
2. In the datasets and modules pane on the left, expand **Save Datasets**, expand **My Datasets**, and then drag **StockMarketData.csv** to the workspace canvas.
3. In the **Search experiment items** box, type **Split**, and then from the module list, drag **Split Data** to the experiment canvas, below **StockMarketData.csv**.
4. Connect the **Split Data** module to the **StockMarketData.csv** dataset To do this, click the **StockMarketData.csv** output node and, keeping the mouse pressed down, draw a line from the dataset module to the upper input port of the **Split Data** module.
5. Click the **Split Data** module.
6. In the **Properties** pane, in the **Fraction of rows in the first output dataset** box, type 0.7.
7. In the **Search experiment items** box, type **Train Anomaly**, and then from the module list, drag **Train Anomaly Detection Model** to the experiment canvas, below the **Split Data** module.
8. Connect the leftmost output of the **Split Data** module to the upper right input port of the **Train Anomaly Detection Model** module (this is the Dataset input).
9. In the **Search experiment items** box, type **One-Class**, and then from the module list, drag **One-Class Support Vector Machine Model** to the experiment canvas, to the left of the **Split Data** module.
10. Connect the output of the **One-Class Support Vector Machine Model** module to the upper left input port of the **Train Anomaly Detection Model** module (this is the untrained model input).
11. Click the **One-Class Support Vector Machine Model**.
12. In the **Properties** pane, set the **nu** parameter to 0.01, and the **epsilon** parameter to 0.0001.
13. In the **Search experiment items** box, type **Score**, and then from the module list, drag **Score Model** to the experiment canvas, below the **Train Anomaly Detection Model** module.
14. Connect the output of the **Train Anomaly Detection Model** module to the leftmost input of the **Score Model** module (this is the trained model input).
15. Connect the rightmost output of the **Split Data** module to the rightmost input of the **Score Model** module (this is the Dataset input).
16. In the toolbar, click **SAVE**, and then click **RUN**.
17. When the experiment has finished running, all modules will show a green check mark to indicate that they have successfully completed.

### Prepare the trained model

1. Right-click the **Train Anomaly Detection Model** module, point to **Trained model**, and then click **Save as Trained Model**.
2. In the **Save trained model** dialog box, in the **Enter a name for the new trained model** box, type **Trained Stock Market Model**, and then click **OK** (the tick icon).
3. Right-click the **One-Class Support Vector Machine** module, and then click **Delete**.
4. Right-click the **Train Anomaly Detection Model** module, then click **Delete**.
5. In the **Search experiment items** box, type **Trained**, and then from the module list, drag **Trained Stock Market Model** to the experiment canvas, to the left of the **Split Data** module.
6. Connect the output of the **Trained Stock Market Model** module to the leftmost input of the **Score Model** module.

### Expose the trained model as a web service

1. In the toolbar, click **SET UP WEB SERVICE**.
2. If the **Step 1** dialog box appears, click **X** to close it.
3. Right-click the connection from the **Web service input** module to the **Split Data** module, and then click **Delete**.
4. Connect the output of the **Web service input** module to the rightmost input of the **Score Model** module.
5. In the toolbar, click **SAVE**, and then click **RUN** (to validate the changes). When the experiment has finished running, all modules will show a green check mark to indicate that they have completed successfully.
6. In the toolbar, click **DEPLOY WEB SERVICE**, and then click **Deploy Web Service [Classic]**.
7. On the **experiment created on &lt;date&gt;** page, copy the **API** to the **Config_details.txt** file in the **E:\\Demofiles\\Mod03** folder.
8. On the **experiment created on &lt;date&gt;** page, click **New Web Services Experience**.
9. On the **default** page, under **BASICS**, click **Configure endpoint**.
10. In the **Description** box, type **Stock market price movement anomaly detection web service**, and then click **Save**.
11. In the toolbar, click **Quickstart**.
12. On the **default** page, under **BASICS**, click **Use endpoint**.
13. Under **Basic consumption info**, copy the **Request-Response URL**, to the **Config_details.txt** file in the **E:\\Demofiles\\Mod03** folder.

### Reconfigure the Stream Analytics job to use the web service

1. Return to the Azure portal.
2. Click **All resources**, and then click **StockMarketProcessor**.
3. On the **StockMarketProcessor Stream Analytics jobs** blade, under **Job Topology**, click **Functions**, click **+ Add**, and then click **Azure ML**.
4. On the **Azure MAchine LEarning function New function**, enter the followig details and then clisk **Save**:
    - **Function Alias**: StockPriceAnomaly
    - **Provide Azure Machine Learning function settings manually**: selected
    - **URL**: paste the Request-Response URL that you copied to Config_details.txt.
    - **Key**: paste the Web service API key that you copied to Config_details.txt.
5. On the **StockMarketProcessor - Functions** blade, under **Job Topology**, click **Outputs**, click **+ Add**, and then click **Data Lake Store**.
6. On the **Data Lake Store New output** blade, enter the following details, and then click **Save**:
    - **Output alias**: SuspectPrices
    - **Select Data Lake Store from your subscriptions**: selected
    - **Account Name**: adls&lt;your_initials&gt;&lt;day number of month&gt;
    - **Path prefix pattern**: SuspectPrices/
    - **Event serialization format**: CSV
    - Click **Authorize**
7. Wait until the output has been successfully created before continuing with the demo.
8. On the **StockMarketProcessor - Outputs** blade, under **Job Topology**, click **Query**, and then replace the existing query text with the following statements:

    ```SQL
    WITH
      StockPriceAnomalies AS (
        SELECT
          StockPriceAnomaly(Ticker, Price, DATEPART(hour, QuoteTime)) AS Results 
        FROM
          StockMarket
        TIMESTAMP BY
          QuoteTime)

    SELECT
      Results.Ticker, Results.Price, Results.HourOfDay
    INTO
      SuspectPrices
    FROM
      StockPriceAnomalies
    WHERE
      Results.[Scored Labels] = '1'
    ```

    You can copy this code from the file **E:\\Demofiles\\Mod03\\Demo2Cmd2.txt**.

9. Click **Save**, and then click **Yes**.
10. On the **StockMarketProcessor - Query** blade, click **Overview**, and then click **Start**.
11. On the **Start job** blade, click **Now**, and then click **Start**.
12. Wait until the job has been successfully started before continuing with the demo.

### Rerun the simulation app to use “live” data

1. Switch to Visual Studio.
2. Edit the **App.config** file.
3. In the **appSettings** section, set the value for the **TrainingRun** key to **false**.
4. Press F5 to build and run the app.
5. Leave the app to run for a few minutes while the StockMarketProcessor Stream Analytics job captures "live" stock market data.
6. Switch to the Azure portal, click **All resources**, and then click **adls&lt;your_initials&gt;&lt;day number of month&gt;**.
7. On the **adls&lt;your_initials&gt;&lt;day number of month&gt;** blade, click **Data Explorer**.
8. On the **Data Explorer** blade, click **SuspectPrices**, and then click the CSV file to view the contents in the **File Preview** blade. 
9. Verify that the file contains records with the tickers, prices, and times, and that all the prices are at least 1000.

### Demo clean up

1. In the Azure portal, click **All resources**, and then click **StockMarketProcessor**.
2. On the **StockMarketProcessor Stream Analytics** job blade, click **Stop**, and then click **Yes**.
3. Switch to the StockMarketSimulation app window, press Enter to stop the app, and then close Visual Studio.
4. In the Azure portal, click **Resource groups**, and then click the **StocksRG** reource group.
5. On the **StocksRG Resource group** blade, click **Delete resource group**.
6. On the **Are you sure you want to delete "StocksRG"?** blade, in the **TYPE THE RESOURCE GROUP NAME:** box, type **StocksRG**, and then click **Delete**.
7. Close Internet Explorer.

---

©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
