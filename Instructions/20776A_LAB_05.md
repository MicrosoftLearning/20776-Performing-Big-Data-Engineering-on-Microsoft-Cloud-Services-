
# Module 5: Processing big data using Azure Data Lake Analytics

- [Module 5: Processing big data using Azure Data Lake Analytics](#module-5-processing-big-data-using-azure-data-lake-analytics)
    - [Lab: Processing big data using Data Lake Analytics](#lab-processing-big-data-using-data-lake-analytics)
        - [Scenario](#scenario)
        - [Objectives](#objectives)
        - [Lab Setup](#lab-setup)
    - [Exercise 1: Create and test a Data Lake Analytics job](#exercise-1-create-and-test-a-data-lake-analytics-job)
        - [Scenario](#scenario)
        - [Task 1: Prepare data files for a local Data Lake Analytics instance](#task-1-prepare-data-files-for-a-local-data-lake-analytics-instance)
        - [Task 2: Create a new Data Lake project](#task-2-create-a-new-data-lake-project)
        - [Task 3: Run a Data Lake Analytics job locally](#task-3-run-a-data-lake-analytics-job-locally)
        - [Task 4: Create a Data Lake Analytics account](#task-4-create-a-data-lake-analytics-account)
        - [Task 5: Prepare data files for a cloud Data Lake Analytics instance](#task-5-prepare-data-files-for-a-cloud-data-lake-analytics-instance)
        - [Task 6: Run a Data Lake Analytics job in the cloud](#task-6-run-a-data-lake-analytics-job-in-the-cloud)
        - [Task 7: Edit a Data Lake Analytics job to add rolling averages](#task-7-edit-a-data-lake-analytics-job-to-add-rolling-averages)
        - [Task 8: Run the Data Lake Analytics updated job](#task-8-run-the-data-lake-analytics-updated-job)
    - [Exercise 2: Use Data Lake Analytics with data joins](#exercise-2-use-data-lake-analytics-with-data-joins)
        - [Scenario](#scenario)
        - [Task 1: Prepare data files for a local Data Lake Analytics instance](#task-1-prepare-data-files-for-a-local-data-lake-analytics-instance)
        - [Task 2: Create a new Data Lake project](#task-2-create-a-new-data-lake-project)
        - [Task 3: Test the Data Lake Analytics job locally](#task-3-test-the-data-lake-analytics-job-locally)
        - [Task 4: Modify the Data Lake Analytics job to join with vehicle owner data and generate fines and summonses](#task-4-modify-the-data-lake-analytics-job-to-join-with-vehicle-owner-data-and-generate-fines-and-summonses)
        - [Task 5: Test the modified Data Lake Analytics job locally](#task-5-test-the-modified-data-lake-analytics-job-locally)
        - [Task 6: Run the updated Data Lake Analytics job in the cloud](#task-6-run-the-updated-data-lake-analytics-job-in-the-cloud)
    - [Exercise 3: Use Data Lake Analytics with SQL Database](#exercise-3-use-data-lake-analytics-with-sql-database)
        - [Scenario](#scenario)
        - [Task 1: Create a SQL Database](#task-1-create-a-sql-database)
        - [Task 2: Upload data to SQL Database and add an index](#task-2-upload-data-to-sql-database-and-add-an-index)
        - [Task 3: Store a SQL Database credential in the Data Lake Analytics catalog using PowerShell](#task-3-store-a-sql-database-credential-in-the-data-lake-analytics-catalog-using-powershell)
        - [Task 4: Configure a Visual Studio-based Data Lake Analytics job to use stored credentials](#task-4-configure-a-visual-studio-based-data-lake-analytics-job-to-use-stored-credentials)
        - [Task 5: Run a Data Lake Analytics job using stored credentials to access data in SQL Database](#task-5-run-a-data-lake-analytics-job-using-stored-credentials-to-access-data-in-sql-database)
    - [Exercise 4: Use Data Lake Analytics to categorize data and present results using Power BI](#exercise-4-use-data-lake-analytics-to-categorize-data-and-present-results-using-power-bi)
        - [Scenario](#scenario)
        - [Task 1: Create a new Data Lake project](#task-1-create-a-new-data-lake-project)
        - [Task 2: Test the Data Lake Analytics job locally then run it in the cloud](#task-2-test-the-data-lake-analytics-job-locally-then-run-it-in-the-cloud)
        - [Task 3: Use Power BI to visualize the data](#task-3-use-power-bi-to-visualize-the-data)
        - [Task 4: Lab closedown](#task-4-lab-closedown)

## Lab: Processing big data using Data Lake Analytics

### Scenario

You work for Adatum as a data engineer, and you have been asked to build a traffic surveillance system for traffic police. This system must be able to analyze significant amounts of dynamically streamed data, captured from speed cameras and automatic number plate recognition (ANPR) devices, and then crosscheck the outputs against large volumes of reference data holding vehicle, driver, and location information. Fixed roadside cameras, hand-held cameras (held by traffic police), and mobile cameras (in police patrol cars) are used to monitor traffic speeds and raise an alert if a vehicle is travelling too quickly for the local speed limit. The cameras also have built-in ANPR software that can read vehicle registration plates.

For this phase of the project, you are going to use Data Lake Analytics to calculate the average speeds detected by speed cameras, use data joins with a Data Lake Analytics job to generate speeding notices linked to vehicle owner information, scale up the system to use Data Lake Analytics speed camera data stored in the cloud in Azure SQL Database, and use Power BI to present the average speeds using map visualizations.

### Objectives

After completing this lab, you will be able to:

- Create and test a Data Lake Analytics job.
- Use Data Lake Analytics with data joins.
- Use Data Lake Analytics with SQL Database.
- Use Data Lake Analytics to categorize data and present results using Power BI.

### Lab Setup

Estimated time: 90 minutes
Virtual machine: **20776A-LON-DEV**
User name: **ADATUM\\AdatumAdmin**
Password: **Pa55w.rd**

If you haven't completed lab 2, perform the following steps to create a Data Lake Storage Account:

1. In Internet Explorer, go to http://portal.azure.com, and sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.
2. In the Azure portal, click **+ Create a resource**, click **Storage**, and then click **Data Lake Storage Gen1**.
3. On the **Data Lake Storage Gen1** blade, click **Create**.
4. On the **New Data Lake Storage Gen1** blade, in the **Name** box, type **adls&lt;_your name_&gt;&lt;_date_&gt;**.
5. Under **Resource group**, click **Create new**, and type **CamerasRG**.
6. In the **Location** list, select your nearest location from the currently available Data Lake Store regions.
7. Leave all other settings at their defaults, and click **Create**.
8. Wait until the storage has deployed before continuing with the lab.

## Exercise 1: Create and test a Data Lake Analytics job

### Scenario

You are going to use Data Lake Analytics to calculate the average speeds detected by speed cameras, across a city area. In this exercise, you will use Data Lake Analytics to analyze speed camera data, and calculate the rolling average speed over the previous 25 observations for each speed camera.

The main tasks for this exercise are as follows:

1. Prepare data files for a local Data Lake Analytics instance
2. Create a new Data Lake project
3. Run a Data Lake Analytics job locally
4. Create a Data Lake Analytics account
5. Prepare data files for a cloud Data Lake Analytics instance
6. Run a Data Lake Analytics job in the cloud
7. Edit a Data Lake Analytics job to add rolling averages
8. Run the Data Lake Analytics updated job

### Task 1: Prepare data files for a local Data Lake Analytics instance

1. Ensure that the **MT17B-WS2016-NAT**, **20776A-LON-DC**, and **20776A-LON-DEV** virtual machines are running, and then log on to **20776A-LON-DEV** as **ADATUM\\AdatumAdmin** with the password **Pa55w.rd**.
2. Using File Explorer, go to the **C:\\Users\\Student\\AppData\\Local\\USQLDataRoot** folder. This is the folder used by the local instance of ADLA for holding ADLS data during testing.

    > **Note** AppData is a hidden folder so it might not appear in **File Explorer** by default. TO show hidden items in File Explorer, click the **View** menu, and then check the **Hidden items** check box.

3. Copy the **speeds** folder and **subfolders** from **D:\\Labfiles\\Lab05\\## Exercise 1** into the **USQLDataRoot** folder. This folder contains sample speed camera data for three hours (examine the folders and the file contents—the files are in CSV format).
4. Go to the folder **C:\\Users\\AdatumAdmin\\AppData\\Local\\USQLDataRoot\\speeds\\2017\\08\\30\\16**, and double-click **speeds1.csv** to open the file in Excel. Note that, for each hour, the speeds1 file contains the camera ID, vehicle registration, recorded speed, speed limit at the camera location, camera location (latitude and longitude), and time for each record.
5. Close Microsoft Excel, without saving any changes.

### Task 2: Create a new Data Lake project

1. Using Visual Studio 2017, create a new Data Lake project named **CameraSpeeds** using the U-SQL project template. The job will analyze the data in a single file, **speeds1.csv**, in the **/speeds/2017/08/30/16 folder**. 
2. Create an extractor to retrieve the data from this file (this requires defining fields for the contents of the file). Your Script.usql should be similar to the following (you can copy this code from the file **E:\\Labfiles\\Lab05\\Exercise1\\SpeedsJob1.usql**):

    ```SQL
    @cameraData =
        EXTRACT CameraID string,
                VehicleRegistration string,
                Speed int,
                SpeedLimit int,
                LocationLatitude double,
                LocationLongitude double,
                Time DateTime
        FROM "/speeds/2017/08/30/16/speeds1.csv"
        USING Extractors.Csv(skipFirstNRows: 1);

    @avgspeeds =
        SELECT CameraID, AVG(Speed) AS AvgSpeed
        FROM @cameraData
        GROUP BY CameraID;

    OUTPUT @avgspeeds
    TO "/AverageSpeeds.csv"
    USING Outputters.Csv(outputHeader: true);
    ```

    Note that, in the first statement (@cameraData), because the source data first line contains headers, you need a command to skip over it. In the second statement (@avgspeeds), the code creates a rowset that reads the file, and calculates the average speed reported by each speed camera. In the third statement (@avgspeeds), the code saves the results to a CSV file named AverageSpeeds.csv, and includes the column header data.

### Task 3: Run a Data Lake Analytics job locally

1. Submit the job using the (Local) ADLA account in Visual Studio, and examine the simple job graph.
2. In the **Local Run Results** pane, open the **AverageSpeeds.csv** file to see the data generated by the job. It should contain 50 lines (one for each camera), displaying the camera ID and the average speed reported. The data is not sorted.

### Task 4: Create a Data Lake Analytics account

1. Using the Azure portal, create a new ADLA account, using the following details:
    - **Name**: speedsdla&lt;_your name_&gt;&lt;_date_&gt;
    - **Resource group (Use existing)**: CamerasRG
    - **Location**: use the same location as you used for your Data Lake Store
    - **Data Lake Store**: use the Data Lake Store that you created earlier (adls&lt;_your name_&gt;&lt;_date_&gt;)
    - Leave all other settings at their defaults
2. Wait until the account has deployed before continuing with the lab.

### Task 5: Prepare data files for a cloud Data Lake Analytics instance

1. In the **adls&lt;_your name_&gt;&lt;_date_&gt;** Data Lake Store, create a folder named **speeds**, and add a folder structure that matches that under the **E:\\Labfiles\\Lab05\\Exercise1\\speeds** folder on the VM.
2. Upload the data files from the subfolders under the **E:\\Labfiles\\Lab05\\Exercise1\\speeds\\2017\\08\\30** folder to the ADLS account (/speeds should be in the root folder). Do this manually in Data Explorer.

### Task 6: Run a Data Lake Analytics job in the cloud

1. In Visual Studio, in the **Script.usql** pane,  elect the new ADLA account in the toolbar, and submit the job to run in the cloud (you might need to log in to your Azure account using Server Explorer first, and then close and reopen the Script.usql window).
2. Verify that the job runs as before.
3. Use Data Explorer for your Data Lake Storage account to verify that the AverageSpeeds.csv file has been created in the root folder and that it contains the same data as before.

### Task 7: Edit a Data Lake Analytics job to add rolling averages

- In Visual Studio, edit **Script.usql** to:
  
  - Create another rowset that calculates the average speed over the last 25 observations for each speed camera.
  - Output the **CameraID**, **observation time**, and **speed observed at that time** in addition to the **rolling average**.

   Your **Script.usql** pane should now be similar to the following (you can copy this code from the file **E:\\Labfiles\\Lab05\\Exercise1\\SpeedsJob2.usql**):

    ```SQL
    @rollingAvgSpeeds =
        SELECT CameraID, Time, Speed, AVG(Speed) OVER (PARTITION BY CameraID ORDER BY Time ROWS 25 PRECEDING) AS RollingAverage
        FROM @cameraData;

    OUTPUT @rollingAvgSpeeds
    TO "/RollingAverageSpeeds.csv"
    USING Outputters.Csv(outputHeader: true);
    ```

### Task 8: Run the Data Lake Analytics updated job

1. Submit the job using your ADLA account. Notice that the job graph is more complex, and that the two rowsets are generated in parallel.
2. Close the **Job View** panes, and then close the solution.
3. Use Data Explorer in the ADLS account to view the contents of the **RollingAverageSpeeds.csv** file in the root folder.

>**Result**: In this exercise, you prepared data files for a local Data Lake Analytics instance, created a new Data Lake project, and ran this Data Lake Analytics job locally. You created a Data Lake Analytics account, prepared data files for a cloud Data Lake Analytics instance, ran this Data Lake Analytics job in the cloud. Finally, you edited the job to add rolling averages, and ran the updated job.

## Exercise 2: Use Data Lake Analytics with data joins

### Scenario

You need to be able to generate and send out speeding tickets that are addressed to the registered owners of vehicles that have been caught exceeding the local speed limit. In this exercise, you will add functionality to your Data Lake Analytics job to generate fines/summonses from speed data and vehicle owner address information.

The main tasks for this exercise are as follows:

1. Prepare data files for a local Data Lake Analytics instance
2. Create a new Data Lake project
3. Test the Data Lake Analytics job locally
4. Modify the Data Lake Analytics job to use joins
5. Test the modified Data Lake Analytics job locally
6. Run the updated Data Lake Analytics job in the cloud

### Task 1: Prepare data files for a local Data Lake Analytics instance

1. In the **C:\\Users\\Student\\AppData\\Local\\USQLDataRoot** folder, create a new folder called **vehicles**, and copy the **ownerdata.csv** file from the **E:\\Labfiles\\Lab05\\Exercise2** folder into this folder.
2. Examine this file; it contains the names and addresses of the owner for each vehicle.

### Task 2: Create a new Data Lake project

1. Using Visual Studio 2017, create a new Data Lake project named **SpeedAnalyzer** using the U-SQL project template.
2. At the start of the script, add a statement to create a new database named **VehicleData** (if it doesn't already exist).
3. Add a statement to create a table for holding speed camera information, and to index and hash the data by the speed camera ID.
4. Add a statement to remove any existing data from the table.
5. The job must only process CSV files for a specific hour on a selected date (this will be a batch job, and it must not process files that are still being written to by ASA); add a statement to declare a variable that uses pattern matching to specify the file name.
6. Add a statement to create an extractor to retrieve speed data from the selected CSV file; this requires defining virtual fields for the date, hour, and filename components of the file in addition to the fields within the file (the first line contains headers, so skip over it).
7. Add a statement to create a rowset that fetches the data for the specified date and time, and to ensure that only CSV files are read (this is specified using the Filename variable).
8. Add a statement to declare variables that specify the date and hour for the data to process (there are three hours of data—16-18, for August 30).
9. Add a statement to save the data to the Speeds table in the database for further analysis later.

    By the end of this task, your **Script.usql** should be similar to the following script. You can copy the code for this exercise from the file **E:\\Labfiles\\Lab05\\Exercise2\\VehiclesJobCmds.txt**:

    ```SQL
    CREATE DATABASE IF NOT EXISTS VehicleData;

    CREATE TABLE IF NOT EXISTS VehicleData.dbo.Speeds
    (
        CameraID string,
        VehicleRegistration string,
        Speed int,
        SpeedLimit int,
        LocationLatitude double,
        LocationLongitude double,
        Time DateTime,
        INDEX speedidx
        CLUSTERED (CameraID ASC)
        DISTRIBUTED BY HASH (CameraID)
    );

    TRUNCATE TABLE VehicleData.dbo.Speeds;

    DECLARE @speedCameraData string = @"/speeds/{Date:yyyy}/{Date:MM}/{Date:dd}/{Hour}/{Filename}";

    @speedData =
        EXTRACT CameraID string,
                VehicleRegistration string,
                Speed int,
                SpeedLimit int,
                LocationLatitude double,
                LocationLongitude double,
                Time DateTime,
                Date DateTime,
                Hour int,
                Filename string
        FROM @speedCameraData
        USING Extractors.Csv(skipFirstNRows: 1);

        DECLARE @selectedDate DateTime = new DateTime(2017, 8, 30);
        DECLARE @hour int = 16; // Could also be 17 or 18

    @speedRecords =
        SELECT CameraID, VehicleRegistration, Speed, SpeedLimit, LocationLatitude, LocationLongitude, Time
        FROM @speedData
        WHERE Date == @selectedDate AND Hour == @hour AND Filename LIKE "%.csv";

    INSERT INTO VehicleData.dbo.Speeds (CameraID, VehicleRegistration, Speed, SpeedLimit, LocationLatitude, LocationLongitude, Time)
    SELECT CameraID, VehicleRegistration, Speed, SpeedLimit, LocationLatitude, LocationLongitude, Time
    FROM @speedRecords;
    ```

### Task 3: Test the Data Lake Analytics job locally

1. Run the job locally, and verify that it executes successfully.
2. Using Server Explorer, expand **Azure**, expand **Data Lake Analytics**, expand **(Local)**, expand **U-SQL Databases**, expand **VehicleData**, expand **Tables**, right-click **dbo.Speeds**, and then click **Preview**. Verify that the Speeds table contains the data copied from the CSV file (it will be in a different order, due to the index). Note that the preview has run another Data Lake Analytics job that fetches the data in the table and returns it as a TSV file.

### Task 4: Modify the Data Lake Analytics job to join with vehicle owner data and generate fines and summonses

1. Edit **Script.usql** to add a statement to create a table for recording fines and summonses.
2. Add a statement to define a rowset that fetches the vehicle owner data from the **ownerdata.csv** file (and skip the headers in the first line).
3. Add a statement to create a rowset that joins the speed data with the vehicle owner data, so that the application will know where to send fines/summonses. The code should combine owner title and names into a single string, and concatenate the address information into another single string (with semicolons between each line of the address).
4. Add a statement to process each row of data from the speed camera as follows:
    - If the speed shown is less than the speed limit plus 5 percent, do not raise a fixed penalty or summons.
    - If the speed shown is between the speed limit plus 5 percent and the speed limit plus 30 mph, generate a fixed penalty fine.
    - If the speed is greater than the speed limit plus 30 mph, then issue a summons to appear in court.
5. Add a statement to save the details for fines and summonses to the Fines table in the database, and do not record data for legal traffic speeds.
6. Add a statement to copy the data to an output file; note that you cannot read data from the VehicleData.dbo.Fines table as it could be mutating. By the end of this task, your **Script.usql** should be similar to the following code. You can copy the code for this exercise from the file **E:\\Labfiles\\Lab05\\Exercise2\\VehiclesJobCmds2.txt**:

    ```SQL
    CREATE TABLE IF NOT EXISTS VehicleData.dbo.Fines
    (
        CameraID string,
        VehicleRegistration string,
        Speed int,
        SpeedLimit int,
        TimeCaught DateTime,
        PenaltyType string, // "P" for a Fixed Penalty (£100), "S" for a Summons (for excessive speeds, fine amount to be determined by a magistrate)
        PenaltyIssuedWhen DateTime,
        PenaltyIssuedTo string, // Full name of owner, including title
        OffenderAddress string, // Full address of the owner
        INDEX finesidx
        CLUSTERED (CameraID ASC)
        DISTRIBUTED BY HASH (CameraID)
    );

    @vehicleOwnerData =
        EXTRACT VehicleRegistration string,
                Title string,
                Forename string,
                Surname string,
                AddressLine1 string,
                AddressLine2 string,
                AddressLine3 string,
                AddressLine4 string
        FROM "/vehicles/ownerdata.csv"
        USING Extractors.Csv(skipFirstNRows: 1);

    @speedsAndOwners =
        SELECT S.CameraID AS CameraID, S.VehicleRegistration AS VehicleRegistration, S.Speed AS Speed,  S.SpeedLimit AS SpeedLimit,
            S.Time AS TimeCaught, O.Title + " " + O.Forename + " " + O.Surname AS PenaltyIssuedTo,
            O.AddressLine1 + "; " + O.AddressLine2 + "; " + O.AddressLine3 + "; " + O.AddressLine4 AS OffenderAddress
        FROM @speedRecords AS S
        JOIN @vehicleOwnerData AS O
        ON S.VehicleRegistration == O.VehicleRegistration;

    @fines =
        SELECT *, DateTime.UtcNow AS PenaltyIssuedWhen,
            Speed >= SpeedLimit * 1.05 && Speed <= SpeedLimit + 30 ? "P" :
            Speed >= SpeedLimit + 30 ? "S" : "" AS PenaltyType
        FROM @speedsAndOwners;

    INSERT INTO VehicleData.dbo.Fines (CameraID, VehicleRegistration, Speed, SpeedLimit, TimeCaught, PenaltyType, PenaltyIssuedWhen, PenaltyIssuedTo, OffenderAddress)
    SELECT CameraID, VehicleRegistration, Speed, SpeedLimit, TimeCaught, PenaltyType, PenaltyIssuedWhen, PenaltyIssuedTo, OffenderAddress
    FROM @fines
    WHERE !String.IsNullOrEmpty(PenaltyType);

    @results =
        SELECT CameraID, VehicleRegistration, Speed, SpeedLimit, TimeCaught, PenaltyType, PenaltyIssuedWhen, PenaltyIssuedTo, OffenderAddress
        FROM @fines
        WHERE !String.IsNullOrEmpty(PenaltyType);

    OUTPUT @results
    TO "/FinesAndSummonses.csv"
    USING Outputters.Csv(outputHeader: true);
    ```

### Task 5: Test the modified Data Lake Analytics job locally

1. Run the job locally, and verify that it executes successfully.
2. Examine the job graph and note that there are now more processing steps shown.
3. Using Server Explorer, verify that the Fines table contains the details of any fines and summonses.

### Task 6: Run the updated Data Lake Analytics job in the cloud

1. Using the Azure portal, go to the ADLS account for the ADLA account created in Exercise 1.
2. Use Data Explorer to create a new folder named **/vehicles** and upload the **E:\\Labfiles\\Lab05\\Exercise2\\ownerdata.csv** file into this folder.
3. In Visual Studio, run the job again, using your ADLA account.
4. Verify that it executes successfully.
5. When the job has completed, examine the job graph.
6. In the Azure portal, go to the ADLS account and use Data Explorer to view the **FinesAndSummonses.csv** file generated by the job.
7. In Visual Studio, use Server Explorer to examine the contents of the **Speeds** and **Fines** tables in the **VehicleData** database created in the catalog for the ADLA account. You should see the same data as before, when the job was run locally. Additionally, verify that the **Fines** table contains the same details of fines and summonses as the **FinesAndSummonses.csv** file written to Data Lake Storage.

>**Result**: In this exercise, you prepared data files for a local Data Lake Analytics instance, created a new Data Lake project and tested this job locally. Then you modified the job to include data joined from another data source, and tested the job locally before running it in the cloud.

## Exercise 3: Use Data Lake Analytics with SQL Database

### Scenario

You need to scale up the speed camera system to use speed camera data stored in the cloud in Azure SQL Database and then ensure that Data Lake Analytics can use this data in a secure manner. In this exercise, you will replace the “temporary” vehicle owner CSV file used in the previous exercise with data held in a SQL Database. You will create a data source for this database, using stored database credentials, and update the job to use this database rather than the CSV file.

The main tasks for this exercise are as follows:

1. Create a SQL Database
2. Upload data to SQL Database and add an index
3. Store a SQL Database credential in the Data Lake Analytics catalog using PowerShell
4. Configure a Visual Studio-based Data Lake Analytics job to use stored credentials
5. Run a Data Lake Analytics job using stored credentials to access data in SQL Database

### Task 1: Create a SQL Database

1. Using the Azure portal, create a new SQL Database, with the following details:
    - **Database name**: VehicleDB
    - **Resource group (Use existing)**: CamerasRG
    - **Location**: select the same location as you used for the Data Lake Store
    - **Source**: Blank database
    - **Server**: New server, with the following details:
    - **Server name**: camerasdbs&lt;your name&gt;&lt;date&gt;
    - **Server admin login**: student
    - **Password**: Pa55w.rd
    - **Location**: select the same location as you used for the Data Lake Store that you created in Exercise 1
    - Ensure that the **Allow azure services to access server** check box is selected
    - **Pricing tier**: Basic
    - Leave all other settings at their defaults
2. Wait until the database and server have deployed before continuing with the lab.
3. Configure the server firewall to add your Client IP address, and enable the **Allow access to Azure Services** setting.

### Task 2: Upload data to SQL Database and add an index

1. Use SQL Server Management Studio to upload the Vehicle Owner data in the file  **E:\\Labfiles\\Lab05\\Exercise3\\ownerdata.csv** to the database. Note that the first data row in this file contains column names.
2. Verify that the transfer is successful (the operation should upload 1782 records to the database). You can ignore any data truncation warnings.
3. Verify that the data has been uploaded by viewing the top 1000 rows in the table. The table should contain vehicle registration numbers and the details of their owners.
4. Add an index to the **dbo.ownerdata**, using the following SQL command. You can copy this SQL code from the folder **E:\\Labfiles\\Lab05\\Exercise3\\Index.txt**:

    ```SQL
    CREATE INDEX RegIdx
    ON [dbo].[ownerdata]
    (
        [Vehicle Registration]
    )
    GO
    ```

### Task 3: Store a SQL Database credential in the Data Lake Analytics catalog using PowerShell

1. Run the **E:\\Labfiles\\Lab05\\Exercise3\\Setup.cmd** script as administrator.
2. Start PowerShell ISE as administrator, and open the script **E:\\Labfiles\\Lab05\\Exercise3\\CreateCredential.ps1**:

    ```PowerShell
    # Install Azure PowerShell modules
    Install-Module AzureRM -AllowClobber;
    Import-Module AzureRM;

    # Login to Azure
    Login-AzureRmAccount;

    # Create credential
    New-AzureRmDataLakeAnalyticsCatalogCredential -AccountName "<name of your ADLA account>" -DatabaseName "VehicleData" -CredentialName "VehicleOwnerDataCredential" -Credential (Get-Credential) -DatabaseHost "<name of your Azure SQL Database Server>.database.windows.net" -Port 1433;

    # Remove credential
    # Remove-AzureRmDataLakeAnalyticsCatalogCredential -AccountName "<name of your ADLA account>" -DatabaseName "VehicleData" -Name "VehicleOwnerDataCredential";
    ```

3. Edit the script, and replace **&lt;name of your ADLA account&gt;** with **speedsdla&lt;_your name_&gt;&lt;_date_&gt;**.
4. Replace **&lt;name of your Azure SQL Database Server&gt;** with **camerasdbs&lt;_your name_&gt;&lt;_date_&gt;**.
5. Save the updated script.
6. Execute lines 1-3, to install the latest AzureRM cmdlets. If you get a warning message about installing modules from an untrusted repository, click **Yes to All**.
7. Execute lines 5-6, to log in to Azure, using the details of the Microsoft account that is associated with your Azure Learning Pass subscription.
8. Execute lines 8-9, to create a credential in your ADLA catalog that contains the connection information for connecting to your SQL Database.
9. When prompted, enter the following credentials:
    - **User name**: student
    - **Password**: Pa55w.rd

    Note that these credentials, for accessing the database server, are stored and encrypted as part of the credential in the catalog. Note also that, in the script, the **-DatabaseName** parameter is the name of the database in the ADLA catalog that will hold the credential, and is *not* the name of the SQL database to which the credential connects! Note also that the script shows the **Remove-AzureRmDataLakeAnalyticsCatalogCredential** command (commented out); this is how you remove a credential from the catalog. You can use this command if you make a mistake when creating the credential; you can then recreate the credential with the correct information.

### Task 4: Configure a Visual Studio-based Data Lake Analytics job to use stored credentials

1. Using Visual Studio, open the **SpeedAnalyzer** starter project in the **E:\\Labfiles\\Lab05\\Exercise3** folder. This project is a sample solution for the U-SQL job that you created in Exercise 2.
2. In **Script.usql**, locate the comment **// TODO: Switch the context to the VehicleData database in the catalog**, then on the next line, add the following USE statement for the credential store:

    ```SQL
    USE DATABASE VehicleData;
    ```

3. Locate the comment **// TODO: Drop the VehicleOwner Data Source if it already exists**, then on the next line, add the following statement:

    ```SQL
    DROP DATA SOURCE IF EXISTS VehicleOwner;
    ```

4. Locate the comment **// TODO: Create the VehicleOwner data source**, then on the next line, add the following statement to create the data source:

    ```SQL
    CREATE DATA SOURCE VehicleOwner
    FROM AZURESQLDB
    WITH (
        PROVIDER_STRING = "Database=VehicleDB;Trusted_Connection=False;Encrypt=True;",
        CREDENTIAL = VehicleOwnerDataCredential ,
        REMOTABLE_TYPES = (bool, byte, sbyte, short, int, long, decimal, float, double, string, DateTime)
    );
    ```

5. Locate the comment **// TODO: Fetch the vehicle owner data from the SQL database**, then on the next line, add the following statement to create a rowset from the dbo.ownerdata table held by the SQL database, using the data source you just created:

    ```SQL
    @vehicleOwnerData =
        SELECT [Vehicle Registration] AS VehicleRegistration, [Title] AS Title, [Forename] AS Forename, [Surname] AS Surname,
            [Address Line 1] AS AddressLine1, [Address Line 2] AS AddressLine2, [Address Line 3] AS AddressLine3, [Address Line 4] AS AddressLine4
        FROM EXTERNAL VehicleOwner
        LOCATION "dbo.ownerdata";
    ```

6. Leave the remainder of the script as it is; it is unchanged from Exercise 2, and performs the same tasks, but now uses the data retrieved from the SQL database instead of a flat file.

### Task 5: Run a Data Lake Analytics job using stored credentials to access data in SQL Database

1. In Visual Studio, submit the job using your ADLA account. This script must be run in the cloud, because the credential was created in the ADLA account, not locally.
2. Verify that the job runs successfully and, when the job has completed, examine the job graph.
3. Use Data Explorer to view the **FinesAndSummonses.csv** file generated by the job, and verify that it contains the same data as before.
4. In Visual Studio, use Server Explorer to examine the contents of the Speeds and Fines tables in the database created in the catalog for the ADLA account. You should see the same data as before, when the job was run locally using the data from the CSV file.

>**Result**: In this exercise, you created a SQL Database, uploaded data to SQL Database and added an index. You stored a SQL Database credential in the Data Lake Analytics catalog using PowerShell, configured a Data Lake Analytics job to use stored credentials, and ran this job using stored credentials to access data in SQL Database.

## Exercise 4: Use Data Lake Analytics to categorize data and present results using Power BI

### Scenario

You want to present average speed data on a map, so that it’s easy to see traffic patterns across a city area. In this exercise, you will generate analytics for speed cameras, showing the number of cars that passed each camera in different speed “buckets” (< 10 mph, 10-29 mph, 30-49 mph, 50-69 mph, 70-99 mph, and 100+ mph); this analysis should incorporate current and historical data from the speed cameras. You will then visualize the data using the ARCGis map control in Power BI.

The main tasks for this exercise are as follows:

1. Create a new Data Lake project
2. Test the Data Lake Analytics job locally then run it in the cloud
3. Use Power BI to visualize the data
4. Lab closedown

### Task 1: Create a new Data Lake project

1. Using Visual Studio, create a new U-SQL project named **SpeedCameraAnalytics**.
2. In the **Script.usql** file, define a rowset variable that extracts the data from the speed camera data files (uploaded in Exercise 1) under the **/speeds** folder. These files are organized by date and time, but you should ensure that you read every available file. Remember that each file is in CSV format, and has a header. Your code should be similar to the following:

    > **Note**: You can copy the code for this exercise from the file **E:\\Labfiles\\Lab05\\Exercise4\\CameraJobCmds.txt**:

    ```SQL
    // Capture the historical and current data from all available files containing speed data
    DECLARE @speedCameraData string = @"/speeds/{Date:yyyy}/{Date:MM}/{Date:dd}/{Hour}/{*}.csv";

    // Specify the format of the data in the CSV files
    @speedData =
        EXTRACT CameraID string,
                VehicleRegistration string,
                Speed int,
                SpeedLimit int,
                LocationLatitude double,
                LocationLongitude double,
                Time DateTime,
                Date DateTime,
                Hour int
        FROM @speedCameraData
        USING Extractors.Csv(skipFirstNRows: 1);
    ```

3. Create a rowset that retrieves the data in the CameraID and Speed fields from these files. Your code should be similar to the following:

    ```SQL
    // Generate a rowset that reads the relevant data from the CSV files
    @cameraData =
        SELECT CameraID, Speed, LocationLatitude, LocationLongitude
        FROM @speedData;
    ```

4. Define a rowset that calculates, for each camera, the number of vehicles that were travelling at less than 10 mph (this is the first "bucket"). Your code should be similar to the following:

    ```SQL
    // Define a rowset that calculates the number of vehicles for each camera that were travelling at less than 10 mph (this is the first "bucket")
    @lessThan10 =
        SELECT CameraID, LocationLatitude, LocationLongitude, COUNT(*) AS LessThan10
        FROM @cameraData
        WHERE Speed < 10
        GROUP BY CameraID, LocationLatitude, LocationLongitude;
    ```

5. Define another rowset that, for each camera, calculates the number of vehicles that were travelling at between 10 and 29 mph (this is the second "bucket). Your code should be similar to the following:

    ```SQL
    // Define a rowset that calculates the number of vehicles for each camera that were travelling at between 10 and 29 mph (this is the second "bucket)
    @between10And29 =
        SELECT CameraID, LocationLatitude, LocationLongitude, COUNT(*) AS Between10And29
        FROM @cameraData
        WHERE Speed BETWEEN 10 AND 29
        GROUP BY CameraID, LocationLatitude, LocationLongitude;
    ```

6. Repeat the previous code for the following speed ranges—30-49 mph, 50-69 mph, 70-99 mph, 100 mph and more. Your code should be similar to the following:

    ```SQL
    // Define rowsets for the following speed ranges; 30-49 mph, 50-69 mph, 70-99 mph, 100 mph and over
    @between30And49 =
        SELECT CameraID, LocationLatitude, LocationLongitude, COUNT(*) AS Between30And49
        FROM @cameraData
        WHERE Speed BETWEEN 30 AND 49
        GROUP BY CameraID, LocationLatitude, LocationLongitude;

    @between50And69 =
        SELECT CameraID, LocationLatitude, LocationLongitude, COUNT(*) AS Between50And69
        FROM @cameraData
        WHERE Speed BETWEEN 50 AND 69
        GROUP BY CameraID, LocationLatitude, LocationLongitude;

    @between70And99 =
        SELECT CameraID, LocationLatitude, LocationLongitude, COUNT(*) AS Between70And99
        FROM @cameraData
        WHERE Speed BETWEEN 70 AND 99
        GROUP BY CameraID, LocationLatitude, LocationLongitude;

    @over99 =
        SELECT CameraID, LocationLatitude, LocationLongitude, COUNT(*) AS Over99
        FROM @cameraData
        WHERE Speed > 99
        GROUP BY CameraID, LocationLatitude, LocationLongitude;
    ```

7. Create another rowset that joins the buckets together over the CameraID column. You must use an OUTER join in case the bucket for a particular speed camera is empty (you don't want to discard all the data in the other buckets for that camera); your **Script.usql** code should be similar to the following:

    ```SQL
    // Join the speed buckets together, this requires an OUTER join, as one or more buckets for a camera could be null (empty)

    @speedAnalysis =
        SELECT a.CameraID, a.LocationLatitude, a.LocationLongitude, a.LessThan10, b.Between10And29, c.Between30And49, d.Between50And69, e.Between70And99, f.Over99
        FROM @lessThan10 AS a
        LEFT JOIN @between10And29 AS b
        ON a.CameraID == b.CameraID
        LEFT JOIN @between30And49 AS c
        ON a.CameraID == c.CameraID
        LEFT JOIN @between50And69 AS d
        ON a.CameraID == d.CameraID
        LEFT JOIN @between70And99 AS e
        ON a.CameraID == e.CameraID
        LEFT JOIN @over99 AS f
        ON a.CameraID == f.CameraID;
    ```

8. Save the results to a CSV file named "/SpeedAnalysis.csv". Include a header. Your code should be similar to the following:

    ```SQL
    OUTPUT @speedAnalysis
    TO "/SpeedAnalysis.csv"
    USING Outputters.Csv(outputHeader: true);
    ```

### Task 2: Test the Data Lake Analytics job locally then run it in the cloud

1. Submit the job locally, and verify that it runs successfully.
2. Examine the job graph to see the various processing steps.
3. Examine the results in the ***C:\\Users\\AdatumAdmin\\AppData\\Local\\USQLDataRoot\\SpeedAnalysis.csv** file. The file should contain a tabular list of cameras and the number of cars in each speed bucket.
4. Run the job again, using your ADLA account in the cloud.
5. When the job has completed, examine the job graph again.
6. Review the data in the SpeedAnalysis.csv file generated by the job, and verify that it is the same as before.

### Task 3: Use Power BI to visualize the data

1. Start Power BI Desktop create a data connection to your Data Lake Store, using the following URL (replace **&lt;name of your Data Lake Store&gt;** with **adls&lt;_your name_&gt;&lt;_date_&gt;**):

    ```Text
    adl://<Your ADLS account>.azuredatalakestore.net/SpeedAnalysis.csv
    ```
2. If prompted to log in to Azure, authenticate using the Microsoft account that is associated with your Azure Learning Pass subscription.
3. In the **Query Editor**, ensure that the content is identified as Binary data (the data in the file should appear).
4. In Power BI, use the ArcGIS Maps for Power BI Visualization:
    - Drag **LocationLatitude** to the **Latitude** box.
    - Drag **LocationLongitude** to the **Longitude** box.
    - Drag **CameraID** to the **Location** box.
5. To make the names more meaningful, rename **LessThan10**, to **Less than 10 mph**.
6. Rename each of the **BetweenXAndY** fields to **Between X and Y mph** (for example, Between10And29 should be renamed to Between 10 and 29 mph).
7. Rename **Over99** to **Over 99 mph**.
8. Add the **Less than 10 mph** field, all the **Between X and Y mph fields**, and the **Over 99 mph** field to the **Tooltips** (in order, with Less than 10 mph at the top).You should now have all the speed fields listed under **Tooltips**, and in ascending speed order.
9. Use **Focus Mode**, and view the map. Zoom in and hover over any speed camera to see the detected speed values for that camera.
10. Save the report as **SpeedCameraAnalysis.pbix**.

### Task 4: Lab closedown

1. Close Windows PowerShell ISE.
2. Do not remove the Azure resources; these resources will be used in later labs.
3. Close Internet Explorer.

>**Result**: In this exercise, you created a new Data Lake project, tested the job locally, ran it in the cloud, and then used Power BI to visualize the results.

---

©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
