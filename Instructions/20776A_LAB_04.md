# Module 4: Managing Big Data in Azure Data Lake Store

- [Module 4: Managing Big Data in Azure Data Lake Store](#module-4-managing-big-data-in-azure-data-lake-store)
    - [Lab: Managing big data in Data Lake Store](#lab-managing-big-data-in-data-lake-store)
        - [Scenario](#scenario)
        - [Objectives](#objectives)
        - [Lab Setup](#lab-setup)
    - [Exercise 1: Perform Data Lake Store operations using PowerShell](#exercise-1-perform-data-lake-store-operations-using-powershell)
        - [Scenario](#scenario)
        - [Setup](#setup)
        - [Task 1: Install AzCopy](#task-1-install-azcopy)
        - [Task 2: Install AdlCopy](#task-2-install-adlcopy)
        - [Task 3: Prepare the PowerShell environment](#task-3-prepare-the-powershell-environment)
        - [Task 4: Use PowerShell to create a new Data Lake Store](#task-4-use-powershell-to-create-a-new-data-lake-store)
        - [Task 5: Use PowerShell to manage files and folders in Data Lake Store](#task-5-use-powershell-to-manage-files-and-folders-in-data-lake-store)
    - [Exercise 2: Upload bulk data to Data Lake Store](#exercise-2-upload-bulk-data-to-data-lake-store)
        - [Scenario](#scenario)
        - [Task 1: Create a new Storage account and Blob container](#task-1-create-a-new-storage-account-and-blob-container)
        - [Task 2: Upload files to the new Blob container](#task-2-upload-files-to-the-new-blob-container)
        - [Task 3: Verify the file upload using Cloud Explorer](#task-3-verify-the-file-upload-using-cloud-explorer)
        - [Task 4: Use AdlCopy to copy the files from Blob storage to Data Lake Store](#task-4-use-adlcopy-to-copy-the-files-from-blob-storage-to-data-lake-store)
        - [Task 5: Verify the data copy using Data Explorer](#task-5-verify-the-data-copy-using-data-explorer)
    - [Exercise 3: Set ACLs on Data Lake Store folders and files](#exercise-3-set-acls-on-data-lake-store-folders-and-files)
        - [Scenario](#scenario)
        - [Task 1: Create a Microsoft account for a Data Lake guest user](#task-1-create-a-microsoft-account-for-a-data-lake-guest-user)
        - [Task 2: Add the guest user account to Azure Active Directory](#task-2-add-the-guest-user-account-to-azure-active-directory)
        - [Task 3: Set guest user permissions for a Data Lake Store folder](#task-3-set-guest-user-permissions-for-a-data-lake-store-folder)
        - [Task 4: Start a PowerShell session as the guest user](#task-4-start-a-powershell-session-as-the-guest-user)
        - [Task 5: Use PowerShell to attempt to list folder contents](#task-5-use-powershell-to-attempt-to-list-folder-contents)
        - [Task 6: Set permissions for the root folder](#task-6-set-permissions-for-the-root-folder)
        - [Task 7: Use PowerShell to list the folder contents after a permissions change](#task-7-use-powershell-to-list-the-folder-contents-after-a-permissions-change)
        - [Task 8: Use PowerShell to attempt to read a file](#task-8-use-powershell-to-attempt-to-read-a-file)
        - [Task 9: Set permissions for a file](#task-9-set-permissions-for-a-file)
        - [Task 10: Use PowerShell to read a file after a permissions change](#task-10-use-powershell-to-read-a-file-after-a-permissions-change)
        - [Task 11: Use PowerShell to attempt to upload a new file](#task-11-use-powershell-to-attempt-to-upload-a-new-file)
        - [Task 12: Edit folder permissions to enable file upload](#task-12-edit-folder-permissions-to-enable-file-upload)
        - [Task 13: Use PowerShell to overwrite a file](#task-13-use-powershell-to-overwrite-a-file)
        - [Task 14: Use PowerShell to attempt to overwrite a file after a folder permissions change](#task-14-use-powershell-to-attempt-to-overwrite-a-file-after-a-folder-permissions-change)
    - [Exercise 4: Encrypt Data Lake Store data using your own key](#exercise-4-encrypt-data-lake-store-data-using-your-own-key)
        - [Scenario](#scenario)
        - [Task 1: Create a new Key Vault account and key](#task-1-create-a-new-key-vault-account-and-key)
        - [Task 2: Configure a new Data Lake storage account to use Key Vault encryption](#task-2-configure-a-new-data-lake-storage-account-to-use-key-vault-encryption)
        - [Task 3: Use Data Explorer to upload a file and verify its contents](#task-3-use-data-explorer-to-upload-a-file-and-verify-its-contents)
        - [Task 4: Back up the Key Vault key, and then delete the key](#task-4-back-up-the-key-vault-key-and-then-delete-the-key)
        - [Task 5: Attempt to access encrypted data without a key](#task-5-attempt-to-access-encrypted-data-without-a-key)
        - [Task 6: Attempt to upload data to encrypted storage without a key](#task-6-attempt-to-upload-data-to-encrypted-storage-without-a-key)
        - [Task 7: Restore a deleted key and verify data access](#task-7-restore-a-deleted-key-and-verify-data-access)
        - [Task 8: Lab closedown](#task-8-lab-closedown)
## Lab: Managing big data in Data Lake Store

### Scenario

You work for Adatum as a data engineer, and have been asked to build a traffic surveillance system for the traffic police. This system must be able to analyze significant amounts of dynamically streamed data that is captured from speed cameras and automatic number plate recognition (ANPR) devices, and then crosscheck the outputs against large volumes of reference data holding vehicle, driver, and location information. Fixed roadside cameras, hand-held cameras (held by traffic police), and mobile cameras (in police patrol cars) are used to monitor traffic speeds and raise an alert if a vehicle is travelling too quickly for the local speed limit. The cameras also have built-in ANPR software that reads vehicle registration plates.

For the next phase of the project, you will use a range of tools to enable batch mode, and automated operations, with Data Lake Store. You will also add security to your Data Lake Store, using custom ACLs and data encryption that uses your own managed key.

### Objectives

After completing this lab, you will be able to:

- Perform Data Lake Store operations using PowerShell.
- Upload bulk data to Data Lake Store.
- Set ACLs on Data Lake Store folders and files.
- Encrypt Data Lake Store data using your own key.

### Lab Setup

Estimated time: 90 minutes
Virtual machine: **20776A-LON-DEV**
User name: **ADATUM\\AdatumAdmin**
Password: **Pa55w.rd**

## Exercise 1: Perform Data Lake Store operations using PowerShell

### Scenario

To enable automated and batch mode operations with Data Lake Store, you will first investigate the use of PowerShell cmdlets for Data Lake Store. In this exercise, you will use PowerShell to deploy a new Data Lake Store, and to manage folders and files in this store.

The main tasks for this exercise are as follows:

1. Install AzCopy
2. Install AdlCopy
3. Prepare the PowerShell environment
4. Use PowerShell to create a new Data Lake Store
5. Use PowerShell to manage files and folders in Data Lake Store

### Setup

1. Ensure that the **MT17B-WS2016-NAT**, **20776A-LON-DC**, and **20776A-LON-DEV** virtual machines are running, and then log on to **20776A-LON-DEV** as **ADATUM\\AdatumAdmin** with the password **Pa55w.rd**.
2. Run the script **E:\\Labdiles\\Lab04\\Setup.cmd** as administrator.

### Task 1: Install AzCopy

1. Download and install **AzCopy** from **https://aka.ms/downloadazcopy**.
2. Add: **C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\AzCopy** to the **Path** environment variable.

### Task 2: Install AdlCopy

1. Download and install **AdlCopy** from **https://www.microsoft.com/en-us/download/details.aspx?id=50358**.
2. Add **%HOMEPATH%\\Documents\\AdlCopy** to the **Path** environment variable.

### Task 3: Prepare the PowerShell environment

1. Start **PowerShell ISE as Administrator**, and open the file  **E:\\Labfiles\\Lab04\\ADLS\_commands.ps1**. The script looks like this:

    ``` PowerShell
    # Log in to Azure
    Login-AzureRmAccount

    # Register the Azure Data Lake Store provider
    Register-AzureRmResourceProvider -ProviderNamespace "Microsoft.DataLakeStore"

    # Create a new Resource Group
    New-AzureRmResourceGroup -Location "<location>" -Name "LakeStoreRG"

    # Create a new Data Lake Store Account
    $resourceGroupName = "LakeStoreRG"
    $dataLakeStoreName = "<name of new Data Lake Store"
    New-AzureRmDataLakeStoreAccount -ResourceGroupName $resourceGroupName -Name $dataLakeStoreName -Location "<location>"

    # Verify that the Data Lake Store account has been created
    Test-AzureRmDataLakeStoreAccount -Name $dataLakeStoreName

    # Create a folder named VehicleOwnerData
    $folder="/VehicleOwnerData"
    New-AzureRmDataLakeStoreItem -Folder -AccountName $dataLakeStoreName -Path $folder

    # Upload the VehicleOwners.csv file to the VehicleOwnerData folder
    Import-AzureRmDataLakeStoreItem -Account $dataLakeStoreName -Path "E:\Labfiles\Lab04\Exercise1\VehicleOwners.csv" -Destination $folder\VehicleOwners.csv

    # List all files and folders in the VehicleOwnerData directory
    Get-AzureRmDataLakeStoreChildItem -AccountName $dataLakeStoreName -Path $folder
    ```

2. In the script, replace the text **&lt;name of new Data Lake Store&gt;** with **lakestore&lt;your name&gt;&lt;date&gt;**, and both instances of **&lt;location&gt;** with **Central US**, **East US 2**, or **North Europe**.
3. Save the updated script.

### Task 4: Use PowerShell to create a new Data Lake Store

1. Execute lines 1-13 to create a new Data Lake Store, and then on the toolbar, click **Run Selection**.
2. In the **Sign in to your account** dialog box, enter the details of the Microsoft account that is associated with your Azure Learning Pass subscription, and then click **Sign in**.
3. Execute lines 15-16 to verify that the new Data Lake Store has been created.

### Task 5: Use PowerShell to manage files and folders in Data Lake Store

1. Execute lines 18-20 to create a folder called **VehicleOwnerData**.
2. Execute lines 22-23 to upload a file called **VehicleOwners.csv** to the **VehicleOwnerData** folder.
3. Execute lines 25-26 to list all files and folders in the **VehicleOwnerData** directory. You should see the **VehicleOwners.csv** file listed.

>**Result**: In this exercise, you used PowerShell cmdlets to:
>
> - Create a new Data Lake Store (in a new resource group).
> - Create a folder in this store.
> - Upload a file to the folder.
> - Display to files in the folder.

## Exercise 2: Upload bulk data to Data Lake Store

### Scenario

To support automated batch mode operations with Data Lake Store, you will next investigate the command-line tools that enable bulk data upload to Azure Blob storage, and bulk transfers of data from Azure Blob storage to Data Lake Storage. In this exercise, you will use AzCopy to upload data to Blob storage, and then use AdlCopy to transfer this data to a Data Lake Store.

The main tasks for this exercise are as follows:

1. Create a new Storage account and Blob container
2. Upload files to the new Blob container
3. Verify the file upload using Cloud Explorer
4. Use AdlCopy to copy the files from Blob storage to Data Lake Store
5. Verify the data copy using Data Explorer

### Task 1: Create a new Storage account and Blob container

1. Open the Azure portal, and sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.
2. Create a new Storage account, with the following details:
    - **Resource group (Use existing)**: LakeStoreRG (this resource group was created by the PowerShell script that you ran in Exercise 1)
    - **Name**: blob&lt;your name&gt;&lt;date&gt;
    - **Location**: select the same location as you used for the Data Lake Store in Exercise 1
    - **Account kind**: Blob storage
    - Leave all other details at their defaults
3. Wait until the storage account has been successfully created before continuing with the exercise.
4. Create a new Blob container in your storage account called **vehiclefeed**.
5. Copy the **key1** access key for the Storage account to the clipboard.

### Task 2: Upload files to the new Blob container

1. Go to **E:\\Labfiles\\Lab04\\Exercise2**, and note that there are multiple folders and subfolders containing vehicle registration data, which are organized by year and month.
2. Open an Admin-level Command Prompt, and execute the following command. Replace the text **&lt;storage account name&gt;** with the name of the account that was previously created—that is, **blob&lt;your name&gt;&lt;date&gt;**), and the text **&lt;access key&gt;** with the key you copied to the clipboard. The command should upload 216 files:

    ```Cmd
    azcopy /Source:"E:\Labfiles\Lab04\Exercise2" /Dest:https://<storage account name>.blob.core.windows.net/vehiclefeed /DestKey:<access key> /S
    ```

    You can copy this command from the file **E:\\Labfiles\\Lab04\\AZCopyCmd.txt**.

### Task 3: Verify the file upload using Cloud Explorer

1. Start Visual Studio, and open **Cloud Explorer**.
2. In **Cloud Explorer**, if subscription associated with your Azure Learning Pass is not listed, add the Microsoft account that is associated with your Azure Learning Pass subscription.
3. Use Cloud Explorer to examine the contents of **vehiclefeed**. The folder structure should be the same as that under the **E:\\Labfiles\\Lab04\\Exercise2** folder, and it should contain the same files.

### Task 4: Use AdlCopy to copy the files from Blob storage to Data Lake Store

1. Execute the following command. Replace the text **&lt;storage account name&gt;** with **blob&lt;your name&gt;&lt;date&gt;**, replace **&lt;name of your Data Lake Store&gt;** with **lakestore&lt;your name&gt;&lt;date&gt;**, and replace **&lt;access key&gt;** with the blob store key that you copied to the clipboard earlier:

    ```Cmd
    adlcopy /source https://<storage account name>.blob.core.windows.net/vehiclefeed/ /dest adl://<name of your Data Lake Store>.azuredatalakestore.net/VehicleRegistrations/ /sourcekey <access key>
    ```

    You can copy this command from the file **E:\\Labfiles\\Lab04\\AdlCopyCmd.txt**.

2. If you are prompted, sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.

   > **Note**: Both the source and destination URLs must end with a **/**; AdlCopy is automatically recursive, and there is no **/S** parameter.

3. When the copy is complete, close the command prompt.

### Task 5: Verify the data copy using Data Explorer

- Use Data Explorer for your data Lake Storage account to view the contents of the **VehicleRegistrations** folder, and verify that the files that were uploaded to Blob storage have now been copied to your Data Lake Store.

>**Result**: In this exercise, you:
>
> - Created a new Azure storage account and Blob container.
> - Uploaded a set of files and folders to the container using AzCopy.
> - Verified the upload using Cloud Explorer in Visual Studio.
> - Copied the data from Blob storage to Data Lake Store using AdlCopy.
> - Verified the copy using Data Explorer in the Azure portal.

## Exercise 3: Set ACLs on Data Lake Store folders and files

### Scenario

Adatum’s new traffic surveillance system will use Data Lake Store for primary data storage; therefore, it’s essential that this data is protected. In this exercise, you will add security to your Data Lake Store, using custom ACLs. To test and evaluate the security model in Data Lake Store, you will use a guest user account.

The main tasks for this exercise are as follows:

1. Create a Microsoft account for a Data Lake guest user
2. Add the guest user account to Azure Active Directory
3. Set guest user permissions for a Data Lake Store folder
4. Start a PowerShell session as the guest user
5. Use PowerShell to attempt to list folder contents
6. Set permissions for the root folder
7. Use PowerShell to list the folder contents after a permissions change
8. Use PowerShell to attempt to read a file
9. Set permissions for a file
10. Use PowerShell to read a file after a permissions change
11. Use PowerShell to attempt to upload a new file
12. Edit folder permissions to enable file upload
13. Use PowerShell to overwrite a file
14. Use PowerShell to attempt to overwrite a file after a folder permissions change

### Task 1: Create a Microsoft account for a Data Lake guest user

1. Use an InPrivate browser session to go to **https://signup.live.com**, and create a new Microsoft account with the following information:
    - An email address of **Course20776aLab-&lt;date&gt;-&lt;your name&gt;@outlook.com**.
    - A password of **Pa55w.rd**.
    - Your first name and last name.
    - Your local country/region.
    - Your date of birth.
  
2. When the account has been created, configure you email account with the default settings, open the mail inbox, and then click **Let's go**.

### Task 2: Add the guest user account to Azure Active Directory

1. In the Azure portal, open **Azure Active Directory**, and add a guest user, using the email address of the guest user account that you created in the previous task.
2. In the mail inbox for the guest user account, open the **Microsoft Invitations** email, and click **Get Started**, and follow the instructions to complete sign-up as a guest user.

### Task 3: Set guest user permissions for a Data Lake Store folder

1. In the Azure portal, on the **Azure Active Directory** blade, copy the **Directory ID** to the clipboard.
2. Use the Data Explorer for your Data Lake Storage account to add the following permissions to the **VehicleOwnerData** folder for the guest user account:
    - **Read** and **Execute** as **An access permission entry and a default permission entry**
3. Verify that your guest account is shown under **Assigned permissions** for the **VehicleOwnerData** folder, and that it has Read and Execute permissions.

### Task 4: Start a PowerShell session as the guest user

1. Start Windows PowerShell, and at the PowerShell command prompt, execute the following command. Replace &lt;DirectoryID&gt; with the directory ID that you copied to the clipboard in the previous task:

    ``` PowerShell
    Login-AzureRmAccount -TenantID <DirectoryID>
    ```

    > **Note**: The the PowerShell commands used in this exercise can be copied from the file **E:\\Labfiles\\Lab04\\PSCmds.txt**.

2. Sign in using the details of the guest user account.
3. If the **You're already signed in** dialog box appears, use the **Sign out and sign in with a different account** option.

### Task 5: Use PowerShell to attempt to list folder contents

1. Execute the following PowerShell command. Replace **&lt;name of your Data Lake Store&gt;** with **lakestore&lt;your name&gt;&lt;date&gt;**:

    ```PowerShell
    $dataLakeStoreName = "<name of your Data Lake Store>"
    ```

2. Execute the following PowerShell command:

    ``` PowerShell
    Get-AzureRmDataLakeStoreChildItem -AccountName $dataLakeStoreName -Path "/VehicleOwnerData"
    ```

   This command will return the following error: **Operation returned an invalid status code 'Forbidden'.**. This is because you do not have Read and Execute permissions over the root folder of the Data Lake Storage account.

### Task 6: Set permissions for the root folder

1. In the Azure portal, use Data Explorer to set the following permissions to the root folder for the guest user account:
    - **Read** and **Execute**
2. Verify that your guest account is shown under **Assigned permissions**, and with the correct permissions.

### Task 7: Use PowerShell to list the folder contents after a permissions change

- Execute the following PowerShell command:

    ``` PowerShell
    Get-AzureRmDataLakeStoreChildItem -AccountName $dataLakeStoreName -Path "/VehicleOwnerData"
    ```

    This command should now successfully list the contents of the **VehicleOwnerData** folder and display the details of the **VehicleOwners.csv** file.

### Task 8: Use PowerShell to attempt to read a file

1. Execute the following PowerShell command:

    ``` PowerShell
    $path= "/VehicleOwnerData/VehicleOwners.csv"
    ```

2. Execute the following PowerShell command:

    ``` PowerShell
    $download= "E:\Labfiles\Lab04\VehicleOwners-downloaded.csv"
    ```

3. Execute the following PowerShell command:

    ```PowerShell
    Export-AzureRmDataLakeStoreItem -Account $dataLakeStoreName -Path $path -Destination $download
    ```

    This command attempts to download the VehicleOwners.csv file, but will return an error.

4. Execute the following PowerShell command:

    ```PowerShell
    DIR E:\Labfiles\Lab04
    ```

    Note that the **Export-AzureRmDataLakeStoreItem** cmdlet has created the **VehicleOwners-downloaded.csv** file (0 bytes), but was unable to read the file's contents. This is because the file existed before permissions were granted to your guest account and, therefore, this account does not have permission to read the contents of the file.

### Task 9: Set permissions for a file

1. In the Azure portal, use the **Data Explorer** blade to set the following permissions to **VehicleOwners.csv** for the guest user account:
    - Read
2. Verify that your guest account is shown under Assigned Permissions, and has the correct permissions.

### Task 10: Use PowerShell to read a file after a permissions change

1. Execute the following PowerShell command:

    ```PowerShell
    Export-AzureRmDataLakeStoreItem -Account $dataLakeStoreName -Path $path -Destination $download -Force
    ```

    This command should now successfully read and download the **VehicleOwners-downloaded.csv** file.

2. Execute the following PowerShell command:

    ```PowerShell
    DIR E:\Labfiles\Lab04
    ```

3. Verify that the **VehicleOwners-downloaded.csv** file has been downloaded to the **E:\\Labfiles\\Lab04** folder and that it has a non-zero length.

### Task 11: Use PowerShell to attempt to upload a new file

1. Execute the following PowerShell command:

    ``` PowerShell
    $upload= "/VehicleOwnerData/VehicleOwnersUpdated.csv"
    ```

2. Execute the following PowerShell command:

    ``` PowerShell
    $fileName = "E:\Labfiles\Lab04\Exercise3\VehicleOwners.csv"
    ```

3. Execute the following PowerShell command:

    ``` PowerShell
    Import-AzureRmDataLakeStoreItem -Account $dataLakeStoreName -Path $fileName -Destination $upload
    ```

    This command will return the following error: **Operation returned an invalid status code 'Forbidden'.**. This is because you do not have the necessary Write permission in the **VehicleOwnerData** folder in the Data Lake Store.

### Task 12: Edit folder permissions to enable file upload

1. In the Azure portal, use the **Data Explorer** blade to add the following permissions to the **VehicleOwnerData** folder for the guest user account:
    - Write

2. Execute the following PowerShell command:

    ```PowerShell
    Import-AzureRmDataLakeStoreItem -Account $dataLakeStoreName -Path $fileName -Destination $upload
    ```

    This time, the file should be uploaded successfully.

3. Switch to the Azure portal, and on the **Data Explorer** blade, verify that **VehicleOwnersUpdated.csv** has now been successfully uploaded.
4. On the **Data Explorer** blade, click the ellipses (**...**) for **VehicleOwnersUpdated.csv**, and then click **Access**. Note that your guest user account has been assigned the permissions that you set in the Default Permissions Entry for the VehicleOwnerData folder in Task 3.
5. Close the **Access** blade.
6. On the **Data Explorer** blade, click the ellipses (**...**) for **VehicleOwnersUpdated.csv**, and then click **Properties**.
7. Make a note of the **LAST MODIFIED TIME** property, and then close the **Properties** blade.

### Task 13: Use PowerShell to overwrite a file

1. Execute the following PowerShell command:

    ``` PowerShell
    Import-AzureRmDataLakeStoreItem -Account $dataLakeStoreName -Path $fileName -Destination $upload -Force
    ```

    This command should successfully overwrite the **VehicleOwnersUpdated.csv** file.
2. To verify the upload, execute the following PowerShell command:

    ``` PowerShell
    Get-AzureRmDataLakeStoreChildItem –AccountName $dataLakeStoreName -Path "/VehicleOwnerData"
    ```

3. Compare the **LastWriteTime** for **VehicleOwnersUpdated.csv** that you recorded in the previous task. It should have changed to the current system time.

### Task 14: Use PowerShell to attempt to overwrite a file after a folder permissions change

1. Use the **Data Explorer** blade to remove the **Write** permission to the **VehicleOwnerData** folder for your guest account.
2. Execute the following PowerShell command:

    ``` PowerShell
    Import-AzureRmDataLakeStoreItem -Account $dataLakeStoreName -Path $fileName -Destination $upload -Force
    ```

    This command will return the following error: **Operation returned an invalid status code 'Forbidden'**. This is because, although your guest user account has Write permission on the file, it does not have Write permission on the folder. Write privileges on a file only enable the file to be appended; they do not enable deletion or overwrite. Without Write access to the folder, the attempted overwrite fails and the old file is left intact.

3. Close PowerShell ISE.

>**Result**: In this exercise, you created a guest user account in Azure AD, and then tested the ability of this account to view folder contents, open files, and upload and update files in Data Lake Store, depending on the specific permissions that are set. You used the Azure portal to manage the permissions, and used PowerShell as the guest user environment.

## Exercise 4: Encrypt Data Lake Store data using your own key

### Scenario

As a final step, you decide to configure additional security for your Data Lake Store by setting up data encryption using your own managed key. In this exercise, you will create a new Key Vault and key, use this key to protect a new Data Lake Store, and then investigate the effects of key deletion and key restoration on the ability to access data.

The main tasks for this exercise are as follows:

1. Create a new Key Vault account and key
2. Configure a new Data Lake storage account to use Key Vault encryption
3. Use Data Explorer to upload a file and verify its contents
4. Back up the Key Vault key, and then delete the key
5. Attempt to access encrypted data without a key
6. Attempt to upload data to encrypted storage without a key
7. Restore a deleted key and verify data access
8. Lab closedown

### Task 1: Create a new Key Vault account and key

1. Use the Azure portal to create a new **Key Vault**, with the following details:
    - **Name**: VehiclesKV&lt;your name&gt;&lt;date&gt;
    - **Resource Group (Use existing)**: LakeStoreRG
    - Leave all other settings at their defaults
2. Wait until the Key Vault has deployed before continuing with the exercise.
3. Add a **key** called **Key1** to the Key Vault.

### Task 2: Configure a new Data Lake storage account to use Key Vault encryption

1. Use the Azure portal to create a new **Data Lake Store**, with the following details:
    - **Name**: lakestore2&lt;your name&gt;&lt;date&gt;
    - **Resource Group (Use existing)**: LakeStoreRG
    - **Location**: Select the location you used for the Data Lake Store in Exercise 1
    - **Encryption Settings**: Use Key1 from your Key Vault
2. Wait until the storage has deployed before continuing with the lab.
3. Grant your new Data Lake Store permissions to your Key Vault. Record the value of the **Key Id** used by the Data Lake Store.

### Task 3: Use Data Explorer to upload a file and verify its contents

1. Use Data Explorer for your Data Lake Storage account to upload **E:\\Labfiles\\Lab04\\Exercise4\\testdata1.json** to your new Data Lake Store.
2. View the contents of **testdata1.json**. This file containsa copy of the data about fictitious criminal offenses.

### Task 4: Back up the Key Vault key, and then delete the key

1. Use the Azure portal to create a backup of the key **Key1** in your key vault to the folder **E:\\Labfiles\\Lab04\\Exercise4**.
2. Delete **Key1** from your Key Vault.
3. Wait 2-3 minutes before continuing with the exercise.

### Task 5: Attempt to access encrypted data without a key

- Use Data Explorer for your Data Lake Storage account to attempt to view the contents of **testdata1.json**. Note the **RuntimeException** and **Azure Key Vault key not found** message.

### Task 6: Attempt to upload data to encrypted storage without a key

- Use the **Data Explorer** blade to attempt to upload **E:\\Labfiles\\Lab04\\Exercise4\\testdata2.csv**. Note the Upload Error message and note that the failed upload has created a 0 byte file.

### Task 7: Restore a deleted key and verify data access

1. Restore **Key1** to your key vault from your local backup.
2. Use Data Explorer for your Data Lake Storage account to view the contents of **testdata1.json**. Verify that you can now open the file.
3. Use Data Explorer to retry the upload of **E:\\Labfiles\\Lab04\\Exercise4\\testdata2.csv**, ensuring that you allow the overwriting of existing files. Note that you can now upload files again.
4. View the contents of **testdata2.csv**, and view the file contents.

### Task 8: Lab closedown

- In the Azure portal, delete the **LakeStoreRG** resource group.

>**Result**: In this exercise, you created a new Key Vault and key, used this key to encrypt a new Data Lake Store, uploaded data to the store, created a key backup and then deleted the key. You then attempted to access and upload data after key deletion. Finally, you restored the deleted key and verified data access.

**Question**: Why might you set a default permission entry on a folder in Data Lake Store?

**Question**: Is encryption using Key Vault the only way to encrypt data at rest properly in Data Lake Store?

---

©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
